
    
    
    
    
    [{"authors":[],"categories":[],"content":"News 📰 Determinable and interpretable network representation for link prediction\nAzure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students\nPenn State researchers to explore using quantum computers to design new drugs\nMultiverse Computing and Mila Join Forces to Advance Artificial Intelligence with Quantum Computing\nQuantum AI Elon Musk Review / Scam App Or Legit?\nVideos 📽️ Quantum Machine Learning Explained\nQiskit Falll Fest CIC-IPN Mexico 2022- Quantum Machine Learning\nQuantum Machine Learning Neuroimaging for Alzheimer’s Disease\nMLBBQ: Quantum Machine Learning by Pavel Popov\nQuantum Machine Learning: Opportunities and Challenges\nHands-on quantum machine learning | Rodrigo Morales | ADC2022\n“Reinforcement Learning for Quantum Technologies,” presented by Florian Marquardt\nPublications 📃 Representation Theory for Geometric Quantum Machine Learning Abstract: Recent advances in classical machine learning have shown that creating models with inductive biases encoding the symmetries of a problem can greatly improve performance. Importation of these ideas, combined with an existing rich body of work at the nexus of quantum theory and symmetry, has given rise to the field of Geometric Quantum Machine Learning (GQML). Following the success of its classical counterpart, it is reasonable to expect that GQML will play a crucial role in developing problem-specific and quantum-aware models capable of achieving a computational advantage. Despite the simplicity of the main idea of GQML – create architectures respecting the symmetries of the data – its practical implementation requires a significant amount of knowledge of group representation theory. We present an introduction to representation theory tools from the optics of quantum learning, driven by key examples involving discrete and continuous groups. These examples are sewn together by an exposition outlining the formal capture of GQML symmetries via “label invariance under the action of a group representation”, a brief (but rigorous) tour through finite and compact Lie group representation theory, a reexamination of ubiquitous tools like Haar integration and twirling, and an overview of some successful strategies for detecting symmetries.\nTheory for Equivariant Quantum Neural Networks Abstract: Most currently used quantum neural network architectures have little-to-no inductive biases, leading to trainability and generalization issues. Inspired by a similar problem, recent breakthroughs in classical machine learning address this crux by creating models encoding the symmetries of the learning task. This is materialized through the usage of equivariant neural networks whose action commutes with that of the symmetry. In this work, we import these ideas to the quantum realm by presenting a general theoretical framework to understand, classify, design and implement equivariant quantum neural networks. As a special implementation, we show how standard quantum convolutional neural networks (QCNN) can be generalized to group-equivariant QCNNs where both the convolutional and pooling layers are equivariant under the relevant symmetry group. Our framework can be readily applied to virtually all areas of quantum machine learning, and provides hope to alleviate central challenges such as barren plateaus, poor local minima, and sample complexity.\nTheoretical Guarantees for Permutation-Equivariant Quantum Neural Networks Abstract: Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry Sn), and show how to build Sn-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and can generalize well from small amounts of data. To verify our results, we perform numerical simulations for a graph state classification task. Our work provides the first theoretical guarantees for equivariant QNNs, thus indicating the extreme power and potential of GQML.\nProtocols for classically training quantum generative models on probability distributions Abstract: Quantum Generative Modelling (QGM) relies on preparing quantum states and generating samples from these states as hidden - or known - probability distributions. As distributions from some classes of quantum states (circuits) are inherently hard to sample classically, QGM represents an excellent …","date":1667030093,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667030093,"objectID":"4e6d6305089bb3f03343b938b120491d","permalink":"https://example.com/newsletter/octorber-2022/","publishdate":"2022-10-29T14:54:53+07:00","relpermalink":"/newsletter/octorber-2022/","section":"newsletter","summary":"News 📰 Determinable and interpretable network representation for link prediction\nAzure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students\nPenn State researchers to explore using quantum computers to design new drugs","tags":[],"title":"Octorber 2022","type":"newsletter"},{"authors":[],"categories":[],"content":"Trong bài này, mình đi qua một phương pháp xử lý bài toán nearest neighbour bằng thuật toán quantum. Bài viết dưới đây sẽ dựa vào bài báo gốc: Implementing a distance-based classifier with a quantum interference circuit, nếu ai muốn tìm hiểu sâu hơn về ý tưởng này thì có thể ghé qua.\nNội dung Squared-Distance Classifier Quantum Squared-Distance Classifier Kết luận Source Code Squared-Distance Classifier Ở đây, mình xét ví dụ bài toán phân loại tập data Titanic. Giả sử tập data được biểu diễn dưới dạng:\n$$ \\mathcal{D} = \\Big\\{ ({\\bf{x}}^1, y^1), \\ldots ({\\bf{x}}^M , y^M) \\Big\\}, $$ trong đó các véc-tơ đầu vào 2 chiều: ${ {\\bf{x}}^m = ({x_0}^m, {x_1}^m)^T}, m = 1,2,…,M$ tượng trưng cho một hành khách trên chuyến tàu Titanic đã bị nhấn chìm vào năm 1912. Trong đó $x_0$ là giá vé trong khoảng từ 0 đến 10,000 đô la, và $x_1$ là số hiệu cabin trong khoảng từ 1 đến 2,500. Ứng với mỗi một véc-tơ đầu vào là nhãn $y^m = {0,1}$ tương ứng để chỉ ra hành khách đó đã sống sót hay không.\nSouce Nếu từng tìm hiểu qua về Machine Learning, chắc hẳn các bạn đã nghe hoặc đọc qua về thuật toán nearest neighbour: với mỗi véc-tơ đầu vào mới, thì nhãn của nó sẽ được quyết định bởi điểm dữ liệu gần nhất với nó. Có nhiều cách để xác định những điểm dữ liệu gần nhất đó nhưng phổ biến là Euclidean distance. Do vậy, ta có cách tính hệ số cho việc gán nhãn véc-tơ $\\tilde{x}$ mới theo nhãn của $x^m$: $$ \\gamma_m = 1-\\frac{1}{c}|\\tilde{{\\bf x}}-{\\bf x}^m|^2, $$ trong đó $c$ là hằng số. Hệ số càng cao chứng tỏ $\\tilde{{\\bf x}}$ càng gần $x^m$. Gọi $\\tilde{y}$ là nhãn được gán cho $\\tilde{{\\bf x}}$, ta có xác xuất $p_{\\tilde{{\\bf x}}}(\\tilde{y}=1)$ là tổng trung bình hệ số của $M_1$ điểm dữ liệu mà có nhãn là $1$: $$ p_{\\tilde{{\\bf x}}}(\\tilde{y}=1) = \\frac{1}{\\chi}\\frac{1}{M_1} \\sum_{m|y^m=1}(1-\\frac{1}{c}|\\tilde{{\\bf x}}-{\\bf x}^m|^2) $$ Tương tự như vậy, $p_{\\tilde{{\\bf x}}}(\\tilde{y}=0)$ là tổng trung bình hệ số của các điểm dữ liệu mà có nhãn là $0$. Trong đó $\\frac{1}{\\chi}$ là normalizing factor sao cho $p_{\\tilde{{\\bf x}}}(\\tilde{y}=0)+p_{\\tilde{{\\bf x}}}(\\tilde{y}=1)=1$.\nSouce Áp dụng phương pháp ta thấy Passenger 3 sẽ gần với Passenger 1 hơn so với Passenger 2 (Fig 1.2), và mô hình sẽ đưa ra dự đoán là $1$ tương ứng với survival.\nQuantum Squared-Distance Classifier Giờ hãy xử lý bài toàn này bằng phương pháp ‘quantum’.\nBước 1: Data preprocessing and Encoding Đầu tiên chúng ta sẽ đi tới một câu hỏi kinh điển “Làm sao có thể biểu diễn dữ liệu trên máy tính lượng tử?”. Nếu như trên máy tính truyền thống các thông tin như ảnh sẽ thường được biển diễn trên không gian RBG có giá trị từ 0 đến 255, hay chúng ta có Word Embedding để biểu thông tin dạng văn bản thành các véc-tơ, thì vấn đề của các thuật toán lượng tử cũng như vậy. Thực chất, chủ đề về việc mã hóa thông tin trên không gian lượng tử (Quantum Embedding) vẫn đang được cộng đồng nghiên cứu quan tâm đặc biệt trong lĩnh vực Quantum Machine Learning. Nếu các bạn quan tâm đến chủ đề này có thể xem qua paper này để biết rõ hơn về các cách mã thông tin trong QML và sự quan trọng của nó. Mình sẽ làm một bài viết chi tiết hơn về chủ đề này trong tương lai.\nQuay lại với bài toán của chúng ta, mình sẽ áp dụng một phương pháp gọi là Amplitude Embedding - một phương pháp rất phố biến trong QML: Cho $X \\in \\mathbb{R}^N$ là một véc-tơ đơn nhất ($||X|| = 1$), ta có thể mã hóa $X$ bằng $n$ qubits dưới dạng: $$ \\ket{\\psi_X} = \\sum_{i=0}^{N-1}x_i \\ket{i}, $$ trong đó $n = \\log{N}$. Có thể thấy phương pháp này chỉ tốn $O(\\log{N})$ qubits để biểu diễn một véc-tơ $N$ chiều. Hãy lấy ví dụ trong bài toán xử lý ngôn ngữ tự nhiên, giả sử bạn có một text corpus với 10000 từ thì nếu như cách thông thường ta sử dụng One-hot encoding ta sẽ cần tới 10000 bits để mã hóa, nhưng điều này hoàn toán có thể giải quyết với 14 ($\\lceil \\log{10000} \\rceil$) qubits với amplitude embedding.\nTừ đây, với mỗi đầu vào $\\ket{\\psi_{\\bf\\tilde{x}}}$ mới, bài toán sẽ được khởi tạo dưới dạng:\nTrong đó $\\ket{m}$ và $\\ket{y^m}$ mã hóa cho số thứ tự và nhãn tương ứng của véc-tơ đầu vào thứ $m^{th}$. Tuy nhiên điều chú ý ở đây nằm ở ancilla qubit được kết nối với $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ và $\\ket{\\psi_{\\bf{x}^m}}$. Chú ý rằng khi ta thực hiện phép do trên một hoặc một hệ qubits thì qubit(s) sẽ bị collapsed hay terminated. Vậy khi ta chuyển phép đo của $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ hay $\\ket{\\psi_{\\bf{x}^m}}$ sang phép đo của một ancilla qubit được kết nối với chúng thì ta vẫn thu được kết quả cần thiết của $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ và $\\ket{\\psi_{\\bf{x}^m}}$ mà không cần chấm dứt (terminate) cả hệ thống. Kỹ thuật này rất hay sử dụng ở trong các thuật toán lượng tử và việc kết nối giữa ancilla qubit với hệ thống là chúng ta đang tạo ra entanglement - một tính chất quan trọng khác trong lĩnh vực tính toán lượng tử.\nNhư vậy với ví dụ Titanic trên ta có: $$ \\ket{\\mathcal{D}} = \\frac{1}{\\sqrt{4}} \\Big\\{\\ket{0}[\\ket{0}(0.866\\ket{0}+0.5\\ket{1})+\\ket{1}(0.921\\ket{0}+0.39\\ket{1})]\\ket{1} $$ $$ + …","date":1665505552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665505552,"objectID":"41280be87b10845fb59ca0bce349f39e","permalink":"https://example.com/post/example-1/","publishdate":"2022-10-11T23:25:52+07:00","relpermalink":"/post/example-1/","section":"post","summary":"Trong bài này, mình đi qua một phương pháp xử lý bài toán nearest neighbour bằng thuật toán quantum. Bài viết dưới đây sẽ dựa vào bài báo gốc: Implementing a distance-based classifier with a quantum interference circuit, nếu ai muốn tìm hiểu sâu hơn về ý tưởng này thì có thể ghé qua.","tags":[],"title":"Bài 2: Quantum Squared-Distance Classifier","type":"post"},{"authors":[],"categories":[],"content":"Nội dung Giới thiệu bài toán Classical Interference Quantum Interference Kết luận Trước khi đi vào các thuật toán quantum trong học máy, mình muốn so sánh bài toán suy luận xác suất trên máy tính truyền thống cũng như trên máy tính lượng tử. Bài viết này sẽ giúp các bạn viết qua những thành phần cơ bản của vật lý lượng tử: qubits, unitary transformation, và measurement và cách hoạt động của chúng thông qua một ví dụ cụ thể.\nGiới thiệu bài toán Cho hai đồng xu đồng chất: $c_1$ và $c_2$ với xác suất ở mặt sấp (tail) hay mặt ngửa (head) là như nhau. Không gian mẫu của việc tung 2 đồng xu trên sẽ bao gồm: (head, head), (head, tail), (tail, head), và (tail, tail). Mình sẽ xét bài toán như sau: Bước 1, ta lật 2 đồng xu thành mặt ngửa (head sẽ là giá trị ban đầu của 2 đống xu). Bước 2, ta tung đồng xu thứ nhất $c_1$ và kiểm tra kết quả. Và bước 3, ta cũng lại tung đồng xu $c_1$ lần thứ hai và kiểm tra kết quả (giả sử kết quả thu được là sau số lần thử đủ lớn).\nClassical Interference (Suy luận xác suất truyền thống) Có thể thấy với máy tính truyền thống (classical computer), 2 đồng xu có thể được coi là 2 bits ngẫu nhiên (random bits). Ở bước 1, bài toán sẽ đưa về kết quả (head, head). Tuy nhiên, sau bước hai thì (head, head) và (tail, head) sẽ có xác suất bằng nhau và bằng 0.5. Phân phối này sẽ không thay đổi sau bước 3.\nQuantum Interference (Suy luận xác suất trên máy tính quantum) Tuy nhiên, ở đây sẽ có một chút khác biệt nếu ta biểu diễn bài toán trên máy tính quantum. Hai đồng xu sẽ được biểu diễn bằng hai qubits (quantum bits). Mỗi qubit có dạng $\\alpha \\ket{0} + \\beta \\ket{1}$, trong đó $\\ket{0}$ và $\\ket{1}$ là hai trạng thái cơ sở (basis state) được biểu diễn dưới dạng véc-tơ tương ứng: $[1,0]^T$ và $[0,1]^T$. Có thể thấy rằng $\\ket{0}$ và $\\ket{1}$ tương ứng với hai giá trị nhị phân 0, 1 ở máy tính truyền thống; tuy nhiên, thay vì được mã hóa rời rạc thành chuỗi bit 1 hoặc 0, mỗi qubit có thể tạo thành một tổ hợp tuyến tính của các trạng thái cơ sở theo xác suất: $$ p(0) = |\\alpha|^2, p(1) = |\\beta|^2; \\alpha, \\beta \\in \\mathbb{C} $$ Chú ý rằng $|\\alpha|^2 + |\\beta|^2 = 1$ để thỏa mãn xác suất trên. Như vậy nếu ta coi hai mặt của đồng xu là hai trạng thái cơ sở: $\\ket{head} = \\ket{0}$ và $\\ket{tail} = \\ket{1}$, thì việc tung đồng xu sẽ tương đương việc chúng ta thực hiện biến đổi Hadamard. Việc biến đổi ở đây trên máy tính quantum chính là phép nhân ma trận Hadamard với trạng thái hiện tại của qubit. Trong đó biến đổi Hadamard có thể được biểu diễn dưới dạng ma trận:\n$$ H = \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{cc} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{array} \\right) $$ Từ đó, ta có thể triển khai bài toán trên như sau: 2 qubits sẽ được khởi tạo thành $\\ket{head}\\ket{head}$ sau bước 1. Ở bước 2, ta nhân ma trận Hadamard với trạng thái của qubit thứ nhất, ta có: $$ H\\ket{head}\\ket{head}= H\\ket{0}\\ket{head} = \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{cc} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right) \\ket{head} $$ $$ =\\!\\frac{1}{\\sqrt{2}}\\left( \\begin{array}{c} 1 \\\\ 1 \\end{array} \\right)\\! \\ket{head}\\! =\\! \\frac{1}{\\sqrt{2}} (\\ket{0}\\!+\\!\\ket{1})\\!\\ket{head}\\! =\\! \\frac{1}{\\sqrt{2}} (\\ket{head}\\!+\\!\\ket{tail})\\!\\ket{head} $$ $$ = \\frac{1}{\\sqrt{2}}\\ket{head}\\ket{head} + \\frac{1}{\\sqrt{2}}\\ket{tail}\\ket{head} $$ Như vậy, ta thấy giống như trường hợp trên, sau bước 2 xác suất đạt được $\\ket{head}\\ket{head}$ và $\\ket{tail}\\ket{head}$ là bằng nhau là cũng bằng $(\\frac{1}{\\sqrt{2}})^2 = 0.5$. Tuy nhiên sự khác biệt nằm ở bước 3, nếu ta tiếp tục tung đồng xu thứ nhất (hay thực hiện biến đổi Hadamard), với một chút tính toán ta nhận được kết quả:\n$$ \\frac{1}{\\sqrt{2}}H\\ket{head}\\ket{head} + \\frac{1}{\\sqrt{2}}H\\ket{tail}\\ket{head} = \\ket{head}\\ket{head} $$ Sau bước 3 ta sẽ luôn nhận được $\\ket{head}\\ket{head}$, kết quả này khác hoàn toán khi thực hiện bài toán trên máy tính truyền thống. Có thể nói đây là một sự khác nhau thú vị giữa máy tính lượng tử và máy tính truyền thống. Khác với máy tính truyền thống có xu hướng tối đa hóa sự không chắc chắn (maximize uncertainty) vì luôn cho ra kết quả 50-50 giữa 2 trạng thái (head, head) và (tail, head), thì máy tính lượng tử cho ra kết quả có độ không chắc chắn thấp hơn. Chính vì lý do này, đã có nghiên cứu áp dụng quantum inference như một hàm dự đoán (prediction function) trong bài toán học máy có giám sát (supervised learning) [1]\nSự khác nhau trên cũng dẫn ta tới một vấn đề quan trọng khác: measurement (phép đo). Phép đo chính là cầu nối giữa quantum và classical, giúp chúng ta đánh giá và phân tích trạng thái hiện tại của một hoặc một hệ qubit, và nó thường mang tính thống kê xác suất hơn là một đánh giá đơn lẻ. Nói cách khác, phép đo cho phép chúng ta phá bỏ tính chống chât của qubits (phá bỏ đi tổ hợp tuyến tính), từ đó làm cho trạng thái của qubit ‘collapse’ về một trong các trạng thái cơ sở. Ví dụ, thực hiện phép đo một qubit bất kỳ $\\ket{\\phi} = \\alpha \\ket{0} + \\beta \\ket{1}$ trên cơ sở chuẩn (canonical basis) thì ta sẽ đạt được giá trị 0 …","date":1665301286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665301286,"objectID":"e81b5d40081a3db7ff1a08d56238a88b","permalink":"https://example.com/post/fair-coins/","publishdate":"2022-10-09T14:41:26+07:00","relpermalink":"/post/fair-coins/","section":"post","summary":"Nội dung Giới thiệu bài toán Classical Interference Quantum Interference Kết luận Trước khi đi vào các thuật toán quantum trong học máy, mình muốn so sánh bài toán suy luận xác suất trên máy tính truyền thống cũng như trên máy tính lượng tử.","tags":[],"title":"Bài 1: Quantum vs Classical Interference: First Example","type":"post"},{"authors":[],"categories":[],"content":"Nội dung Quantum Machine Learning là gì? Một vài hướng tiếp cận của Quantum Machine Learning Kết luận Khác với các chủ đề của Machine Learning hay Deep Learning khi mà các ứng dụng của chúng đang dần trở nên phổ biến những năm gần đây, chủ đề về Quantum Machine Learning (hay QML) là một lĩnh vực nghiên cứu mới và đang được chú ý ở các công ty hàng đầu thế giới như IBM hay Google. Do đó, ở bài viết này ngoài việc cung cấp cho bạn đọc cái nhìn cụ thể Quantum Machine Learning là gì, mình cũng sẽ giải thích tại sao chúng ta lại cần QML và một vài hướng tiếp cận cụ thể.\nQuantum Machine Learning là gì? Nếu như ai đã làm quen với các bài toán của Machine Learning hay Deep Learning, các mô hình đang dần được xây dựng lớn hơn và phức tạp hơn để giải quyết các bài toán khó (hard combinatorial optimization problems), nó dẫn tới việc tiêu tốn rất nhiều tài nguyên tính toán (computational resources) trong việc huấn luyện cũng như là vận hành. Một ví dụ điển hình là mô hình GPT-3 gồm 175 tỷ tham số sẽ cần tốn 355 năm và gần 5 triệu đô nếu train trên một NVIDIA Tesla V100 GPU. Do đó, trên thực tế họ đã train GPT-3 với 1024 A100 GPUs và mất 34 ngày.\nTuy nhiên, vấn đề đó có thể sẽ được giải quyết với sự xuất hiện của máy tính lượng tử (quantum computer). Máy tính lượng tử được phát triển dựa theo các thuyết của vật lý lượng tử để đưa ra một khả năng tính toán vượt trội so với máy tính truyền thống. Hãy lấy một bài toán tìm kiếm là một ví dụ: giả sử bạn phải tìm 1 quả bóng trong 1 triệu ngăn kéo và câu hỏi là bạn sẽ phải mở qua bao nhiêu ngăn kéo trước khi tìm được quả bóng đó? Đôi khi bạn sẽ may mắn tìm được quả bóng trong chỉ vài lần thử và ngược lại bạn cũng có thể phải mở gần như toàn bộ 1 triệu ngăn kéo kia. Trung bình bạn sẽ cần tới 500,000 lượt để tìm ra quả bóng. Tuy nhiên, với máy tính lượng tử, bạn có thể thực hiện bài toán đó trong vòng 1000 lượt bằng một thuật toán được gọi là Grover’s algorithm.\nTừ đó sự ra đời của Quantum Machine Learning như một sự giao thoa của các thuật toán trên máy tính lượng tử với mô hình Machine Learning để cải thiện cả về mặt tính toán cũng như độ chính xác (được gọi là quantum advantage).\nMột vài hướng tiếp cận của Quantum Machine Learning Cho đến này đã có khá nhiều hướng triển khai QML được đề xuất, mặc dù nhiều trong số chúng vẫn chỉ là lý thuyết thuần túy và cần một máy tính lượng tử hoàn chỉnh để thực nghiệm; tuy nhiên, cũng đã có các thuật toán đã được triển khai trên quy mô nhỏ và chứng minh đạt được ‘quantum advantage’. Sau đây mình sẽ đề cập tới hai hướng tiệp cận phổ biến của QML.\na) QRAM-based Quantum Machine Learning\nTương tự RAM (Random Access Memory) ở các máy tính truyền thống, các nhà nghiên cứu đã giới thiệu một ‘quantum-version’ của RAM được gọi là QRAM để xử lý vấn đề ghi và đọc thông tin trên máy tính lượng tử. Có thể nói QRAM là một phần rất quan trọng nhiều thuật toán của QML. Thậm chí chúng đạt được ‘quantum advantage’ là nhờ QRAM.\nMột ứng dụng cụ thể và cũng như được dùng nhiều nhất của QRAM là khả năng cải thiện tốc độ tính toán của tích vô hướng (dot product) hay Kernel Method - một phương pháp quen thuộc của Machine Learning mà điển hình là Support Vector Machine (SVM). Với sự can thiệp của QRAM, ta có thể tính tích vô hướng $x^Ty$ với độ phức tạp là $O(logN)$ so với $O(N)$ trên máy tính truyền thống, trong đó $x, y$ là các vectors $N$ chiều.\nTừ đó các thuật toán được ra đời như là Quantum K-Means dựa vào QRAM để có độ phức tạp $O(log(Nd))$ (so với $O(Nd)$ của thuật toán K-Means), trong đó $N$ là số data và $d$ là số chiều. Hay Quantum Support Vector Machine đạt được độ phức tạp $O(log(Nd))$ so với $O(poly(N,d))$ của thuật toán SVM bình thường, và một số khác: Quantum PCA, Quantum K-Medians, etc.\nỞ hướng tiếp cận này, các thuật toán sẽ dựa vào khả năng tính toán vượt trội của quantum computing để cải thiện độ phức tạp. Tuy nhiên ở máy tính lượng tử không chỉ có vậy. Thông tin ở đó được biểu diễn dựa theo nguyên lý chồng chập (Superposition), thay vì được mã hóa rời rạc thành các bits 0 và 1, có nghĩa thông tin có thể tồn tại đồng thời ở bit 0 và bit 1 theo một phân phối nào đó. Do đó, ’learning space’ ở máy tính lượng tử sẽ hoàn toàn khác và thậm chí được mở rộng hơn so với máy tính truyền thống. Thực tế đã có nhiều nghiên cứu với mục tiêu khám phá không gian này để cải thiện khả năng học tập (learning capability) của mô hình Machine Learning và hướng tiếp cận sau đây là ví dụ điển hình cho việc này.\nb) Quantum Neural Network.\nĐược thúc đẩy từ sự thành công của mạng học sâu (classical deep learning), mạng nơ-ron lượng tử (Quantum Neural Network, hay QNN) cũng mang những nét tương đồng với mạng nơ-ron truyền thống (NN). Chúng được thiết kế theo cấu trúc feed-forward, trong đó các layers là các phép biến đổi đơn nhất (unitary transformation). Hầu hết cấu trúc của QNN dựa theo Variational Quantum Circuits hay thường được gọi Parameterised Quantum Circuits. Ở đó các biến đổi trong cấu trúc mạng QNN sẽ phụ thuộc vào tham số $\\theta$ (learning parameters) và chúng …","date":1664552270,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664552270,"objectID":"db5dfd9db2c98204b3c7e2f0bf0ffcb8","permalink":"https://example.com/post/why-qml/","publishdate":"2022-09-30T22:37:50+07:00","relpermalink":"/post/why-qml/","section":"post","summary":"Nội dung Quantum Machine Learning là gì? Một vài hướng tiếp cận của Quantum Machine Learning Kết luận Khác với các chủ đề của Machine Learning hay Deep Learning khi mà các ứng dụng của chúng đang dần trở nên phổ biến những năm gần đây, chủ đề về Quantum Machine Learning (hay QML) là một lĩnh vực nghiên cứu mới và đang được chú ý ở các công ty hàng đầu thế giới như IBM hay Google.","tags":[],"title":"Bài 0: Giới thiệu về Quantum Machine Learning","type":"post"},{"authors":null,"categories":null,"content":"Chào mọi người, cảm ơn mọi người ghé qua trang blog của mình.\nMình là Nguyễn Quang Tuyến, mình tốt nghiệp đại học nghành Khoa học máy tính tại trường University of Aizu, Nhật Bản. Trong thời gian làm đồ án tốt nghiệp liên quan đến Quantum Machine Learning (mình tạm dịch là Học Máy Lượng Tử), mình đã có mong muốn làm ra một blog cá nhân viết về lĩnh vực này. Do đó, dựa theo hai nguồn cảm hứng từ trang Machine Learning cơ bản của anh Vũ Hữu Tiệp và Deep Learning cơ bản của anh Tuấn, mình tạo trang này với hai kỳ vọng. Một là giúp mình tổng hợp kiến thức về Quantum Machine Learning cũng giúp mình nắm chắc nền móng trong giai đoạn sơ khai của lĩnh vực này. Hai là chia sẻ cũng như mong muốn tạo được một cộng đồng các bạn đọc Việt Nam tiếp cận tới một lĩnh vực mới nhưng cũng đầy triển vọng này.\nTrong quá trình chuẩn bị cũng như là viết bài, mình sẽ cố gắng ít động chạm tới các hiện tượng vĩ mô ở trong thế giới lượng tử và mình sẽ đi từ góc nhìn của một computer scientist chứ không phải là một physicist. Nhưng mình hi vọng mọi người đã làm quen qua các kiến thức cơ bản của Đại Số Tuyến Tính (vì hầu hết các biến đổi trong máy tính lượng tử là tuyến tính) và kỹ năng lập trình Python (nếu có thể mình sẽ cố đưa ra các code demo sau các bài viết để giúp mọi người hiểu rõ được vấn đề hơn).\nNgoài các bài viết ở trên Blogs, Newsletter là nơi mình cập nhật những thông tin mới nhất hàng tháng liên quan tới Quantum Machine Learning (các bài báo mới, hội nghị, workshop, etc.)\nMình vui lòng tiếp nhận mọi ý kiến thảo luận của mọi người qua email quantummachinelearning.vietnam@gmail.com\nMình xin chân thành cảm ơn.\n","date":1664496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664496000,"objectID":"ed769a13ba9f25479e7927567f68c77f","permalink":"https://example.com/authors/intro/","publishdate":"2022-09-30T00:00:00Z","relpermalink":"/authors/intro/","section":"authors","summary":"Here we describe how to add a page to your site.","tags":null,"title":"About","type":"authors"}]