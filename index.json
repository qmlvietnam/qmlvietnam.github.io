
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"ChÃ o má»i ngÆ°á»i, cáº£m Æ¡n má»i ngÆ°á»i ghÃ© qua trang blog cá»§a mÃ¬nh.\nMÃ¬nh lÃ  Nguyá»…n Quang Tuyáº¿n, mÃ¬nh tá»‘t nghiá»‡p Ä‘áº¡i há»c nghÃ nh Khoa há»c mÃ¡y tÃ­nh táº¡i trÆ°á»ng University of Aizu, Nháº­t Báº£n. Trong thá»i gian lÃ m Ä‘á»“ Ã¡n tá»‘t nghiá»‡p liÃªn quan Ä‘áº¿n Quantum Machine Learning (mÃ¬nh táº¡m dá»‹ch lÃ  Há»c MÃ¡y LÆ°á»£ng Tá»­), mÃ¬nh Ä‘Ã£ cÃ³ mong muá»‘n lÃ m ra má»™t blog cÃ¡ nhÃ¢n viáº¿t vá» lÄ©nh vá»±c nÃ y. Do Ä‘Ã³, dá»±a theo hai nguá»“n cáº£m há»©ng tá»« trang Machine Learning cÆ¡ báº£n cá»§a anh VÅ© Há»¯u Tiá»‡p vÃ  Deep Learning cÆ¡ báº£n cá»§a anh Tuáº¥n, mÃ¬nh táº¡o trang nÃ y vá»›i hai ká»³ vá»ng. Má»™t lÃ  giÃºp mÃ¬nh tá»•ng há»£p kiáº¿n thá»©c vá» Quantum Machine Learning cÅ©ng giÃºp mÃ¬nh náº¯m cháº¯c ná»n mÃ³ng trong giai Ä‘oáº¡n sÆ¡ khai cá»§a lÄ©nh vá»±c nÃ y. Hai lÃ  chia sáº» cÅ©ng nhÆ° mong muá»‘n táº¡o Ä‘Æ°á»£c má»™t cá»™ng Ä‘á»“ng cÃ¡c báº¡n Ä‘á»c Viá»‡t Nam tiáº¿p cáº­n tá»›i má»™t lÄ©nh vá»±c má»›i nhÆ°ng cÅ©ng Ä‘áº§y triá»ƒn vá»ng nÃ y.\nTrong quÃ¡ trÃ¬nh chuáº©n bá»‹ cÅ©ng nhÆ° lÃ  viáº¿t bÃ i, mÃ¬nh sáº½ cá»‘ gáº¯ng Ã­t Ä‘á»™ng cháº¡m tá»›i cÃ¡c hiá»‡n tÆ°á»£ng vÄ© mÃ´ á»Ÿ trong tháº¿ giá»›i lÆ°á»£ng tá»­ vÃ  mÃ¬nh sáº½ Ä‘i tá»« gÃ³c nhÃ¬n cá»§a má»™t computer scientist chá»© khÃ´ng pháº£i lÃ  má»™t physicist. NhÆ°ng mÃ¬nh hi vá»ng má»i ngÆ°á»i Ä‘Ã£ lÃ m quen qua cÃ¡c kiáº¿n thá»©c cÆ¡ báº£n cá»§a Äáº¡i Sá»‘ Tuyáº¿n TÃ­nh (vÃ¬ háº§u háº¿t cÃ¡c biáº¿n Ä‘á»•i trong mÃ¡y tÃ­nh lÆ°á»£ng tá»­ lÃ  tuyáº¿n tÃ­nh) vÃ  ká»¹ nÄƒng láº­p trÃ¬nh Python (náº¿u cÃ³ thá»ƒ mÃ¬nh sáº½ cá»‘ Ä‘Æ°a ra cÃ¡c code demo sau cÃ¡c bÃ i viáº¿t Ä‘á»ƒ giÃºp má»i ngÆ°á»i hiá»ƒu rÃµ Ä‘Æ°á»£c váº¥n Ä‘á» hÆ¡n).\nNgoÃ i cÃ¡c bÃ i viáº¿t á»Ÿ trÃªn Blogs, Newsletter lÃ  nÆ¡i mÃ¬nh cáº­p nháº­t nhá»¯ng thÃ´ng tin má»›i nháº¥t hÃ ng thÃ¡ng liÃªn quan tá»›i Quantum Machine Learning (cÃ¡c bÃ i bÃ¡o má»›i, há»™i nghá»‹, workshop, etc.)\nMÃ¬nh vui lÃ²ng tiáº¿p nháº­n má»i Ã½ kiáº¿n tháº£o luáº­n cá»§a má»i ngÆ°á»i qua email quantummachinelearning.vietnam@gmail.com\nMÃ¬nh xin chÃ¢n thÃ nh cáº£m Æ¡n.\n","date":1664496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664496000,"objectID":"ed769a13ba9f25479e7927567f68c77f","permalink":"https://example.com/authors/intro/","publishdate":"2022-09-30T00:00:00Z","relpermalink":"/authors/intro/","section":"authors","summary":"Here we describe how to add a page to your site.","tags":null,"title":"QML Vietnam","type":"authors"},{"authors":[],"categories":[],"content":"News ğŸ“° D-Wave Says Revenue Up, Added More Commercial Customers in Q3\nIonQ and Hyundai Motors Expand Quantum Computing Partnership, Continuing Pursuit of Automotive Innovation\nQNLP at The Beeb â€” Quantinuum, BBC Join Consortium to Explore Real-World Uses for Quantum Natural Language Processing\nUnileverâ€™s Head of R\u0026amp;D Alberto Prado Talks Quantum at IoT World \u0026amp; The AI Summit Austin\nHackathon and Internship ğŸ“ 2023 Quantum Computing Summer School Fellowship provided by Los Alamos National Laboratory\n2023 Zapata Quantum Computing Internship\n2023 IBM Quantum Summer Internship\nChicago Quantum Exchange Intership\nPublications ğŸ“ƒ Reservoir Computing via Quantum Recurrent Neural Networks Abstract: Recent developments in quantum computing and machine learning have propelled the interdisciplinary study of quantum machine learning. Sequential modeling is an important task with high scientific and commercial value. Existing VQC or QNN-based methods require significant computational resources to perform the gradient-based optimization of a larger number of quantum circuit parameters. The major drawback is that such quantum gradient calculation requires a large amount of circuit evaluation, posing challenges in current near-term quantum hardware and simulation software. In this work, we approach sequential modeling by applying a reservoir computing (RC) framework to quantum recurrent neural networks (QRNN-RC) that are based on classical RNN, LSTM and GRU. The main idea to this RC approach is that the QRNN with randomly initialized weights is treated as a dynamical system and only the final classical linear layer is trained. Our numerical simulations show that the QRNN-RC can reach results comparable to fully trained QRNN models for several function approximation and time series prediction tasks. Since the QRNN training complexity is significantly reduced, the proposed model trains notably faster. In this work we also compare to corresponding classical RNN-based RC implementations and show that the quantum version learns faster by requiring fewer training epochs in most cases. Our results demonstrate a new possibility to utilize quantum neural network for sequential modeling with greater quantum hardware efficiency, an important design consideration for noisy intermediate-scale quantum (NISQ) computers.\nBenchmarking Adversarially Robust Quantum Machine Learning at Scale Abstract: Machine learning (ML) methods such as artificial neural networks are rapidly becoming ubiquitous in modern science, technology and industry. Despite their accuracy and sophistication, neural networks can be easily fooled by carefully designed malicious inputs known as adversarial attacks. While such vulnerabilities remain a serious challenge for classical neural networks, the extent of their existence is not fully understood in the quantum ML setting. In this work, we benchmark the robustness of quantum ML networks, such as quantum variational classifiers (QVC), at scale by performing rigorous training for both simple and complex image datasets and through a variety of high-end adversarial attacks. Our results show that QVCs offer a notably enhanced robustness against classical adversarial attacks by learning features which are not detected by the classical neural networks, indicating a possible quantum advantage for ML tasks. Contrarily, and remarkably, the converse is not true, with attacks on quantum networks also capable of deceiving classical neural networks. By combining quantum and classical network outcomes, we propose a novel adversarial attack detection technology. Traditionally quantum advantage in ML systems has been sought through increased accuracy or algorithmic speed-up, but our work has revealed the potential for a new kind of quantum advantage through superior robustness of ML models, whose practical realisation will address serious security concerns and reliability issues of ML algorithms employed in a myriad of applications including autonomous vehicles, cybersecurity, and surveillance robotic systems.\nQuantum Feature Maps for Graph Machine Learning on a Neutral Atom Quantum Processor Abstract: Using a quantum processor to embed and process classical data enables the generation of correlations between variables that are inefficient to represent through classical computation. A fundamental question is whether these correlations could be harnessed to enhance learning performances on real datasets. Here, we report the use of a neutral atom quantum processor comprising up to 32 qubits to implement machine learning tasks on graph-structured data. To that end, we introduce a quantum feature map to encode the information about graphs in the parameters of a tunable Hamiltonian acting on an array of qubits. Using this tool, we first show that interactions in the quantum system can be used to distinguish non-isomorphic graphs that are locally equivalent. We then realize a toxicity screening experiment, consisting of a binary classification â€¦","date":1670676790,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670676790,"objectID":"9297d9e12cf9ee548cfc0a557c349bb8","permalink":"https://example.com/newsletter/november-2022/","publishdate":"2022-12-10T19:53:10+07:00","relpermalink":"/newsletter/november-2022/","section":"newsletter","summary":"News ğŸ“° D-Wave Says Revenue Up, Added More Commercial Customers in Q3\nIonQ and Hyundai Motors Expand Quantum Computing Partnership, Continuing Pursuit of Automotive Innovation\nQNLP at The Beeb â€” Quantinuum, BBC Join Consortium to Explore Real-World Uses for Quantum Natural Language Processing","tags":[],"title":"November 2022","type":"newsletter"},{"authors":[],"categories":[],"content":" Cuá»‘i cÃ¹ng trong bá»™ 3 bÃ i viáº¿t vá» cÃ¡c subroutines cá»§a thuáº­t toÃ¡n lÆ°á»£ng tá»­, mÃ¬nh muá»‘n Ä‘á» cáº­p tá»›i thuáº­t toÃ¡n Harrow-Hassidim-Lloyd (hay HHL) Ä‘Æ°á»£c Ä‘áº·t theo tÃªn cá»§a ba tÃ¡c giáº£ Ä‘Ã£ giá»›i thiá»‡u thuáº­t toÃ¡n 1. Thuáº­t toÃ¡n HHL Ä‘Æ°á»£c phÃ¡t triá»ƒn nháº±m giáº£i quyáº¿t phÆ°Æ¡ng trÃ¬nh hoáº·c há»‡ phÆ°Æ¡ng trÃ¬nh tuyáº¿n tÃ­nh báº±ng mÃ¡y tÃ­nh lÆ°á»£ng tá»­, hay nÃ³i cÃ¡ch khÃ¡c thuáº­t toÃ¡n sáº½ giáº£i phÆ°Æ¡ng trÃ¬nh:\n$$ Ax = b $$ Theo phÆ°Æ¡ng phÃ¡p truyá»n thá»‘ng, phÆ°Æ¡ng trÃ¬nh trÃªn cÃ³ thá»ƒ Ä‘Æ°á»£c giáº£i báº±ng cÃ¡ch nghá»‹ch Ä‘áº£o ma tráº­n $A$ vÃ  cÃ³ káº¿t quáº£ lÃ  $x=A^{-1}b$. Tuy nhiÃªn, viá»‡c tÃ­nh toÃ¡n ma tráº­n nghá»‹ch Ä‘áº£o tá»‘n ráº¥t nhiá»u tÃ i nguyÃªn khi ma tráº­n $A$ lá»›n. Vá»›i má»™t cÃ¡ch phá»‘ biáº¿n nháº¥t nhÆ° lÃ  Gaussian emilination, ta cáº§n tá»›i Ä‘á»™ phá»©c táº¡p $O(N^3)$ Ä‘á»ƒ nghá»‹ch Ä‘áº£o má»™t ma tráº­n $N\\times N$; hay tiÃªn tiáº¿n hÆ¡n vá»›i thuáº­t toÃ¡n conjugate gradient cÃ³ Ä‘á»™ phá»©c táº¡p $O(Nsk \\log{1/\\epsilon})$, á»Ÿ Ä‘Ã³ $s$ lÃ  tá»· lá»‡ pháº¥n tá»­ cÃ³ giÃ¡ trá»‹ $0$ trong ma tráº­n $A$ (sparsity proportion), $k$ lÃ  tá»· sá»‘ giá»¯a giÃ¡ trá»‹ riÃªng lá»›n nháº¥t vÃ  giÃ¡ trá»‹ riÃªng nhá» nháº¥t, vÃ  cuá»‘i cÃ¹ng lÃ  $\\epsilon$ kÃ½ hiá»‡u cho Ä‘á»™ sai sá»‘ cá»§a thuáº­t toÃ¡n. NhÆ°ng vá»›i HHL, thuáº­t toÃ¡n cÃ³ thá»ƒ xá»­ lÃ½ váº«n Ä‘á» ma tráº­n nghá»‹ch Ä‘áº£o vá»›i $O(\\log{(N)}s^2k^2/\\epsilon)$.\ná» Ä‘Ã¢y, mÃ¬nh sáº½ cÃ¹ng má»i ngÆ°á»i phÃ¢n tÃ­ch rÃµ hÆ¡n vá» thuáº­t toÃ¡n nÃ y.\nNá»™i dung Thuáº­t toÃ¡n HHL Source Code Thuáº­t toÃ¡n HHL Thuáº­t toÃ¡n HHL Ä‘áº·c biá»‡t quan trá»ng cho cÃ¡c bÃ i toÃ¡n cá»§a QML, vÃ¬ nhÆ° má»i ngÆ°á»i Ä‘Ã£ biáº¿t háº§u háº¿t cÃ¡c thuáº­t toÃ¡n machine learning Ä‘á»u Ä‘i há»c tham sá»‘ $\\theta$ theo phÆ°Æ¡ng trÃ¬nh $y = \\theta^{T}x$. Do Ä‘Ã³, ta hoÃ n toÃ n cÃ³ thá»ƒ Ã¡p dá»¥ng HHL trong viá»‡c tÃ¬m $\\theta$ trong cÃ¡c bÃ i toÃ¡n cá»§a machine learning.\nThá»±c cháº¥t váº¥n Ä‘á» lá»›n nháº¥t cá»§a HHL lÃ  lÃ m sao Ä‘Æ°a ma tráº­n $A$ cá»§a phÆ°Æ¡ng trÃ¬nh $Ax = b$ vÃ o mÃ¡y tÃ­nh lÆ°á»£ng tá»­. NhÆ° mÃ¬nh Ä‘Ã£ Ä‘á» cáº­p ráº¥t nhiá»u tá»« cÃ¡c bÃ i viáº¿t trÆ°á»›c, cÃ¡c phÃ©p biáº¿n Ä‘á»•i trong mÃ¡y tÃ­nh lÆ°á»£ng tá»­ pháº£i tá»« ma tráº­n Ä‘Æ¡n nháº¥t (unitary matrix). Do Ä‘Ã³, ta khÃ´ng thá»ƒ trá»±c tiáº¿p dÃ¹ng $A$ Ä‘á»ƒ biáº¿n Ä‘á»•i, nhÆ°ng may thay má»i ma tráº­n Ä‘Æ¡n nháº¥t Ä‘á»u cÃ³ thá»ƒ biá»ƒu diá»…n theo cÃ´ng thá»©c tá»•ng quÃ¡t: $U = e^{iHt}$, trong Ä‘Ã³ $H$ lÃ  ma tráº­n Hermitian, vÃ  $t$ biá»ƒu thá»‹ thá»i gian (má»i ngÆ°á»i cÃ³ thá»ƒ xem chá»©ng minh á»Ÿ Box 1).\nBox 1: Ma tráº­n $H$ Ä‘Æ°á»£c gá»i lÃ  Hermitian náº¿u $H = H^{\\dagger}$ vÃ  $U$ lÃ  má»™t ma tráº­n Ä‘Æ¡n nháº¥t khi $UU^{\\dagger} = U^{\\dagger}U = I$.\nDo Ä‘Ã³, ta cÃ³: $UU^{\\dagger} = e^{iHt}e^{-iH^{\\dagger}t} = e^{iHt-iHt} = I$. NhÆ° váº­y náº¿u $H$ lÃ  má»™t ma tráº­n Hermitian thÃ¬ $U = e^{iHt}$ lÃ  má»™t ma tráº­n Ä‘Æ¡n nháº¥t.\nMá»i ngÆ°á»i cÃ³ thá»ƒ Ä‘á»c chi tiáº¿t hÆ¡n lÃ½ do hÃ¬nh thÃ nh cá»§a cÃ´ng thá»©c trÃªn á»Ÿ Section 2.2.2 Nielson \u0026amp; Chuang.\nNhÆ° váº­y, ta hoÃ n toÃ n cÃ³ thá»ƒ sá»­ dá»¥ng má»™t ká»¹ thuáº­t nhá» Ä‘á»ƒ biáº¿n $A$ thÃ nh má»™t ma tráº­n Hermitian, $\\tilde{A}$, Ä‘á»ƒ cÃ³ má»™t ma tráº­n Ä‘Æ¡n nháº¥t $U = e^{i\\tilde{A}t}$. á» Ä‘Ã¢y, mÃ¬nh cÃ³ thá»ƒ biá»ƒu diá»…n $\\tilde{A}$ dÆ°á»›i dáº¡ng:\n$$ \\tilde{A} = \\left( \\begin{array}{cc} 0 \u0026amp; A^{\\dagger} \\\\ A \u0026amp; 0 \\end{array} \\right) $$ Dá»… dÃ ng cÃ³ thá»ƒ tháº¥y $\\tilde{A} =\\tilde{A}^{\\dagger}$ nÃªn $\\tilde{A}$ lÃ  má»™t ma tráº­n Hermitian. NhÆ° váº­y thay vÃ¬ trá»±c tiáº¿p giáº£i $x = A^{-1}b$, mÃ¬nh sáº½ Ä‘i giáº£i quyáº¿t bÃ i toÃ¡n $\\ket{x} = \\tilde{A}^{-1}\\ket{b}$\nMáº·t khÃ¡c, náº¿u ta phÃ¢n rÃ£ trá»‹ riÃªng cá»§a $\\tilde{A}$ ta Ä‘Æ°á»£c $V\\Lambda V^{-1}$ trong Ä‘Ã³ $V$ gá»“m cÃ¡c vÃ©c-tÆ¡ cá»™t lÃ  vÃ©c-tÆ¡ riÃªng cá»§a $\\tilde{A}$ vÃ  $\\Lambda$ cÃ³ cÃ¡c pháº§n tá»­ Ä‘Æ°á»ng chÃ©o lÃ  cÃ¡c giÃ¡ trá»‹ riÃªng cá»§a $\\tilde{A}$. MÃ  $\\tilde{A} = \\tilde{A}^{\\dagger}$ nÃªn dá»… dÃ ng cÃ³ thá»ƒ tháº¥y $VV^{\\dagger} = I$ nÃªn $V^{-1} = V^{\\dagger}$. Do Ä‘Ã³, ta cÃ³ thá»ƒ viáº¿t cÃ´ng thá»©c phÃ¢n rÃ£ trá»‹ riÃªng cá»§a $\\tilde{A}$ nhÆ° sau: $$ \\tilde{A} = \\sum_{i} \\lambda_i \\ket{v_i}\\bra{v_i} $$ $$ \\Rightarrow \\tilde{A}^{-1} = \\sum_{i} \\frac{1}{\\lambda_i} \\ket{v_i}\\bra{v_i} \\quad (1) $$ á» Ä‘Ã³, cÃ¡c vÃ©c-tÆ¡ riÃªng $\\ket{v_i}$ táº¡o thÃ nh há»‡ cÆ¡ sá»Ÿ trá»±c giao (orthonormal basis), hay $\\left\\langle v_i| v_i \\right\\rangle = 1$ vÃ  $\\left\\langle v_i| v_j \\right\\rangle = 0 $. NÃªn ta cÅ©ng cÃ³ thá»ƒ biá»ƒu diá»…n Ä‘Æ°á»£c $\\ket{b}$ theo $\\ket{v_i}$: $$ \\ket{b} = \\sum_{i} \\beta_i \\ket{v_i} \\quad (2) $$ Thay (1) vÃ  (2) vÃ o $\\ket{x} = \\tilde{A}^{-1}\\ket{b}$, ta Ä‘Æ°á»£c:\n$$ \\ket{x} = \\sum_{i} \\frac{1}{\\lambda_i} \\ket{v_i}\\bra{v_i} \\sum_{i} \\beta_i \\ket{v_i} $$ $$ = \\sum_{i} \\frac{\\beta_i}{\\lambda_i} \\ket{v_i} \\quad (3) $$ Tá»« káº¿t quáº£ trÃªn, cÃ³ thá»ƒ tháº¥y nhiá»‡m vá»¥ cá»§a thuáº­t toÃ¡n HHL sáº½ biáº¿n Ä‘á»•i Ä‘áº¿n tráº¡ng thÃ¡i $\\sum_{i} \\frac{\\beta_i}{\\lambda_i} \\ket{v_i}$. Sau Ä‘Ã¢y, mÃ¬nh sáº½ phÃ¢n tÃ­ch tá»«ng bÆ°á»›c má»™t trong nhá»¯ng bÆ°á»›c triá»ƒn khai cá»§a HHL.\nHÃ¬nh 1: Cáº¥u trÃºc máº¡ch cá»§a thuáº­t toÃ¡n HHL BÆ°á»›c 1: Khá»Ÿi táº¡o NhÆ° minh há»a á»Ÿ HÃ¬nh 1, thuáº­t toÃ¡n HHL cÃ³ Ä‘áº§u vÃ o gá»‘m cÃ³ 3 thanh ghi:\nThanh ghi Ancilla gá»“m 1 qubit Ä‘Æ°á»£c khá»Ÿi táº¡o báº±ng $\\ket{0}_{ANC}$. Qubit nÃ y sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ há»— trá»£ cho phÃ©p quay (rotation) tá»« $\\lambda_i\\ket{v_i}$ thÃ nh $\\frac{1}{\\lambda_i}\\ket{v_i}$.\nThanh ghi thá»© hai sáº½ chá»‹u trÃ¡ch nhiá»‡m lÆ°u giá»¯ thÃ´ng tin vá» giÃ¡ trá»‹ riÃªng tá»« káº¿t quáº£ cá»§a thuáº­t toÃ¡n Phase estimation - QPE. VÃ  tÆ°Æ¡ng tá»± vá»›i BÃ i 4, sá»‘ qubits Ä‘Æ°á»£c dÃ¹ng á»Ÿ thanh ghi nÃ y sáº½ phá»¥ thuá»™c vÃ o sai sá»‘ cá»§a thuáº­t toÃ¡n QPE. CÃ¡c qubits á»Ÿ Ä‘Ã¢y cÅ©ng Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i giÃ¡ trá»‹ $\\ket{0}_{W}$.\nThanh ghi cuá»‘i cÃ¹ng mÃ£ hÃ³a giÃ¡ trá»‹ cá»§a $\\ket{b}$. Má»i biáº¿n Ä‘á»•i sau nÃ y mÃ¬nh sáº½ giáº£ sá»­ $\\ket{b}$ Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a Ä‘á»ƒ Ä‘Æ¡n giáº£n trong quÃ¡ trÃ¬nh trÃ¬nh bÃ y cÃ¡c cÃ´ng thá»©c. Trong â€¦","date":1670555740,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670555740,"objectID":"48744332ee7a2ee512d09a68251ae4f9","permalink":"https://example.com/post/important-subroutine-3/","publishdate":"2022-12-09T10:15:40+07:00","relpermalink":"/post/important-subroutine-3/","section":"post","summary":"Cuá»‘i cÃ¹ng trong bá»™ 3 bÃ i viáº¿t vá» cÃ¡c subroutines cá»§a thuáº­t toÃ¡n lÆ°á»£ng tá»­, mÃ¬nh muá»‘n Ä‘á» cáº­p tá»›i thuáº­t toÃ¡n Harrow-Hassidim-Lloyd (hay HHL) Ä‘Æ°á»£c Ä‘áº·t theo tÃªn cá»§a ba tÃ¡c giáº£ Ä‘Ã£ giá»›i thiá»‡u thuáº­t toÃ¡n 1.","tags":[],"title":"BÃ i 5: Important Subroutine 3 - HHL Algorithm","type":"post"},{"authors":[],"categories":[],"content":" Náº±m thá»© hai trong bá»™ ba bÃ i viáº¿t vá» cÃ¡c *subroutines* quan trá»ng trong khoa há»c lÆ°á»£ng tá»­ nÃ³i chung vÃ  QML nÃ³i riÃªng, mÃ¬nh muá»‘n giá»›i thiá»‡u má»i ngÆ°á»i vá» thuáº­t toÃ¡n ***Quantum Phase Estimation*** (QPE). ChÃ­nh sá»± xuáº¥t hiá»‡n cá»§a thuáº­t toÃ¡n nÃ y Ä‘Ã£ táº¡o tiá»n Ä‘á» cho nhÆ°ng á»©ng dá»¥ng cá»§a mÃ¡y tÃ­nh lÆ°á»£ng tá»­ cho cÃ¡c bÃ i toÃ¡n phá»©c táº¡p dÆ°á»ng nhÆ° khÃ´ng thá»ƒ xá»­ lÃ½ trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng nhÆ°: *period finding* hay *factoring numbers*. Do Ä‘Ã³, mÃ¬nh sáº½ cÃ¹ng má»i ngÆ°á»i phÃ¢n tÃ­ch rÃµ hÆ¡n vá» thuáº­t toÃ¡n nÃ y. Ná»™i dung Quantum Phase Estimation Äá»™ phá»©c táº¡p cá»§a QPE Source Code Quantum Phase Estimation Cho má»™t ma tráº­n Ä‘Æ¡n nháº¥t (unitary matrix), $\\mathcal{U}$, ta cáº§n pháº£i tÃ¬m giÃ¡ trá»‹ riÃªng tÆ°Æ¡ng á»©ng vá»›i vÃ©c-tÆ¡ riÃªng $\\ket{\\psi}$ cá»§a ma tráº­n $\\mathcal{U}$. VÃ¬ $\\mathcal{U}$ lÃ  má»™t ma tráº­n Ä‘Æ¡n nháº¥t nÃªn khÃ¡ dá»… dÃ ng cho mÃ¬nh chá»©ng minh Ä‘Æ°á»£c cÃ¡c giÃ¡ trá»‹ riÃªng $\\lambda$ cá»§a $\\mathcal{U}$ cÃ³ Ä‘á»™ lá»›n lÃ  $1$ (má»i ngÆ°á»i cÃ³ thá»ƒ xem chá»©ng minh á»Ÿ Box 1). Do Ä‘Ã³, $\\lambda$ hoÃ n toÃ n cÃ³ thá»ƒ viáº¿t dÆ°á»›i dáº¡ng $e^{2\\pi i \\phi}$, $0\\leq \\phi \u0026lt; 1$, vÃ¬ $\\lambda^{*} \\lambda = e^{-2\\pi i \\phi + 2\\pi i \\phi} = e^0 = 1$. NhÆ° váº­y, thuáº­t toÃ¡n Quantum Phase Estimation (hay QPE) thá»±c cháº¥t sáº½ Ä‘i tÃ­nh $\\phi$ cho giÃ¡ trá»‹ riÃªng $\\lambda$.\nBox 1: Ta cÃ³ $ \\mathcal{U} \\ket{\\psi}=\\lambda \\ket{\\psi}$ vÃ  $\\mathcal{U}^{\\dagger}\\mathcal{U} = I$ (vÃ¬ $\\mathcal{U}$ lÃ  ma tráº­n Ä‘Æ¡n nháº¥t), nÃªn: $\\left\\langle \\psi| \\psi \\right\\rangle = \\left\\langle \\psi| \\mathcal{U}^{\\dagger} \\mathcal{U} |\\psi \\right\\rangle = \\bra{\\psi} \\lambda^{*} \\lambda \\ket{\\psi} = |\\lambda|^2\\left\\langle \\psi| \\psi \\right\\rangle $. Do Ä‘Ã³, $|\\lambda|^2 = 1$ HÃ¬nh 1: Cáº¥u trÃºc máº¡ch cá»§a thuáº­t toÃ¡n QPE Cáº¥u trÃºc máº¡ch cá»§a thuáº­t toÃ¡n QPE Ä‘Æ°á»£c minh há»a á»Ÿ HÃ¬nh 1. Thuáº­t toÃ¡n sáº½ cÃ³ 2 thÃ nh pháº§n chÃ­nh: vá»›i thanh ghi Ä‘áº§u tiÃªn, gá»“m $n$ qubits Ä‘Æ°á»£c khá»Ÿi táº¡o ban Ä‘áº§u báº±ng $\\ket{0}$, chá»‹u trÃ¡ch nhiá»‡m giá»¯ nhÆ°ng thÃ´ng tin vá» $\\phi$ sau khi thuáº­t toÃ¡n káº¿t thÃºc; cÃ²n thanh ghi thá»© 2 mÃ£ hÃ³a giÃ¡ trá»‹ cá»§a vÃ©c-tÆ¡ riÃªng $\\ket{\\psi}$. VÃ¬ giÃ¡ trá»‹ cá»§a $\\phi$ báº¥t Ä‘á»‹nh trong khoáº£ng $[0,1)$ nÃªn cÃ³ thá»ƒ tháº¥y ráº±ng giÃ¡ trá»‹ cá»§a $n$ dÃ¹ng Ä‘á»ƒ giáº£i mÃ£ $\\phi$ sáº½ linh hoáº¡t tÃ¹y thuá»™c Ä‘á»™ chÃ­nh xÃ¡c mÃ  chÃºng ta mong muá»‘n (má»i ngÆ°á»i cÃ³ thá»ƒ xem giáº£i thÃ­ch á»Ÿ Box 2).\nBox 2: Ta cÃ³ $\\phi \\in [0,1)$, tÆ°Æ¡ng tá»± vá»›i thuáº­t toÃ¡n QFT á»Ÿ BÃ i 3, mÃ¬nh cÃ³ thá»ƒ biá»ƒu diá»…n dÆ°á»›i dáº¡ng nhá»‹ phÃ¢n nhÆ° sau:\n$$ \\phi = 0.\\phi_1\\phi_2â€¦\\phi_k\\phi_{k+1}â€¦ $$\nVÃ¬ $\\phi$ chÆ°a xÃ¡c Ä‘á»‹nh nÃªn giáº£ sá»­ ta dÃ¹ng $n$ qubit Ä‘á»ƒ giáº£i mÃ£ $\\phi$ thÃ¬ luÃ´n tá»“n táº¡i giÃ¡ trá»‹ $\\epsilon \\geq 0$ sao cho\n$$ \\phi = 0.\\phi_1\\phi_2â€¦\\phi_n + \\epsilon $$\nTrong Ä‘Ã³ $\\epsilon = 0$ khi vÃ  chá»‰ khi $\\phi$ cÃ³ thá»ƒ biá»ƒu diá»…n chÃ­nh xÃ¡c vá»›i $n$ qubits. Do Ä‘Ã³ Ä‘á»™ chÃ­nh xÃ¡c cá»§a thuáº­t toÃ¡n QPE sáº½ phÃ¹ thuá»™c vÃ o má»i ngÆ°á»i tinh chá»‰nh giÃ¡ trá»‹ $n$.\nNhÆ° má»i ngÆ°á»i cÃ³ thá»ƒ tháº¥y á»Ÿ HÃ¬nh 1, thuáº­t toÃ¡n QPE gá»“m cÃ³ 3 bÆ°á»›c:\nBÆ°á»›c 1: Sá»­ dá»¥ng cá»•ng Hadamard lÃªn cÃ¡c qubits á»Ÿ thanh ghi Ä‘áº§u tiÃªn. VÃ¬ cÃ¡c qubits Ä‘Æ°á»£c khá»Ÿi táº¡o báº±ng vÃ©c-tÆ¡ $\\ket{0}$ nÃªn: $$ \\ket{q^1} = H^{\\otimes n} \\ket{0}^{\\otimes n} = \\frac{1}{2^{n/2}} (\\ket{0}+\\ket{1})(\\ket{0}+\\ket{1})...(\\ket{0}+\\ket{1}) $$ $$ = \\frac{1}{2^{n/2}} \\sum_{k=0}^{2^n-1}\\ket{k} $$ BÆ°á»›c 2: Láº§n lÆ°á»£t biáº¿n Ä‘á»•i qubits á»Ÿ thanh ghi thá»© hai vá»›i ma tráº­n $\\mathcal{U}^{2^{n-m}}$; á»Ÿ Ä‘Ã³, phÃ©p biáº¿n Ä‘á»•i nÃ y sáº½ phá»¥ thuá»™c vÃ o qubit thá»© $m$ cá»§a thanh ghi Ä‘áº§u tiÃªn, $\\ket{q^1_m}$. Náº¿u $\\ket{q^1_m} = \\ket{0}$, phÃ©p biáº¿n Ä‘á»•i sáº½ giá»¯ nguyÃªn giÃ¡ trá»‹ cá»§a $\\ket{\\psi}$ á»Ÿ thanh ghi thá»© hai, vÃ  nÃ³ chá»‰ thá»±c hiá»‡n biáº¿n Ä‘á»•i vá»›i ma tráº­n $\\mathcal{U}^{n-m}$ khi $\\ket{q^1_m} = \\ket{1}$. Do Ä‘Ã³, mÃ¬nh cÃ³ thá»ƒ kÃ½ hiá»‡u phÃ©p Ä‘á»•i nÃ y lÃ  controlled- $\\mathcal{U}^{2^{n-m}}$ (hay $C-U^{2^{n-m}}$ nhÆ° HÃ¬nh 1). NhÆ° váº­y, cá»•ng $C-U^{2^{n-m}}$ sáº½ biáº¿n Ä‘á»•i bÃ i toÃ¡n cá»§a chÃºng ta nhÆ° sau: $$ (C-U^{2^{n-m}}) \\ket{\\psi} \\frac{1}{\\sqrt{2}} (\\ket{0}+\\ket{1}) = \\frac{1}{\\sqrt{2}} (\\ket{\\psi}\\ket{0}+\\mathcal{U}^{2^{n-m}}\\ket{\\psi}\\ket{1}) $$ $$ = \\frac{1}{\\sqrt{2}} (\\ket{\\psi}\\ket{0}+\\lambda^{2^{n-m}}\\ket{\\psi}\\ket{1}) = \\frac{1}{\\sqrt{2}} (\\ket{\\psi}\\ket{0}+e^{2\\pi i \\phi 2^{n-m}}\\ket{\\psi}\\ket{1}) $$ $$ = \\ket{\\psi} \\otimes \\frac{1}{\\sqrt{2}} (\\ket{0}+e^{2\\pi i \\phi 2^{n-m}}\\ket{1}) $$ CÃ³ thá»ƒ tháº¥y, phÃ©p biáº¿n Ä‘á»•i controlled- $\\mathcal{U}^{2^{n-m}}$ thá»±c cháº¥t khÃ´ng thay Ä‘á»•i tráº¡ng thÃ¡i cá»§a thanh ghi thá»© hai, $\\ket{\\psi}$, mÃ  nÃ³ sáº½ thay Ä‘á»•i tráº¡ng thÃ¡i cá»§a qubit $\\ket{q^1_m}$. Tá»« Ä‘Ã¢y bÃ i toÃ¡n Ä‘Æ°a ta vá» váº¥n Ä‘á» giá»‘ng vá»›i thuáº­t toÃ¡n QFT á»Ÿ BÃ i 3.\nGiáº£ sá»­, $\\phi$ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng nhá»‹ phÃ¢n: $$ \\phi = 0.\\phi_1\\phi_2...\\phi_n=\\phi_1 2^{-1}+\\phi_2 2^{-2}+...+\\phi_n 2^{-n} $$ Vá»›i m = 1, ta cÃ³: $$ \\phi 2^{n-1} =\\phi_1 2^{n-2}+\\phi_2 2^{n-3}+...+\\phi_n 2^{-1} $$ $$ \\Rightarrow e^{2\\pi i \\phi 2^{n-1}} =e^{2\\pi i \\phi_1 2^{n-2}}e^{2\\pi i \\phi_2 2^{n-3}}...e^{2\\pi i\\phi_n 2^{-1}} $$ Máº·t khÃ¡c, mÃ¬nh cÃ³ thá»ƒ tháº¥y $e^{2\\pi i} = 1$ nÃªn $e^{2\\pi ik} = 1 \\forall k \\in \\mathbb{Z}$. Tá»« Ä‘Ã³ cÃ³ thá»ƒ dá»… dÃ ng suy ra táº¥t cáº£ cÃ¡c pháº§n tá»­ trong cÃ´ng thá»©c trÃªn Ä‘á»u báº±ng 1 trá»« $e^{2\\pi i \\phi_n 2^{-1}}$, nÃªn $$ e^{2\\pi i \\phi 2^{n-1}} = 1.1.1...e^{2\\pi i \\phi_n 2^{-1}} = e^{2\\pi i 0.\\phi_n} $$ TÆ°Æ¡ng tá»± ta cÅ©ng cÃ³ thá»ƒ suy ra: $$ e^{2\\pi i \\phi 2^{n-2}} = e^{2\\pi i 0.\\phi_{n-1}\\phi_n} $$ $$ ... $$ $$ e^{2\\pi i \\phi 2^{0}} = â€¦","date":1670468805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670468805,"objectID":"8f9aa04c83051c3138aba4dca40b3435","permalink":"https://example.com/post/important-subroutine-2/","publishdate":"2022-12-08T10:06:45+07:00","relpermalink":"/post/important-subroutine-2/","section":"post","summary":"Náº±m thá»© hai trong bá»™ ba bÃ i viáº¿t vá» cÃ¡c *subroutines* quan trá»ng trong khoa há»c lÆ°á»£ng tá»­ nÃ³i chung vÃ  QML nÃ³i riÃªng, mÃ¬nh muá»‘n giá»›i thiá»‡u má»i ngÆ°á»i vá» thuáº­t toÃ¡n ***Quantum Phase Estimation*** (QPE).","tags":[],"title":"BÃ i 4: Important Subroutine 2 - Quantum Phase Estimation","type":"post"},{"authors":[],"categories":[],"content":" á» pháº§n nÃ y mÃ¬nh sáº½ cÃ³ 3 bÃ i viáº¿t nÃ³i vá» 3 subroutines ráº¥t quan trá»ng táº¡o nÃªn kháº£ nÄƒng tÃ­nh toÃ¡n vÆ°á»£t trá»™i cá»§a thuáº­t toÃ¡n lÆ°á»£ng tá»­: Quantum Fourier Transform, Quantum Phase Estimation, and HHL algorithm. Nhá»¯ng thuáº­t toÃ¡n trÃªn cÃ³ máº·t nhiá»u trong cÃ¡c bÃ i toÃ¡n cá»§a QML. Do Ä‘Ã³ mÃ¬nh tin ráº±ng viá»‡c náº¯m rÃµ nhá»¯ng thuáº­t toÃ¡n nÃ y Ä‘áº§u tiÃªn sáº½ giÃºp cÃ¡c báº¡n trong viá»‡c tÃ¬m hiá»u cÃ¡c thuáº­t toÃ¡n cá»§a QML.\nÄá»ƒ khá»Ÿi Ä‘áº§u trong danh sÃ¡ch trÃªn mÃ¬nh sáº½ trÃ¬nh bÃ y thuáº­t toÃ¡n Quantum Fourier Transform (QFT).\nNá»™i dung Discrete Fourier Transform Quantum Fourier Transform Äá»™ phá»©c táº¡p cá»§a QFT Source Code Discrete Fourier Transform Fourier Transform Ä‘Ã£ vÃ  Ä‘ang xuáº¥t hiá»‡n trong ráº¥t nhiá»u á»©ng dá»¥ng á»Ÿ mÃ¡y tÃ­nh truyá»n thá»‘ng, cÃ³ thá»ƒ ká»ƒ Ä‘áº¿n nhÆ°: signal processing hay data compression. Fourier Transform sáº½ biáº¿n Ä‘á»•i má»™t hÃ m sá»‘ hoáº·c má»™t vÃ©c-tÆ¡ theo miá»n thá»i gian hoáº·c khÃ´ng gian sang miá»n táº§n sá»‘. VÃ­ dá»¥ Discrete Fourier Transform (DFT) cá»§a má»™t biáº¿n rá»i ráº¡c $\\mathcal{X_N} = \\{x_0, x_1, â€¦, x_{N-1}\\} \\in \\mathbb{C}^N$ cÃ³ dáº¡ng:\n$$ y_k = \\frac{1}{\\sqrt{N}} \\sum_{j=0}^{N-1} x_j e^{\\frac{2\\pi ijk}{N}}, $$ Tá»« Ä‘Ã³ phÃ©p biáº¿n Ä‘á»•i sáº½ cho ra má»™t chuá»—i má»›i $\\mathcal{Y_N} = \\{y_0, y_1, â€¦, y_{N-1}\\} \\in \\mathbb{C}^N$ cÃ¹ng chiá»u vá»›i chuá»—i Ä‘áº§u vÃ o.\nMáº·t khÃ¡c, mÃ¬nh muá»‘n Ä‘á» cáº­p tá»›i hÃ m Kronecker delta, $\\delta_{Nj}$, Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  má»™t vÃ©c-tÆ¡ $N$ chiá»u, cÃ³ giÃ¡ trá»‹ 1 á»Ÿ vá»‹ trÃ­ thá»© $j$ vÃ  0 á»Ÿ cÃ¡c vá»‹ trÃ­ cÃ²n láº¡i. Do Ä‘Ã³, mÃ¬nh hoÃ n toÃ n cÃ³ thá»ƒ coi hÃ m Kronecker delta lÃ  cÃ¡c vÃ©c-tÆ¡ cÆ¡ sá»Ÿ trá»±c chuáº©n cho biáº¿n $\\mathcal{X_N}$:\n$$ \\mathcal{X_N} = \\sum_{j=0}^{N} x_j \\delta_{Nj} $$ KhÃ´ng máº¥t tÃ­nh tá»•ng quÃ¡t, mÃ¬nh váº«n hoÃ n toÃ n cÃ³ thá»ƒ láº¥y hÃ m Kronecker delta nhÆ° cÃ¡c vÃ©c-tÆ¡ cÆ¡ sá»Ÿ trá»±c chuáº©n $\\ket{j}$ sáº½ Ä‘Æ°á»£c sá»­ dá»¥ng cho thuáº­t toÃ¡n lÆ°á»£ng tá»­ cá»§a chÃºng ta. NÃ³i cÃ¡ch khÃ¡c, biáº¿n $\\mathcal{X_N}$ sáº½ Ä‘Æ°á»£c mÃ£ hÃ³a trong thuáº­t toÃ¡n lÆ°á»£ng tá»­ cÃ³ dáº¡ng 1: $$ \\ket{\\mathcal{X_N}} = \\sum_{j=0}^{N} x_j \\ket{j} $$ KhÃ¡c phÃ©p biáº¿n Ä‘á»•i Fourier trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng, thuáº­t toÃ¡n Quantum Fourier Transform sáº½ biáº¿n Ä‘á»•i cÃ¡c vÃ©c-tÆ¡ cÆ¡ sá»Ÿ trá»±c chuáº©n $\\ket{j}$:\n$$ \\ket{j} \\longrightarrow \\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1} e^{\\frac{2\\pi ijk}{N}} \\ket{k} $$ VÃ  nÃ³ tÆ°Æ¡ng Ä‘Æ°Æ¡ng vá»›i viá»‡c:\n$$ \\sum_{j=0}^{N-1}x_j \\ket{j} \\longrightarrow \\sum_{k=0}^{N-1} y_k \\ket{k} $$ $$ \\ket{\\mathcal{X_N}} \\longrightarrow \\ket{\\mathcal{Y_N}} $$ Váº­y chi tiáº¿t Ä‘áº±ng sau phÃ©p biáº¿n Ä‘á»•i nÃ y nhÆ° tháº¿ nÃ o, mÃ¬nh sáº½ Ä‘i tá»›i pháº§n tiáº¿p theo.\nQuantum Fourier Transform Vá»›i biáº¿n $\\mathcal{X_N}$ gá»“m $N$ pháº§n tá»­, mÃ¬nh cáº§n $n = \\log_2{N}$ qubits Ä‘á»ƒ biá»ƒu diá»…n toÃ n bá»™ cÃ¡c vÃ©c-tÆ¡ $\\ket{j}$. á» Ä‘Ã¢y, $j$ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng nhá»‹ phÃ¢n $j_1j_2â€¦j_n$ sao cho:\n$$ j = j_1 2^{n-1} + j_2 2^{n-2} + ...+ j_n 2^0 $$ VÃ  Ä‘á»ƒ thuáº­n tiá»‡n hÆ¡n trong viá»‡c phÃ¡t triá»ƒn cÃ´ng thá»©c dÆ°á»›i Ä‘Ã¢y, mÃ¬nh sáº½ kÃ½ hiá»‡u $0.j_lj_{l+1}â€¦j_m = j_l/2 + j_{l+1}/4+â€¦+j_m/2^{m-l+1}$.\nHÃ¬nh 1: Cáº¥u trÃºc máº¡ch cá»§a thuáº­t toÃ¡n QFT HÃ£y báº¯t Ä‘áº§u vá»›i qubit Ä‘áº§u tiÃªn $\\ket{j_1}$, cá»•ng Hadamard (H gate) biáº¿n Ä‘á»•i tráº¡ng thÃ¡i cá»§a $\\ket{j_1}$ thÃ nh $\\frac{1}{\\sqrt{2}}(\\ket{0}+(-1)^{j_1}\\ket{1})$. MÃ¬nh thay $(-1) = e^{\\pi i}$, ta Ä‘Æ°á»£c: $$ H\\ket{j_1} = \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i (\\frac{j_1}{2})}\\ket{1}) = \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_1} \\ket{1}) $$ Qubit nÃ y tiáº¿p Ä‘áº¿n Ä‘Æ°á»£c biáº¿n Ä‘á»•i theo ${\\mathbf{R}}_m$ bá»‹ rÃ ng buá»™c bá»Ÿi giÃ¡ trá»‹ cá»§a $j_m$: $$ {\\mathbf{R}}_m = \\left( \\begin{array}{cc} 1 \u0026amp; 0 \\\\ 0 \u0026amp; e^{\\frac{2\\pi i j_m}{2^m}} \\end{array} \\right) $$ NhÆ° váº­y náº¿u thá»±c hiá»‡n phÃ©p biáº¿n Ä‘á»•i ${\\mathbf{R}}_2$ trÃªn qubit thá»© nháº¥t ta Ä‘Æ°á»£c: $$ \\left( \\begin{array}{cc} 1 \u0026amp; 0 \\\\ 0 \u0026amp; e^{\\frac{2\\pi i j_2}{2^2}} \\end{array} \\right) \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_1} \\ket{1}) = \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i (0.j_1+j_2/2^2)} \\ket{1}) $$ $$ = \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_1j_2} \\ket{1}) $$ TÆ°Æ¡ng tá»± nhÆ° váº­y ta tiáº¿p tá»¥c Ã¡p dá»¥ng phÃ©p biáº¿n Ä‘á»•i ${\\mathbf{R}}_3$, ${\\mathbf{R}}_4$,â€¦ ${\\mathbf{R}}_n$ mÃ¬nh cÃ³ tráº¡ng thÃ¡i cá»§a qubit thá»© nháº¥t Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau: $$ \\ket{j_1} \\longrightarrow \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_1j_2...j_n} \\ket{1}) $$ Láº§n lÆ°á»£t nhÆ° váº­y mÃ¬nh dÃ¹ng cÃ¡c phÃ©p biáº¿n Ä‘á»•i tÆ°Æ¡ng tá»± cho cÃ¡c qubit tiáº¿p theo, thu Ä‘Æ°á»£c: $$ \\ket{j_2} \\longrightarrow \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_2...j_n} \\ket{1}) $$ $$ \\ket{j_3} \\longrightarrow \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_3...j_n} \\ket{1}) $$ $$ ... $$ $$ \\ket{j_n} \\longrightarrow \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_n} \\ket{1}) $$ NhÆ° váº­y náº¿u mÃ¬nh tá»•ng há»£p cÃ¡c káº¿t quáº£ tá»« $n$ qubits, bÃ i toÃ¡n cá»§a mÃ¬nh sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau: $$ \\ket{j_1j_2...j_n} \\! \\longrightarrow \\! $$ $$ \\frac{1}{2^\\frac{n}{2}}(\\ket{0}+e^{2\\pi i 0.j_1j_2...j_n} \\ket{1})(\\ket{0}+e^{2\\pi i 0.j_2...j_n} \\ket{1})...(\\ket{0}+e^{2\\pi i 0.j_n} \\ket{1}) $$ VÃ  bÆ°á»›c cuá»‘i cÃ¹ng mÃ¬nh sáº½ swap 2 tráº¡ng thÃ¡i cá»§a qubit thá»© $m$ vá»›i qubit thá»© $n-m$, hay nÃ³i cÃ¡ch khÃ¡c mÃ¬nh Ä‘áº£o ngÆ°á»£c thá»© tá»± tráº¡ng thÃ¡i cá»§a cÃ¡c qubit: $$ \\frac{1}{2^\\frac{n}{2}}(\\ket{0}\\!+\\!e^{2\\pi i 0.j_n} \\ket{1})(\\ket{0}\\!+\\!e^{2\\pi i 0.j_{n-1}j_n} \\ket{1})...(\\ket{0}\\!+\\!e^{2\\pi i 0.j_1j_2...j_n} \\ket{1}) \\quad (*) $$ Äáº¿n Ä‘Ã¢y, má»i ngÆ°á»i cÃ³ thá»ƒ váº«n cÃ²n tháº¯c máº¯c vá» káº¿t quáº£ trÃªn nÃªn mÃ¬nh sáº½ Ä‘i sÃ¢u hÆ¡n Ä‘á»ƒ giáº£i â€¦","date":1669532765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669532765,"objectID":"58dc9016fa2d0a0763779e0481b5afbe","permalink":"https://example.com/post/important-subroutine-1/","publishdate":"2022-11-27T14:06:05+07:00","relpermalink":"/post/important-subroutine-1/","section":"post","summary":"á» pháº§n nÃ y mÃ¬nh sáº½ cÃ³ 3 bÃ i viáº¿t nÃ³i vá» 3 subroutines ráº¥t quan trá»ng táº¡o nÃªn kháº£ nÄƒng tÃ­nh toÃ¡n vÆ°á»£t trá»™i cá»§a thuáº­t toÃ¡n lÆ°á»£ng tá»­: Quantum Fourier Transform, Quantum Phase Estimation, and HHL algorithm.","tags":[],"title":"BÃ i 3: Important Subroutine 1 - Quantum Fourier Transform","type":"post"},{"authors":[],"categories":[],"content":"News ğŸ“° Determinable and interpretable network representation for link prediction\nAzure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students\nPenn State researchers to explore using quantum computers to design new drugs\nMultiverse Computing and Mila Join Forces to Advance Artificial Intelligence with Quantum Computing\nQuantum AI Elon Musk Review / Scam App Or Legit?\nVideos ğŸ“½ï¸ Quantum Machine Learning Explained\nQiskit Falll Fest CIC-IPN Mexico 2022- Quantum Machine Learning\nQuantum Machine Learning Neuroimaging for Alzheimerâ€™s Disease\nMLBBQ: Quantum Machine Learning by Pavel Popov\nQuantum Machine Learning: Opportunities and Challenges\nHands-on quantum machine learning | Rodrigo Morales | ADC2022\nâ€œReinforcement Learning for Quantum Technologies,â€ presented by Florian Marquardt\nPublications ğŸ“ƒ Representation Theory for Geometric Quantum Machine Learning Abstract: Recent advances in classical machine learning have shown that creating models with inductive biases encoding the symmetries of a problem can greatly improve performance. Importation of these ideas, combined with an existing rich body of work at the nexus of quantum theory and symmetry, has given rise to the field of Geometric Quantum Machine Learning (GQML). Following the success of its classical counterpart, it is reasonable to expect that GQML will play a crucial role in developing problem-specific and quantum-aware models capable of achieving a computational advantage. Despite the simplicity of the main idea of GQML â€“ create architectures respecting the symmetries of the data â€“ its practical implementation requires a significant amount of knowledge of group representation theory. We present an introduction to representation theory tools from the optics of quantum learning, driven by key examples involving discrete and continuous groups. These examples are sewn together by an exposition outlining the formal capture of GQML symmetries via â€œlabel invariance under the action of a group representationâ€, a brief (but rigorous) tour through finite and compact Lie group representation theory, a reexamination of ubiquitous tools like Haar integration and twirling, and an overview of some successful strategies for detecting symmetries.\nTheory for Equivariant Quantum Neural Networks Abstract: Most currently used quantum neural network architectures have little-to-no inductive biases, leading to trainability and generalization issues. Inspired by a similar problem, recent breakthroughs in classical machine learning address this crux by creating models encoding the symmetries of the learning task. This is materialized through the usage of equivariant neural networks whose action commutes with that of the symmetry. In this work, we import these ideas to the quantum realm by presenting a general theoretical framework to understand, classify, design and implement equivariant quantum neural networks. As a special implementation, we show how standard quantum convolutional neural networks (QCNN) can be generalized to group-equivariant QCNNs where both the convolutional and pooling layers are equivariant under the relevant symmetry group. Our framework can be readily applied to virtually all areas of quantum machine learning, and provides hope to alleviate central challenges such as barren plateaus, poor local minima, and sample complexity.\nTheoretical Guarantees for Permutation-Equivariant Quantum Neural Networks Abstract: Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry Sn), and show how to build Sn-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and can generalize well from small amounts of data. To verify our results, we perform numerical simulations for a graph state classification task. Our work provides the first theoretical guarantees for equivariant QNNs, thus indicating the extreme power and potential of GQML.\nProtocols for classically training quantum generative models on probability distributions Abstract: Quantum Generative Modelling (QGM) relies on preparing quantum states and generating samples from these states as hidden - or known - probability distributions. As distributions from some classes of quantum states (circuits) are inherently hard to sample classically, QGM represents an excellent â€¦","date":1667030093,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667030093,"objectID":"3d5ac705d961acc8202e0fd046e4603d","permalink":"https://example.com/newsletter/october-2022/","publishdate":"2022-10-29T14:54:53+07:00","relpermalink":"/newsletter/october-2022/","section":"newsletter","summary":"News ğŸ“° Determinable and interpretable network representation for link prediction\nAzure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students\nPenn State researchers to explore using quantum computers to design new drugs","tags":[],"title":"October 2022","type":"newsletter"},{"authors":[],"categories":[],"content":" Trong bÃ i nÃ y, mÃ¬nh Ä‘i qua má»™t phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ bÃ i toÃ¡n nearest neighbour báº±ng thuáº­t toÃ¡n quantum. BÃ i viáº¿t dÆ°á»›i Ä‘Ã¢y sáº½ dá»±a vÃ o bÃ i bÃ¡o gá»‘c: Implementing a distance-based classifier with a quantum interference circuit, náº¿u ai muá»‘n tÃ¬m hiá»ƒu sÃ¢u hÆ¡n vá» Ã½ tÆ°á»Ÿng nÃ y thÃ¬ cÃ³ thá»ƒ ghÃ© qua.\nNá»™i dung Squared-Distance Classifier Quantum Squared-Distance Classifier Káº¿t luáº­n Source Code Squared-Distance Classifier á» Ä‘Ã¢y, mÃ¬nh xÃ©t vÃ­ dá»¥ bÃ i toÃ¡n phÃ¢n loáº¡i táº­p data Titanic. Giáº£ sá»­ táº­p data Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng:\n$$ \\mathcal{D} = \\Big\\{ ({\\bf{x}}^1, y^1), \\ldots ({\\bf{x}}^M , y^M) \\Big\\}, $$ trong Ä‘Ã³ cÃ¡c vÃ©c-tÆ¡ Ä‘áº§u vÃ o 2 chiá»u: ${ {\\bf{x}}^m = ({x_0}^m, {x_1}^m)^T}, m = 1,2,â€¦,M$ tÆ°á»£ng trÆ°ng cho má»™t hÃ nh khÃ¡ch trÃªn chuyáº¿n tÃ u Titanic Ä‘Ã£ bá»‹ nháº¥n chÃ¬m vÃ o nÄƒm 1912. Trong Ä‘Ã³ $x_0$ lÃ  giÃ¡ vÃ© trong khoáº£ng tá»« 0 Ä‘áº¿n 10,000 Ä‘Ã´ la, vÃ  $x_1$ lÃ  sá»‘ hiá»‡u cabin trong khoáº£ng tá»« 1 Ä‘áº¿n 2,500. á»¨ng vá»›i má»—i má»™t vÃ©c-tÆ¡ Ä‘áº§u vÃ o lÃ  nhÃ£n $y^m = {0,1}$ tÆ°Æ¡ng á»©ng Ä‘á»ƒ chá»‰ ra hÃ nh khÃ¡ch Ä‘Ã³ Ä‘Ã£ sá»‘ng sÃ³t hay khÃ´ng.\nSouce Náº¿u tá»«ng tÃ¬m hiá»ƒu qua vá» Machine Learning, cháº¯c háº³n cÃ¡c báº¡n Ä‘Ã£ nghe hoáº·c Ä‘á»c qua vá» thuáº­t toÃ¡n nearest neighbour: vá»›i má»—i vÃ©c-tÆ¡ Ä‘áº§u vÃ o má»›i, thÃ¬ nhÃ£n cá»§a nÃ³ sáº½ Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh bá»Ÿi Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t vá»›i nÃ³. CÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ xÃ¡c Ä‘á»‹nh nhá»¯ng Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t Ä‘Ã³ nhÆ°ng phá»• biáº¿n lÃ  Euclidean distance. Do váº­y, ta cÃ³ cÃ¡ch tÃ­nh há»‡ sá»‘ cho viá»‡c gÃ¡n nhÃ£n vÃ©c-tÆ¡ $\\tilde{x}$ má»›i theo nhÃ£n cá»§a $x^m$: $$ \\gamma_m = 1-\\frac{1}{c}|\\tilde{{\\bf x}}-{\\bf x}^m|^2, $$ trong Ä‘Ã³ $c$ lÃ  háº±ng sá»‘. Há»‡ sá»‘ cÃ ng cao chá»©ng tá» $\\tilde{{\\bf x}}$ cÃ ng gáº§n $x^m$. Gá»i $\\tilde{y}$ lÃ  nhÃ£n Ä‘Æ°á»£c gÃ¡n cho $\\tilde{{\\bf x}}$, ta cÃ³ xÃ¡c xuáº¥t $p_{\\tilde{{\\bf x}}}(\\tilde{y}=1)$ lÃ  tá»•ng trung bÃ¬nh há»‡ sá»‘ cá»§a $M_1$ Ä‘iá»ƒm dá»¯ liá»‡u mÃ  cÃ³ nhÃ£n lÃ  $1$: $$ p_{\\tilde{{\\bf x}}}(\\tilde{y}=1) = \\frac{1}{\\chi}\\frac{1}{M_1} \\sum_{m|y^m=1}(1-\\frac{1}{c}|\\tilde{{\\bf x}}-{\\bf x}^m|^2) $$ TÆ°Æ¡ng tá»± nhÆ° váº­y, $p_{\\tilde{{\\bf x}}}(\\tilde{y}=0)$ lÃ  tá»•ng trung bÃ¬nh há»‡ sá»‘ cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u mÃ  cÃ³ nhÃ£n lÃ  $0$. Trong Ä‘Ã³ $\\frac{1}{\\chi}$ lÃ  normalizing factor sao cho $p_{\\tilde{{\\bf x}}}(\\tilde{y}=0)+p_{\\tilde{{\\bf x}}}(\\tilde{y}=1)=1$.\nSouce Ãp dá»¥ng phÆ°Æ¡ng phÃ¡p ta tháº¥y Passenger 3 sáº½ gáº§n vá»›i Passenger 1 hÆ¡n so vá»›i Passenger 2 (Fig 1.2), vÃ  mÃ´ hÃ¬nh sáº½ Ä‘Æ°a ra dá»± Ä‘oÃ¡n lÃ  $1$ tÆ°Æ¡ng á»©ng vá»›i survival.\nQuantum Squared-Distance Classifier Giá» hÃ£y xá»­ lÃ½ bÃ i toÃ n nÃ y báº±ng phÆ°Æ¡ng phÃ¡p â€˜quantumâ€™.\nBÆ°á»›c 1: Data preprocessing and Encoding Äáº§u tiÃªn chÃºng ta sáº½ Ä‘i tá»›i má»™t cÃ¢u há»i kinh Ä‘iá»ƒn â€œLÃ m sao cÃ³ thá»ƒ biá»ƒu diá»…n dá»¯ liá»‡u trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­?â€. Náº¿u nhÆ° trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng cÃ¡c thÃ´ng tin nhÆ° áº£nh sáº½ thÆ°á»ng Ä‘Æ°á»£c biá»ƒn diá»…n trÃªn khÃ´ng gian RBG cÃ³ giÃ¡ trá»‹ tá»« 0 Ä‘áº¿n 255, hay chÃºng ta cÃ³ Word Embedding Ä‘á»ƒ biá»ƒu thÃ´ng tin dáº¡ng vÄƒn báº£n thÃ nh cÃ¡c vÃ©c-tÆ¡, thÃ¬ váº¥n Ä‘á» cá»§a cÃ¡c thuáº­t toÃ¡n lÆ°á»£ng tá»­ cÅ©ng nhÆ° váº­y. Thá»±c cháº¥t, chá»§ Ä‘á» vá» viá»‡c mÃ£ hÃ³a thÃ´ng tin trÃªn khÃ´ng gian lÆ°á»£ng tá»­ (Quantum Embedding) váº«n Ä‘ang Ä‘Æ°á»£c cá»™ng Ä‘á»“ng nghiÃªn cá»©u quan tÃ¢m Ä‘áº·c biá»‡t trong lÄ©nh vá»±c Quantum Machine Learning. Náº¿u cÃ¡c báº¡n quan tÃ¢m Ä‘áº¿n chá»§ Ä‘á» nÃ y cÃ³ thá»ƒ xem qua paper nÃ y Ä‘á»ƒ biáº¿t rÃµ hÆ¡n vá» cÃ¡c cÃ¡ch mÃ£ thÃ´ng tin trong QML vÃ  sá»± quan trá»ng cá»§a nÃ³. MÃ¬nh sáº½ lÃ m má»™t bÃ i viáº¿t chi tiáº¿t hÆ¡n vá» chá»§ Ä‘á» nÃ y trong tÆ°Æ¡ng lai.\nQuay láº¡i vá»›i bÃ i toÃ¡n cá»§a chÃºng ta, mÃ¬nh sáº½ Ã¡p dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  Amplitude Embedding - má»™t phÆ°Æ¡ng phÃ¡p ráº¥t phá»‘ biáº¿n trong QML: Cho $X \\in \\mathbb{R}^N$ lÃ  má»™t vÃ©c-tÆ¡ Ä‘Æ¡n nháº¥t ($||X|| = 1$), ta cÃ³ thá»ƒ mÃ£ hÃ³a $X$ báº±ng $n$ qubits dÆ°á»›i dáº¡ng: $$ \\ket{\\psi_X} = \\sum_{i=0}^{N-1}x_i \\ket{i}, $$ trong Ä‘Ã³ $n = \\log{N}$. CÃ³ thá»ƒ tháº¥y phÆ°Æ¡ng phÃ¡p nÃ y chá»‰ tá»‘n $O(\\log{N})$ qubits Ä‘á»ƒ biá»ƒu diá»…n má»™t vÃ©c-tÆ¡ $N$ chiá»u. HÃ£y láº¥y vÃ­ dá»¥ trong bÃ i toÃ¡n xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, giáº£ sá»­ báº¡n cÃ³ má»™t text corpus vá»›i 10000 tá»« thÃ¬ náº¿u nhÆ° cÃ¡ch thÃ´ng thÆ°á»ng ta sá»­ dá»¥ng One-hot encoding ta sáº½ cáº§n tá»›i 10000 bits Ä‘á»ƒ mÃ£ hÃ³a, nhÆ°ng Ä‘iá»u nÃ y hoÃ n toÃ¡n cÃ³ thá»ƒ giáº£i quyáº¿t vá»›i 14 ($\\lceil \\log{10000} \\rceil$) qubits vá»›i amplitude embedding.\nTá»« Ä‘Ã¢y, vá»›i má»—i Ä‘áº§u vÃ o $\\ket{\\psi_{\\bf\\tilde{x}}}$ má»›i, bÃ i toÃ¡n sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o dÆ°á»›i dáº¡ng:\nTrong Ä‘Ã³ $\\ket{m}$ vÃ  $\\ket{y^m}$ mÃ£ hÃ³a cho sá»‘ thá»© tá»± vÃ  nhÃ£n tÆ°Æ¡ng á»©ng cá»§a vÃ©c-tÆ¡ Ä‘áº§u vÃ o thá»© $m^{th}$. Tuy nhiÃªn Ä‘iá»u chÃº Ã½ á»Ÿ Ä‘Ã¢y náº±m á»Ÿ ancilla qubit Ä‘Æ°á»£c káº¿t ná»‘i vá»›i $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ vÃ  $\\ket{\\psi_{\\bf{x}^m}}$. ChÃº Ã½ ráº±ng khi ta thá»±c hiá»‡n phÃ©p do trÃªn má»™t hoáº·c má»™t há»‡ qubits thÃ¬ qubit(s) sáº½ bá»‹ collapsed hay terminated. Váº­y khi ta chuyá»ƒn phÃ©p Ä‘o cá»§a $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ hay $\\ket{\\psi_{\\bf{x}^m}}$ sang phÃ©p Ä‘o cá»§a má»™t ancilla qubit Ä‘Æ°á»£c káº¿t ná»‘i vá»›i chÃºng thÃ¬ ta váº«n thu Ä‘Æ°á»£c káº¿t quáº£ cáº§n thiáº¿t cá»§a $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ vÃ  $\\ket{\\psi_{\\bf{x}^m}}$ mÃ  khÃ´ng cáº§n cháº¥m dá»©t (terminate) cáº£ há»‡ thá»‘ng. Ká»¹ thuáº­t nÃ y ráº¥t hay sá»­ dá»¥ng á»Ÿ trong cÃ¡c thuáº­t toÃ¡n lÆ°á»£ng tá»­ vÃ  viá»‡c káº¿t ná»‘i giá»¯a ancilla qubit vá»›i há»‡ thá»‘ng lÃ  chÃºng ta Ä‘ang táº¡o ra entanglement - má»™t tÃ­nh cháº¥t quan trá»ng khÃ¡c trong lÄ©nh vá»±c tÃ­nh toÃ¡n lÆ°á»£ng tá»­.\nNhÆ° váº­y vá»›i vÃ­ dá»¥ Titanic trÃªn ta cÃ³: $$ \\ket{\\mathcal{D}} = \\frac{1}{\\sqrt{4}} \\Big\\{\\ket{0}[\\ket{0}(0.866\\ket{0}+0.5\\ket{1})+\\ket{1}(0.921\\ket{0}+0.39\\ket{1})]\\ket{1} $$ $$ + â€¦","date":1665505552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665505552,"objectID":"41280be87b10845fb59ca0bce349f39e","permalink":"https://example.com/post/example-1/","publishdate":"2022-10-11T23:25:52+07:00","relpermalink":"/post/example-1/","section":"post","summary":"Trong bÃ i nÃ y, mÃ¬nh Ä‘i qua má»™t phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ bÃ i toÃ¡n nearest neighbour báº±ng thuáº­t toÃ¡n quantum. BÃ i viáº¿t dÆ°á»›i Ä‘Ã¢y sáº½ dá»±a vÃ o bÃ i bÃ¡o gá»‘c: Implementing a distance-based classifier with a quantum interference circuit, náº¿u ai muá»‘n tÃ¬m hiá»ƒu sÃ¢u hÆ¡n vá» Ã½ tÆ°á»Ÿng nÃ y thÃ¬ cÃ³ thá»ƒ ghÃ© qua.","tags":[],"title":"BÃ i 2: Quantum Squared-Distance Classifier","type":"post"},{"authors":[],"categories":[],"content":" Ná»™i dung Giá»›i thiá»‡u bÃ i toÃ¡n Classical Interference Quantum Interference Káº¿t luáº­n TrÆ°á»›c khi Ä‘i vÃ o cÃ¡c thuáº­t toÃ¡n quantum trong há»c mÃ¡y, mÃ¬nh muá»‘n so sÃ¡nh bÃ i toÃ¡n suy luáº­n xÃ¡c suáº¥t trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng cÅ©ng nhÆ° trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­. BÃ i viáº¿t nÃ y sáº½ giÃºp cÃ¡c báº¡n viáº¿t qua nhá»¯ng thÃ nh pháº§n cÆ¡ báº£n cá»§a váº­t lÃ½ lÆ°á»£ng tá»­: qubits, unitary transformation, vÃ  measurement vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a chÃºng thÃ´ng qua má»™t vÃ­ dá»¥ cá»¥ thá»ƒ.\nGiá»›i thiá»‡u bÃ i toÃ¡n Cho hai Ä‘á»“ng xu Ä‘á»“ng cháº¥t: $c_1$ vÃ  $c_2$ vá»›i xÃ¡c suáº¥t á»Ÿ máº·t sáº¥p (tail) hay máº·t ngá»­a (head) lÃ  nhÆ° nhau. KhÃ´ng gian máº«u cá»§a viá»‡c tung 2 Ä‘á»“ng xu trÃªn sáº½ bao gá»“m: (head, head), (head, tail), (tail, head), vÃ  (tail, tail). MÃ¬nh sáº½ xÃ©t bÃ i toÃ¡n nhÆ° sau: BÆ°á»›c 1, ta láº­t 2 Ä‘á»“ng xu thÃ nh máº·t ngá»­a (head sáº½ lÃ  giÃ¡ trá»‹ ban Ä‘áº§u cá»§a 2 Ä‘á»‘ng xu). BÆ°á»›c 2, ta tung Ä‘á»“ng xu thá»© nháº¥t $c_1$ vÃ  kiá»ƒm tra káº¿t quáº£. VÃ  bÆ°á»›c 3, ta cÅ©ng láº¡i tung Ä‘á»“ng xu $c_1$ láº§n thá»© hai vÃ  kiá»ƒm tra káº¿t quáº£ (giáº£ sá»­ káº¿t quáº£ thu Ä‘Æ°á»£c lÃ  sau sá»‘ láº§n thá»­ Ä‘á»§ lá»›n).\nClassical Interference (Suy luáº­n xÃ¡c suáº¥t truyá»n thá»‘ng) CÃ³ thá»ƒ tháº¥y vá»›i mÃ¡y tÃ­nh truyá»n thá»‘ng (classical computer), 2 Ä‘á»“ng xu cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  2 bits ngáº«u nhiÃªn (random bits). á» bÆ°á»›c 1, bÃ i toÃ¡n sáº½ Ä‘Æ°a vá» káº¿t quáº£ (head, head). Tuy nhiÃªn, sau bÆ°á»›c hai thÃ¬ (head, head) vÃ  (tail, head) sáº½ cÃ³ xÃ¡c suáº¥t báº±ng nhau vÃ  báº±ng 0.5. PhÃ¢n phá»‘i nÃ y sáº½ khÃ´ng thay Ä‘á»•i sau bÆ°á»›c 3.\nQuantum Interference (Suy luáº­n xÃ¡c suáº¥t trÃªn mÃ¡y tÃ­nh quantum) Tuy nhiÃªn, á»Ÿ Ä‘Ã¢y sáº½ cÃ³ má»™t chÃºt khÃ¡c biá»‡t náº¿u ta biá»ƒu diá»…n bÃ i toÃ¡n trÃªn mÃ¡y tÃ­nh quantum. Hai Ä‘á»“ng xu sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng hai qubits (quantum bits). Má»—i qubit cÃ³ dáº¡ng $\\alpha \\ket{0} + \\beta \\ket{1}$, trong Ä‘Ã³ $\\ket{0}$ vÃ  $\\ket{1}$ lÃ  hai tráº¡ng thÃ¡i cÆ¡ sá»Ÿ (basis state) Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng vÃ©c-tÆ¡ tÆ°Æ¡ng á»©ng: $[1,0]^T$ vÃ  $[0,1]^T$. CÃ³ thá»ƒ tháº¥y ráº±ng $\\ket{0}$ vÃ  $\\ket{1}$ tÆ°Æ¡ng á»©ng vá»›i hai giÃ¡ trá»‹ nhá»‹ phÃ¢n 0, 1 á»Ÿ mÃ¡y tÃ­nh truyá»n thá»‘ng; tuy nhiÃªn, thay vÃ¬ Ä‘Æ°á»£c mÃ£ hÃ³a rá»i ráº¡c thÃ nh chuá»—i bit 1 hoáº·c 0, má»—i qubit cÃ³ thá»ƒ táº¡o thÃ nh má»™t tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c tráº¡ng thÃ¡i cÆ¡ sá»Ÿ theo xÃ¡c suáº¥t: $$ p(0) = |\\alpha|^2, p(1) = |\\beta|^2; \\alpha, \\beta \\in \\mathbb{C} $$ ChÃº Ã½ ráº±ng $|\\alpha|^2 + |\\beta|^2 = 1$ Ä‘á»ƒ thá»a mÃ£n xÃ¡c suáº¥t trÃªn. NhÆ° váº­y náº¿u ta coi hai máº·t cá»§a Ä‘á»“ng xu lÃ  hai tráº¡ng thÃ¡i cÆ¡ sá»Ÿ: $\\ket{head} = \\ket{0}$ vÃ  $\\ket{tail} = \\ket{1}$, thÃ¬ viá»‡c tung Ä‘á»“ng xu sáº½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng viá»‡c chÃºng ta thá»±c hiá»‡n biáº¿n Ä‘á»•i Hadamard. Viá»‡c biáº¿n Ä‘á»•i á»Ÿ Ä‘Ã¢y trÃªn mÃ¡y tÃ­nh quantum chÃ­nh lÃ  phÃ©p nhÃ¢n ma tráº­n Hadamard vá»›i tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a qubit. Trong Ä‘Ã³ biáº¿n Ä‘á»•i Hadamard cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng ma tráº­n:\n$$ H = \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{cc} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{array} \\right) $$ Tá»« Ä‘Ã³, ta cÃ³ thá»ƒ triá»ƒn khai bÃ i toÃ¡n trÃªn nhÆ° sau: 2 qubits sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh $\\ket{head}\\ket{head}$ sau bÆ°á»›c 1. á» bÆ°á»›c 2, ta nhÃ¢n ma tráº­n Hadamard vá»›i tráº¡ng thÃ¡i cá»§a qubit thá»© nháº¥t, ta cÃ³: $$ H\\ket{head}\\ket{head}= H\\ket{0}\\ket{head} = \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{cc} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right) \\ket{head} $$ $$ =\\!\\frac{1}{\\sqrt{2}}\\left( \\begin{array}{c} 1 \\\\ 1 \\end{array} \\right)\\! \\ket{head}\\! =\\! \\frac{1}{\\sqrt{2}} (\\ket{0}\\!+\\!\\ket{1})\\!\\ket{head}\\! =\\! \\frac{1}{\\sqrt{2}} (\\ket{head}\\!+\\!\\ket{tail})\\!\\ket{head} $$ $$ = \\frac{1}{\\sqrt{2}}\\ket{head}\\ket{head} + \\frac{1}{\\sqrt{2}}\\ket{tail}\\ket{head} $$ NhÆ° váº­y, ta tháº¥y giá»‘ng nhÆ° trÆ°á»ng há»£p trÃªn, sau bÆ°á»›c 2 xÃ¡c suáº¥t Ä‘áº¡t Ä‘Æ°á»£c $\\ket{head}\\ket{head}$ vÃ  $\\ket{tail}\\ket{head}$ lÃ  báº±ng nhau lÃ  cÅ©ng báº±ng $(\\frac{1}{\\sqrt{2}})^2 = 0.5$. Tuy nhiÃªn sá»± khÃ¡c biá»‡t náº±m á»Ÿ bÆ°á»›c 3, náº¿u ta tiáº¿p tá»¥c tung Ä‘á»“ng xu thá»© nháº¥t (hay thá»±c hiá»‡n biáº¿n Ä‘á»•i Hadamard), vá»›i má»™t chÃºt tÃ­nh toÃ¡n ta nháº­n Ä‘Æ°á»£c káº¿t quáº£:\n$$ \\frac{1}{\\sqrt{2}}H\\ket{head}\\ket{head} + \\frac{1}{\\sqrt{2}}H\\ket{tail}\\ket{head} = \\ket{head}\\ket{head} $$ Sau bÆ°á»›c 3 ta sáº½ luÃ´n nháº­n Ä‘Æ°á»£c $\\ket{head}\\ket{head}$, káº¿t quáº£ nÃ y khÃ¡c hoÃ n toÃ¡n khi thá»±c hiá»‡n bÃ i toÃ¡n trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng. CÃ³ thá»ƒ nÃ³i Ä‘Ã¢y lÃ  má»™t sá»± khÃ¡c nhau thÃº vá»‹ giá»¯a mÃ¡y tÃ­nh lÆ°á»£ng tá»­ vÃ  mÃ¡y tÃ­nh truyá»n thá»‘ng. KhÃ¡c vá»›i mÃ¡y tÃ­nh truyá»n thá»‘ng cÃ³ xu hÆ°á»›ng tá»‘i Ä‘a hÃ³a sá»± khÃ´ng cháº¯c cháº¯n (maximize uncertainty) vÃ¬ luÃ´n cho ra káº¿t quáº£ 50-50 giá»¯a 2 tráº¡ng thÃ¡i (head, head) vÃ  (tail, head), thÃ¬ mÃ¡y tÃ­nh lÆ°á»£ng tá»­ cho ra káº¿t quáº£ cÃ³ Ä‘á»™ khÃ´ng cháº¯c cháº¯n tháº¥p hÆ¡n. ChÃ­nh vÃ¬ lÃ½ do nÃ y, Ä‘Ã£ cÃ³ nghiÃªn cá»©u Ã¡p dá»¥ng quantum inference nhÆ° má»™t hÃ m dá»± Ä‘oÃ¡n (prediction function) trong bÃ i toÃ¡n há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t (supervised learning) [1]\nSá»± khÃ¡c nhau trÃªn cÅ©ng dáº«n ta tá»›i má»™t váº¥n Ä‘á» quan trá»ng khÃ¡c: measurement (phÃ©p Ä‘o). PhÃ©p Ä‘o chÃ­nh lÃ  cáº§u ná»‘i giá»¯a quantum vÃ  classical, giÃºp chÃºng ta Ä‘Ã¡nh giÃ¡ vÃ  phÃ¢n tÃ­ch tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a má»™t hoáº·c má»™t há»‡ qubit, vÃ  nÃ³ thÆ°á»ng mang tÃ­nh thá»‘ng kÃª xÃ¡c suáº¥t hÆ¡n lÃ  má»™t Ä‘Ã¡nh giÃ¡ Ä‘Æ¡n láº». NÃ³i cÃ¡ch khÃ¡c, phÃ©p Ä‘o cho phÃ©p chÃºng ta phÃ¡ bá» tÃ­nh chá»‘ng chÃ¢t cá»§a qubits (phÃ¡ bá» Ä‘i tá»• há»£p tuyáº¿n tÃ­nh), tá»« Ä‘Ã³ lÃ m cho tráº¡ng thÃ¡i cá»§a qubit â€˜collapseâ€™ vá» má»™t trong cÃ¡c tráº¡ng thÃ¡i cÆ¡ sá»Ÿ. VÃ­ dá»¥, thá»±c hiá»‡n phÃ©p Ä‘o má»™t qubit báº¥t ká»³ $\\ket{\\phi} = \\alpha \\ket{0} + \\beta \\ket{1}$ trÃªn cÆ¡ sá»Ÿ chuáº©n (canonical basis) thÃ¬ ta sáº½ Ä‘áº¡t Ä‘Æ°á»£c giÃ¡ trá»‹ 0 â€¦","date":1665301286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665301286,"objectID":"e81b5d40081a3db7ff1a08d56238a88b","permalink":"https://example.com/post/fair-coins/","publishdate":"2022-10-09T14:41:26+07:00","relpermalink":"/post/fair-coins/","section":"post","summary":"Ná»™i dung Giá»›i thiá»‡u bÃ i toÃ¡n Classical Interference Quantum Interference Káº¿t luáº­n TrÆ°á»›c khi Ä‘i vÃ o cÃ¡c thuáº­t toÃ¡n quantum trong há»c mÃ¡y, mÃ¬nh muá»‘n so sÃ¡nh bÃ i toÃ¡n suy luáº­n xÃ¡c suáº¥t trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng cÅ©ng nhÆ° trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­.","tags":[],"title":"BÃ i 1: Quantum vs Classical Interference: First Example","type":"post"},{"authors":[],"categories":[],"content":" Ná»™i dung Quantum Machine Learning lÃ  gÃ¬? Má»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n cá»§a Quantum Machine Learning Káº¿t luáº­n KhÃ¡c vá»›i cÃ¡c chá»§ Ä‘á» cá»§a Machine Learning hay Deep Learning khi mÃ  cÃ¡c á»©ng dá»¥ng cá»§a chÃºng Ä‘ang dáº§n trá»Ÿ nÃªn phá»• biáº¿n nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, chá»§ Ä‘á» vá» Quantum Machine Learning (hay QML) lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u má»›i vÃ  Ä‘ang Ä‘Æ°á»£c chÃº Ã½ á»Ÿ cÃ¡c cÃ´ng ty hÃ ng Ä‘áº§u tháº¿ giá»›i nhÆ° IBM hay Google. Do Ä‘Ã³, á»Ÿ bÃ i viáº¿t nÃ y ngoÃ i viá»‡c cung cáº¥p cho báº¡n Ä‘á»c cÃ¡i nhÃ¬n cá»¥ thá»ƒ Quantum Machine Learning lÃ  gÃ¬, mÃ¬nh cÅ©ng sáº½ giáº£i thÃ­ch táº¡i sao chÃºng ta láº¡i cáº§n QML vÃ  má»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n cá»¥ thá»ƒ.\nQuantum Machine Learning lÃ  gÃ¬? Náº¿u nhÆ° ai Ä‘Ã£ lÃ m quen vá»›i cÃ¡c bÃ i toÃ¡n cá»§a Machine Learning hay Deep Learning, cÃ¡c mÃ´ hÃ¬nh Ä‘ang dáº§n Ä‘Æ°á»£c xÃ¢y dá»±ng lá»›n hÆ¡n vÃ  phá»©c táº¡p hÆ¡n Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n khÃ³ (hard combinatorial optimization problems), nÃ³ dáº«n tá»›i viá»‡c tiÃªu tá»‘n ráº¥t nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n (computational resources) trong viá»‡c huáº¥n luyá»‡n cÅ©ng nhÆ° lÃ  váº­n hÃ nh. Má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh lÃ  mÃ´ hÃ¬nh GPT-3 gá»“m 175 tá»· tham sá»‘ sáº½ cáº§n tá»‘n 355 nÄƒm vÃ  gáº§n 5 triá»‡u Ä‘Ã´ náº¿u train trÃªn má»™t NVIDIA Tesla V100 GPU. Do Ä‘Ã³, trÃªn thá»±c táº¿ há» Ä‘Ã£ train GPT-3 vá»›i 1024 A100 GPUs vÃ  máº¥t 34 ngÃ y.\nTuy nhiÃªn, váº¥n Ä‘á» Ä‘Ã³ cÃ³ thá»ƒ sáº½ Ä‘Æ°á»£c giáº£i quyáº¿t vá»›i sá»± xuáº¥t hiá»‡n cá»§a mÃ¡y tÃ­nh lÆ°á»£ng tá»­ (quantum computer). MÃ¡y tÃ­nh lÆ°á»£ng tá»­ Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a theo cÃ¡c thuyáº¿t cá»§a váº­t lÃ½ lÆ°á»£ng tá»­ Ä‘á»ƒ Ä‘Æ°a ra má»™t kháº£ nÄƒng tÃ­nh toÃ¡n vÆ°á»£t trá»™i so vá»›i mÃ¡y tÃ­nh truyá»n thá»‘ng. HÃ£y láº¥y má»™t bÃ i toÃ¡n tÃ¬m kiáº¿m lÃ  má»™t vÃ­ dá»¥: giáº£ sá»­ báº¡n pháº£i tÃ¬m 1 quáº£ bÃ³ng trong 1 triá»‡u ngÄƒn kÃ©o vÃ  cÃ¢u há»i lÃ  báº¡n sáº½ pháº£i má»Ÿ qua bao nhiÃªu ngÄƒn kÃ©o trÆ°á»›c khi tÃ¬m Ä‘Æ°á»£c quáº£ bÃ³ng Ä‘Ã³? ÄÃ´i khi báº¡n sáº½ may máº¯n tÃ¬m Ä‘Æ°á»£c quáº£ bÃ³ng trong chá»‰ vÃ i láº§n thá»­ vÃ  ngÆ°á»£c láº¡i báº¡n cÅ©ng cÃ³ thá»ƒ pháº£i má»Ÿ gáº§n nhÆ° toÃ n bá»™ 1 triá»‡u ngÄƒn kÃ©o kia. Trung bÃ¬nh báº¡n sáº½ cáº§n tá»›i 500,000 lÆ°á»£t Ä‘á»ƒ tÃ¬m ra quáº£ bÃ³ng. Tuy nhiÃªn, vá»›i mÃ¡y tÃ­nh lÆ°á»£ng tá»­, báº¡n cÃ³ thá»ƒ thá»±c hiá»‡n bÃ i toÃ¡n Ä‘Ã³ trong vÃ²ng 1000 lÆ°á»£t báº±ng má»™t thuáº­t toÃ¡n Ä‘Æ°á»£c gá»i lÃ  Groverâ€™s algorithm.\nTá»« Ä‘Ã³ sá»± ra Ä‘á»i cá»§a Quantum Machine Learning nhÆ° má»™t sá»± giao thoa cá»§a cÃ¡c thuáº­t toÃ¡n trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­ vá»›i mÃ´ hÃ¬nh Machine Learning Ä‘á»ƒ cáº£i thiá»‡n cáº£ vá» máº·t tÃ­nh toÃ¡n cÅ©ng nhÆ° Ä‘á»™ chÃ­nh xÃ¡c (Ä‘Æ°á»£c gá»i lÃ  quantum advantage).\nMá»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n cá»§a Quantum Machine Learning Cho Ä‘áº¿n nÃ y Ä‘Ã£ cÃ³ khÃ¡ nhiá»u hÆ°á»›ng triá»ƒn khai QML Ä‘Æ°á»£c Ä‘á» xuáº¥t, máº·c dÃ¹ nhiá»u trong sá»‘ chÃºng váº«n chá»‰ lÃ  lÃ½ thuyáº¿t thuáº§n tÃºy vÃ  cáº§n má»™t mÃ¡y tÃ­nh lÆ°á»£ng tá»­ hoÃ n chá»‰nh Ä‘á»ƒ thá»±c nghiá»‡m; tuy nhiÃªn, cÅ©ng Ä‘Ã£ cÃ³ cÃ¡c thuáº­t toÃ¡n Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai trÃªn quy mÃ´ nhá» vÃ  chá»©ng minh Ä‘áº¡t Ä‘Æ°á»£c â€˜quantum advantageâ€™. Sau Ä‘Ã¢y mÃ¬nh sáº½ Ä‘á» cáº­p tá»›i hai hÆ°á»›ng tiá»‡p cáº­n phá»• biáº¿n cá»§a QML.\na) QRAM-based Quantum Machine Learning\nTÆ°Æ¡ng tá»± RAM (Random Access Memory) á»Ÿ cÃ¡c mÃ¡y tÃ­nh truyá»n thá»‘ng, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ giá»›i thiá»‡u má»™t â€˜quantum-versionâ€™ cá»§a RAM Ä‘Æ°á»£c gá»i lÃ  QRAM Ä‘á»ƒ xá»­ lÃ½ váº¥n Ä‘á» ghi vÃ  Ä‘á»c thÃ´ng tin trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­. CÃ³ thá»ƒ nÃ³i QRAM lÃ  má»™t pháº§n ráº¥t quan trá»ng nhiá»u thuáº­t toÃ¡n cá»§a QML. Tháº­m chÃ­ chÃºng Ä‘áº¡t Ä‘Æ°á»£c â€˜quantum advantageâ€™ lÃ  nhá» QRAM.\nMá»™t á»©ng dá»¥ng cá»¥ thá»ƒ vÃ  cÅ©ng nhÆ° Ä‘Æ°á»£c dÃ¹ng nhiá»u nháº¥t cá»§a QRAM lÃ  kháº£ nÄƒng cáº£i thiá»‡n tá»‘c Ä‘á»™ tÃ­nh toÃ¡n cá»§a tÃ­ch vÃ´ hÆ°á»›ng (dot product) hay Kernel Method - má»™t phÆ°Æ¡ng phÃ¡p quen thuá»™c cá»§a Machine Learning mÃ  Ä‘iá»ƒn hÃ¬nh lÃ  Support Vector Machine (SVM). Vá»›i sá»± can thiá»‡p cá»§a QRAM, ta cÃ³ thá»ƒ tÃ­nh tÃ­ch vÃ´ hÆ°á»›ng $x^Ty$ vá»›i Ä‘á»™ phá»©c táº¡p lÃ  $O(logN)$ so vá»›i $O(N)$ trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng, trong Ä‘Ã³ $x, y$ lÃ  cÃ¡c vectors $N$ chiá»u.\nTá»« Ä‘Ã³ cÃ¡c thuáº­t toÃ¡n Ä‘Æ°á»£c ra Ä‘á»i nhÆ° lÃ  Quantum K-Means dá»±a vÃ o QRAM Ä‘á»ƒ cÃ³ Ä‘á»™ phá»©c táº¡p $O(log(Nd))$ (so vá»›i $O(Nd)$ cá»§a thuáº­t toÃ¡n K-Means), trong Ä‘Ã³ $N$ lÃ  sá»‘ data vÃ  $d$ lÃ  sá»‘ chiá»u. Hay Quantum Support Vector Machine Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p $O(log(Nd))$ so vá»›i $O(poly(N,d))$ cá»§a thuáº­t toÃ¡n SVM bÃ¬nh thÆ°á»ng, vÃ  má»™t sá»‘ khÃ¡c: Quantum PCA, Quantum K-Medians, etc.\ná» hÆ°á»›ng tiáº¿p cáº­n nÃ y, cÃ¡c thuáº­t toÃ¡n sáº½ dá»±a vÃ o kháº£ nÄƒng tÃ­nh toÃ¡n vÆ°á»£t trá»™i cá»§a quantum computing Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ phá»©c táº¡p. Tuy nhiÃªn á»Ÿ mÃ¡y tÃ­nh lÆ°á»£ng tá»­ khÃ´ng chá»‰ cÃ³ váº­y. ThÃ´ng tin á»Ÿ Ä‘Ã³ Ä‘Æ°á»£c biá»ƒu diá»…n dá»±a theo nguyÃªn lÃ½ chá»“ng cháº­p (Superposition), thay vÃ¬ Ä‘Æ°á»£c mÃ£ hÃ³a rá»i ráº¡c thÃ nh cÃ¡c bits 0 vÃ  1, cÃ³ nghÄ©a thÃ´ng tin cÃ³ thá»ƒ tá»“n táº¡i Ä‘á»“ng thá»i á»Ÿ bit 0 vÃ  bit 1 theo má»™t phÃ¢n phá»‘i nÃ o Ä‘Ã³. Do Ä‘Ã³, â€™learning spaceâ€™ á»Ÿ mÃ¡y tÃ­nh lÆ°á»£ng tá»­ sáº½ hoÃ n toÃ n khÃ¡c vÃ  tháº­m chÃ­ Ä‘Æ°á»£c má»Ÿ rá»™ng hÆ¡n so vá»›i mÃ¡y tÃ­nh truyá»n thá»‘ng. Thá»±c táº¿ Ä‘Ã£ cÃ³ nhiá»u nghiÃªn cá»©u vá»›i má»¥c tiÃªu khÃ¡m phÃ¡ khÃ´ng gian nÃ y Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng há»c táº­p (learning capability) cá»§a mÃ´ hÃ¬nh Machine Learning vÃ  hÆ°á»›ng tiáº¿p cáº­n sau Ä‘Ã¢y lÃ  vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh cho viá»‡c nÃ y.\nb) Quantum Neural Network.\nÄÆ°á»£c thÃºc Ä‘áº©y tá»« sá»± thÃ nh cÃ´ng cá»§a máº¡ng há»c sÃ¢u (classical deep learning), máº¡ng nÆ¡-ron lÆ°á»£ng tá»­ (Quantum Neural Network, hay QNN) cÅ©ng mang nhá»¯ng nÃ©t tÆ°Æ¡ng Ä‘á»“ng vá»›i máº¡ng nÆ¡-ron truyá»n thá»‘ng (NN). ChÃºng Ä‘Æ°á»£c thiáº¿t káº¿ theo cáº¥u trÃºc feed-forward, trong Ä‘Ã³ cÃ¡c layers lÃ  cÃ¡c phÃ©p biáº¿n Ä‘á»•i Ä‘Æ¡n nháº¥t (unitary transformation). Háº§u háº¿t cáº¥u trÃºc cá»§a QNN dá»±a theo Variational Quantum Circuits hay thÆ°á»ng Ä‘Æ°á»£c gá»i Parameterised Quantum Circuits. á» Ä‘Ã³ cÃ¡c biáº¿n Ä‘á»•i trong cáº¥u trÃºc máº¡ng QNN sáº½ phá»¥ thuá»™c vÃ o tham sá»‘ $\\theta$ (learning parameters) vÃ  â€¦","date":1664552270,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664552270,"objectID":"db5dfd9db2c98204b3c7e2f0bf0ffcb8","permalink":"https://example.com/post/why-qml/","publishdate":"2022-09-30T22:37:50+07:00","relpermalink":"/post/why-qml/","section":"post","summary":"Ná»™i dung Quantum Machine Learning lÃ  gÃ¬? Má»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n cá»§a Quantum Machine Learning Káº¿t luáº­n KhÃ¡c vá»›i cÃ¡c chá»§ Ä‘á» cá»§a Machine Learning hay Deep Learning khi mÃ  cÃ¡c á»©ng dá»¥ng cá»§a chÃºng Ä‘ang dáº§n trá»Ÿ nÃªn phá»• biáº¿n nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, chá»§ Ä‘á» vá» Quantum Machine Learning (hay QML) lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u má»›i vÃ  Ä‘ang Ä‘Æ°á»£c chÃº Ã½ á»Ÿ cÃ¡c cÃ´ng ty hÃ ng Ä‘áº§u tháº¿ giá»›i nhÆ° IBM hay Google.","tags":[],"title":"BÃ i 0: Giá»›i thiá»‡u vá» Quantum Machine Learning","type":"post"}]