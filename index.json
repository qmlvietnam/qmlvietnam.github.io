
    
    
    
    
    
    
    
    
    
    
    
    
    [{"authors":null,"categories":null,"content":"Ch√†o m·ªçi ng∆∞·ªùi, c·∫£m ∆°n m·ªçi ng∆∞·ªùi gh√© qua trang blog c·ªßa m√¨nh.\nM√¨nh l√† Nguy·ªÖn Quang Tuy·∫øn, m√¨nh t·ªët nghi·ªáp ƒë·∫°i h·ªçc ng√†nh Khoa h·ªçc m√°y t√≠nh t·∫°i tr∆∞·ªùng University of Aizu, Nh·∫≠t B·∫£n. Trong th·ªùi gian l√†m ƒë·ªì √°n t·ªët nghi·ªáp li√™n quan ƒë·∫øn Quantum Machine Learning (m√¨nh t·∫°m d·ªãch l√† H·ªçc M√°y L∆∞·ª£ng T·ª≠), m√¨nh ƒë√£ c√≥ mong mu·ªën l√†m ra m·ªôt blog c√° nh√¢n vi·∫øt v·ªÅ lƒ©nh v·ª±c n√†y. Do ƒë√≥, d·ª±a theo hai ngu·ªìn c·∫£m h·ª©ng t·ª´ trang Machine Learning c∆° b·∫£n c·ªßa anh V≈© H·ªØu Ti·ªáp v√† Deep Learning c∆° b·∫£n c·ªßa anh Tu·∫•n, m√¨nh t·∫°o trang n√†y v·ªõi hai k·ª≥ v·ªçng. M·ªôt l√† gi√∫p m√¨nh t·ªïng h·ª£p ki·∫øn th·ª©c v·ªÅ Quantum Machine Learning c≈©ng gi√∫p m√¨nh n·∫Øm ch·∫Øc n·ªÅn m√≥ng trong giai ƒëo·∫°n s∆° khai c·ªßa lƒ©nh v·ª±c n√†y. Hai l√† chia s·∫ª c≈©ng nh∆∞ mong mu·ªën t·∫°o ƒë∆∞·ª£c m·ªôt c·ªông ƒë·ªìng c√°c b·∫°n ƒë·ªçc Vi·ªát Nam ti·∫øp c·∫≠n t·ªõi m·ªôt lƒ©nh v·ª±c m·ªõi nh∆∞ng c≈©ng ƒë·∫ßy tri·ªÉn v·ªçng n√†y.\nTrong qu√° tr√¨nh chu·∫©n b·ªã c≈©ng nh∆∞ l√† vi·∫øt b√†i, m√¨nh s·∫Ω c·ªë g·∫Øng √≠t ƒë·ªông ch·∫°m t·ªõi c√°c hi·ªán t∆∞·ª£ng vƒ© m√¥ ·ªü trong th·∫ø gi·ªõi l∆∞·ª£ng t·ª≠ v√† m√¨nh s·∫Ω ƒëi t·ª´ g√≥c nh√¨n c·ªßa m·ªôt computer scientist ch·ª© kh√¥ng ph·∫£i l√† m·ªôt physicist. Nh∆∞ng m√¨nh hi v·ªçng m·ªçi ng∆∞·ªùi ƒë√£ l√†m quen qua c√°c ki·∫øn th·ª©c c∆° b·∫£n c·ªßa ƒê·∫°i S·ªë Tuy·∫øn T√≠nh (v√¨ h·∫ßu h·∫øt c√°c bi·∫øn ƒë·ªïi trong m√°y t√≠nh l∆∞·ª£ng t·ª≠ l√† tuy·∫øn t√≠nh) v√† k·ªπ nƒÉng l·∫≠p tr√¨nh Python (n·∫øu c√≥ th·ªÉ m√¨nh s·∫Ω c·ªë ƒë∆∞a ra c√°c code demo sau c√°c b√†i vi·∫øt ƒë·ªÉ gi√∫p m·ªçi ng∆∞·ªùi hi·ªÉu r√µ ƒë∆∞·ª£c v·∫•n ƒë·ªÅ h∆°n).\nNgo√†i c√°c b√†i vi·∫øt ·ªü tr√™n Blogs, Newsletter l√† n∆°i m√¨nh c·∫≠p nh·∫≠t nh·ªØng th√¥ng tin m·ªõi nh·∫•t h√†ng th√°ng li√™n quan t·ªõi Quantum Machine Learning (c√°c b√†i b√°o m·ªõi, h·ªôi ngh·ªã, workshop, etc.)\nM√¨nh vui l√≤ng ti·∫øp nh·∫≠n m·ªçi √Ω ki·∫øn th·∫£o lu·∫≠n c·ªßa m·ªçi ng∆∞·ªùi qua email quantummachinelearning.vietnam@gmail.com\nM√¨nh xin ch√¢n th√†nh c·∫£m ∆°n.\n","date":1664496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664496000,"objectID":"ed769a13ba9f25479e7927567f68c77f","permalink":"https://example.com/authors/intro/","publishdate":"2022-09-30T00:00:00Z","relpermalink":"/authors/intro/","section":"authors","summary":"Here we describe how to add a page to your site.","tags":null,"title":"QML Vietnam","type":"authors"},{"authors":[],"categories":[],"content":"News üì∞ Hybrid Quantum-Classical Algorithm Shows Promise for Unraveling the Protein Folding Problem\nQubit Pharmaceuticals Works With NVIDIA to Create Hybrid Computing Platform to Accelerate Drug Discovery with NVIDIA\nIonQ and Hyundai Motors Expand Quantum Computing Partnership, Continuing Pursuit of Automotive Innovation\nVideos üìΩÔ∏è Introduction to Quantum Computing: From Layperson to Programmer in 30 Steps Publications üìÉ Quantum Machine Learning: from physics to software engineering Abstract: Quantum machine learning (QML) is a new, rapidly growing, and fascinating area of research where quantum information science and quantum technologies meet novel machine learning and artificial intelligent facilities. A comprehensive analysis of the main directions of current QML methods and approaches is performed in this review. The aim of our work is twofold. First, we show how classical machine learning approach can help improve the facilities of quantum computers and simulators available today. It is most important due to the modern noisy intermediate-scale quantum (NISQ) era of quantum technologies. In particular, the classical machine learning approach allows optimizing quantum hardware for achieving desired quantum states by implementing quantum devices. Second, we discuss how quantum algorithms and quantum computers may be useful for solving keystone classical machine learning tasks. Currently, quantum-inspired algorithms, which use a quantum approach to classical information processing, represent a powerful tool in software engineering for improving classical computation capacities. In this work, we discuss various quantum neural network capabilities that can be implemented in quantum-classical training algorithms for variational circuits. It is expected that quantum computers will be involved in routine machine learning procedures. In this sense, we are showing how it is essential to elucidate the speedup problem for random walks on arbitrary graphs, which are used in both classical and quantum algorithms. Quantum technologies enhanced by machine learning in fundamental and applied quantum physics, as well as quantum tomography and photonic quantum computing, are also covered.\nDemystify Problem-Dependent Power of Quantum Neural Networks on Multi-Class Classification Abstract: Quantum neural networks (QNNs) have become an important tool for understanding the physical world, but their advantages and limitations are not fully understood. Some QNNs with specific encoding methods can be efficiently simulated by classical surrogates, while others with quantum memory may perform better than classical classifiers. Here we systematically investigate the problem-dependent power of quantum neural classifiers (QCs) on multi-class classification tasks. Through the analysis of expected risk, a measure that weighs the training loss and the generalization error of a classifier jointly, we identify two key findings: first, the training loss dominates the power rather than the generalization ability; second, QCs undergo a U-shaped risk curve, in contrast to the double-descent risk curve of deep neural classifiers. We also reveal the intrinsic connection between optimal QCs and the Helstrom bound and the equiangular tight frame. Using these findings, we propose a method that uses loss dynamics to probe whether a QC may be more effective than a classical classifier on a particular learning task. Numerical results demonstrate the effectiveness of our approach to explain the superiority of QCs over multilayer Perceptron on parity datasets and their limitations over convolutional neural networks on image datasets. Our work sheds light on the problem-dependent power of QNNs and offers a practical tool for evaluating their potential merit.\nOn Machine Learning Knowledge Representation In The Form Of Partially Unitary Operator. Knowledge Generalizing Operator Abstract: A new form of ML knowledge representation with high generalization power is developed and implemented numerically. Initial IN attributes and OUT class label are transformed into the corresponding Hilbert spaces by considering localized wavefunctions. A partially unitary operator optimally converting a state from IN Hilbert space into OUT Hilbert space is then built from an optimization problem of transferring maximal possible probability from IN to OUT, this leads to the formulation of a new algebraic problem. Constructed Knowledge Generalizing Operator U can be considered as a IN to OUT quantum channel; it is a partially unitary rectangular matrix of the dimension dim(OUT)√ódim(IN) transforming operators as AOUT=UAINU‚Ä†. Whereas only operator U projections squared are observable ‚ü®OUT|U|IN‚ü©2 (probabilities), the fundamental equation is formulated for the operator U itself. This is the reason of high generalizing power of the approach; the situation is the same as for the Schr√∂dinger equation: we can only measure œà2, but the equation is written for œà itself.\nImproving ‚Ä¶","date":1673010540,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1673010540,"objectID":"8bba7423d6037593ef8b5cf1f8a3656c","permalink":"https://example.com/newsletter/december-2022/","publishdate":"2023-01-06T20:09:00+07:00","relpermalink":"/newsletter/december-2022/","section":"newsletter","summary":"News üì∞ Hybrid Quantum-Classical Algorithm Shows Promise for Unraveling the Protein Folding Problem\nQubit Pharmaceuticals Works With NVIDIA to Create Hybrid Computing Platform to Accelerate Drug Discovery with NVIDIA\nIonQ and Hyundai Motors Expand Quantum Computing Partnership, Continuing Pursuit of Automotive Innovation","tags":[],"title":"December 2022","type":"newsletter"},{"authors":[],"categories":[],"content":"N·ªôi dung Gi·ªõi thi·ªáu Quantum Support Vector Machine Code Gi·ªõi thi·ªáu Support Vector Machine, hay th∆∞·ªùng ƒë∆∞·ª£c g·ªçi l√† SVM, c√≥ l·∫Ω l√† m·ªôt trong nh·ªØng thu·∫≠t to√°n h·ªçc m√°y c√≥ gi√°m s√°t ph·ªï bi·∫øn nh·∫•t. V·ªÅ c∆° b·∫£n, trong b√†i to√°n ph√¢n lo·∫°i hai classes (binary classification problem), ch√∫ng s·∫Ω c·∫ßn ƒëi t√¨m m·ªôt si√™u ph·∫≥ng ph√¢n chia ch√≠nh x√°c hai classes ƒë√≥. Tuy nhi√™n t·ªìn t·∫°i r·∫•t nhi·ªÅu m·∫∑t ph·∫≥ng nh∆∞ v·∫≠y (H√¨nh 1). Thu·∫≠t to√°n SVM s·∫Ω gi√∫p ch√∫ng ta ƒëi t√¨m m·∫∑t ph·∫≥ng t·ªëi ∆∞u nh·∫•t. V·ªÅ chi ti·∫øt v·ªÅ SVM, b·∫°n c√≥ th·ªÉ tham kh·∫£o ·ªü ƒë√¢y\nH√¨nh 1: Source Cho v√©c-t∆° $x_i \\in \\mathcal{R}^{N}$ v·ªõi nh√£n $y_i = \\{1,-1\\}$ t∆∞∆°ng ·ª©ng, v·ªõi $i = 1,‚Ä¶,M$ v√† $M$ l√† s·ªë ƒëi·ªÉm d·ªØ li·ªáu cho tr∆∞·ªõc. T·ª´ ƒë√≥ thu·∫≠t to√°n SVM s·∫Ω t√¨m m·ªôt m·∫∑t si√™u ph·∫≥ng sao kho·∫£ng c√°ch gi·ªØa c√°c ƒë∆∞·ªùng bi√™n c·ªßa t·ª´ng class t∆∞∆°ng ·ª©ng l√† l·ªõn nh·∫•t, nh∆∞ H√¨nh 2.\nH√¨nh 2 Gi·∫£ s·ª≠ m·∫∑t si√™u ph·∫≥ng ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng m·ªôt v√©c-t∆° tham s·ªë $\\overrightarrow{\\theta} \\in \\mathcal{R}^n$ v√† $b$, th√¨ m√¨nh c√≥ v√©c-t∆° $\\overrightarrow{x} \\in \\mathcal{R}^n$ n·∫±m tr√™n m·∫∑t si√™u ph·∫≥ng n√†y s·∫Ω ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng: $$ \\theta^{T}x - b = 0 $$ T·ª´ ƒë√¢y, thu·∫≠t to√°n SVM s·∫Ω t√¨m $\\overrightarrow{\\theta}$ v√† $b$ sao cho kho·∫£ng c√°ch $\\frac{2}{||\\theta ||_2}$ l·ªõn nh·∫•t v√† t√°ch ri√™ng ƒë∆∞·ª£c hai ƒë∆∞·ªùng bi√™n theo: $$ \\left\\{ \\begin{array}{rcl} \\theta^{T}x - b \\geq 1 \u0026amp; \\forall y = 1 \\\\ \\theta^{T}x - b \\leq 1 \u0026amp; \\forall y = -1 \\end{array}\\right. $$ $$ \\longrightarrow y(\\theta^{T}x - b) \\geq 1 $$ Nh∆∞ v·∫≠y, b√†i to√°n t·ªëi ∆∞u c·ªßa ch√∫ng ta s·∫Ω ƒë∆∞·ª£c tri·ªÉn khai nh∆∞ sau:\n$$ \\min_{\\theta} \\frac{1}{2} \\|\\theta \\|_2 \\quad (1) $$ sao cho $$ \\quad y_i(\\theta^{T}x_i - b) - 1 \\geq 0; \\forall i= \\{1,2,...,M \\} $$ ƒê·∫øn ƒë√¢y ch√∫ng ta th∆∞·ªùng ƒë∆°n gi·∫£n h√≥a b√†i to√°n t·ªëi ∆∞u n√†y theo d·∫°ng dual r·ªìi √°p d·ª•ng kernel method ƒë·ªÉ gi·∫£i. Tuy nhi√™n, c√°ch tr√™n l·∫°i d∆∞·ªùng nh∆∞ kh√¥ng th·ªÉ √°p d·ª•ng tr√™n m·ªôt thu·∫≠t to√°n l∆∞·ª£ng t·ª≠ v√¨ ƒë·ªô ph·ª©c t·∫°p c·ªßa ch√∫ng (m·ªçi ng∆∞·ªùi c√≥ th·ªÉ ƒë·ªçc r√µ h∆°n v·ªÅ ph·∫ßn n√†y ·ªü Chapter 5 Santanu Pattanayak et.al.). M·∫∑t kh√°c, m·ªôt d·∫°ng bi·∫øn ƒë·ªïi kh√°c ƒë∆∞·ª£c g·ªçi l√† least square SVM c√≥ th·ªÉ gi·∫£i quy·∫øt ƒë∆∞·ª£c v·∫•n ƒë·ªÅ n√†y trong m√°y t√≠nh l∆∞·ª£ng t·ª≠ v√† t·ª´ ƒë√≥ ph√°t tri·ªÉn l√™n th√†nh thu·∫≠t to√°n QSVM nh∆∞ hi·ªán t·∫°i.\nQuantum Support Vector Machine Kh√°c v·ªõi dual form c·ªßa SVM, d·∫°ng least square SVM bi·∫øn ƒë·ªïi r√†ng bu·ªôc $y_i(\\theta^T x_i -b) \\geq 1$ th√†nh: $$ y_i(\\theta^T x_i -b) = 1-e_i $$ trong ƒë√≥ sai s·ªë $e_i \\geq 0$ v·ªõi m·ªói ƒëi·ªÉm d·ªØ li·ªáu t∆∞∆°ng ·ª©ng $(x_i, y_i)$. ·ªû d∆∞·ªõi d·∫°ng bi·∫øn ƒë·ªïi n√†y, ngo√†i vi·ªác t√¨m min c·ªßa $\\frac{1}{2} ||\\theta||_2$ nh∆∞ ·ªü c√¥ng th·ª©c (1), th√¨ trong b√†i to√°n t·ªëi ∆∞u, m√¨nh c≈©ng c·∫ßn gi·∫£m thi·ªÉu sai s·ªë, $e_i$. Do ƒë√≥ b√†i to√°n t·ªëi ∆∞u m·ªõi c·ªßa least square SVM t∆∞∆°ng ƒë∆∞∆°ng:\n$$ \\min_{\\theta} \\frac{1}{2} \\|\\theta \\|_2 + \\frac{\\gamma}{2}\\sum_{i=1}^{M} e_i^2 \\quad (2) $$ sao cho $$ y_i(\\theta^{T}x_i - b) = 1 - e_i; \\forall i= \\{1,2,...,M \\}; e_i \\geq 0 \\quad (*) $$ V√¨ $y_i \\in \\{1, -1\\}$, n√™n $y_i^2 = 1$, thay v√†o (*), ta c√≥ $$ y_i^2(\\theta^{T}x_i - b) = y_i - y_ie_i $$ $$ \\Leftrightarrow \\theta^{T}x_i - b = y_i - y_ie_i $$ $$ \\Leftrightarrow y_i - (\\theta^{T}x_i - b) = y_ie_i $$ M√† $y_i \\in \\{1, -1\\}$, n√™n $y_ie_i = e_i$ ho·∫∑c $-e_i$. Do ƒë√≥ m√¨nh ho√†n to√†n x√≥a b·ªè r√†ng bu·ªôc $e_i \\geq 0$ trong (*) ƒë·ªÉ c√≥ r√†ng bu·ªôc m·ªõi\n$$ y_i - (\\theta^{T}x_i - b) = e_i ; \\forall i= \\{1,2,...,M \\} \\quad (**) $$ T·ª´ ƒë√¢y ƒë·ªÉ gi·∫£i b√†i to√°n t·ªëi ∆∞u tr√™n, m√¨nh s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p nh√¢n t·ª≠ Lagrange (Lagrange multipliers): $$ \\min L(\\theta, b, \\alpha, e) = \\frac{1}{2} \\|\\theta \\|_2 + \\frac{\\gamma}{2}\\sum_{i=1}^{M} e_i^2 - \\sum_{i=1}^{M}\\alpha_i[(\\theta^Tx_i-b)-y_i+e_i] $$ trong ƒë√≥ $\\alpha_i$ l√† c√°c nh√¢n t·ª≠ Lagrange. Gi·∫£i b√†i to√†n t√¨m min tr√™n ta ƒë∆∞·ª£c: $$ \\left\\{ \\begin{array}{rcl} \\nabla_{\\theta} L = 0 \\\\ \\frac{\\partial L}{\\partial b} = 0 \\\\ \\nabla_{e} L = 0 \\\\ \\frac{\\partial L}{\\partial \\alpha_i} = 0 \\end{array}\\right. $$ $$ \\Leftrightarrow \\left\\{ \\begin{array}{rcl} \\theta - \\sum_{i=1}^{M}\\alpha_ix_i = 0 \\quad (3)\\\\ \\sum_{i=1}^{M}\\alpha_ix_i = 0 \\quad (4)\\\\ \\gamma e - \\alpha = 0 \\quad (5) \\\\ (\\theta^Tx_i-b)-y_i+e_i = 0 \\quad (6) \\end{array}\\right. $$ Thay gi√° tr·ªã c·ªßa $\\theta$ v√† $e$ ·ªü (3) v√† (5) v√†o (6), ta c√≥: $$ y_i = (\\sum_{j=1}^{M}\\alpha_jx_j)x_i-b - \\alpha_i \\gamma^{-1} \\quad (7) $$ G·ªçi $K$ m√† m·ªôt ma tr·∫≠n $M\\times M$ v·ªõi c√°c ph·∫ßn t·ª≠ $K_{ij} = x_i x_j$. Ta c√≥ th·ªÉ bi·∫øn ƒë·ªïi c√¥ng th·ª©c (4) v√† (7) d∆∞·ªõi d·∫°ng ph∆∞∆°ng tr√¨nh: $$ \\left[ \\begin{array}{cc} 0 \u0026amp; \\mathbb{1}^T \\\\ \\mathbb{1} \u0026amp; K + \\gamma^{-1} \\end{array} \\right] \\left[ \\begin{array}{c} -b \\\\ \\alpha \\end{array} \\right] = \\left[ \\begin{array}{c} 0 \\\\ Y \\end{array} \\right] $$ $Y$ l√† m·ªôt ma tr·∫≠n c·ªôt v·ªõi c√°c ph√¢n t·ª≠ t∆∞∆°ng ·ª©ng l√† nh√£n $y_i$.\nT·ª´ ƒë√¢y m√¨nh ƒë·∫´ bi·∫øn ƒë·ªïi b√†i to√°n SVM th√†nh m·ªôt d·∫°ng c·ªßa ph∆∞∆°ng tr√¨nh $Ax = b$, do ƒë√≥ thu·∫≠t to√°n HHL c·ªßa B√†i 5 s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ ho√†n th√†nh thu·∫≠t to√°n QSVM c·ªßa ch√∫ng ta.\nCode V·ªõi thu·∫≠t to√°n QSVM, m√¨nh c√≥ th·ªÉ tri·ªÉn khai n√≥ m·ªôt c√°ch ƒë∆°n gi·∫£n v·ªõi v√†i d√≤ng code s·ª≠ d·ª•ng IBM‚Äôs Qiskit.\n·ªû ƒë√¢y, m√¨nh s·∫Ω d·ª•ng m·ªôt b·ªô d∆∞ li·ªáu th·∫≠t c·ªßa b√†i to√°n d∆∞ ƒëo√°n ung th∆∞ v√∫ (breast cancer). ƒê·ªÉ ƒë∆°n gi·∫£n h√≥a, m√¨nh s·ª≠ d·ª•ng thu·∫≠t to√°n PCA ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu v·ªÅ 2\nB√†i to√°n QSVM c√≥ th·ªÉ tri·ªÉn khai nh∆∞ sau:\n# G·ªçi c√°c modules c·∫ßn thi·∫øt import matplotlib.pyplot as plt import ‚Ä¶","date":1672215081,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1672215081,"objectID":"7068a1cc37fc9d4c01e95c53655d2ea5","permalink":"https://example.com/post/qsvm/","publishdate":"2022-12-28T15:11:21+07:00","relpermalink":"/post/qsvm/","section":"post","summary":"N·ªôi dung Gi·ªõi thi·ªáu Quantum Support Vector Machine Code Gi·ªõi thi·ªáu Support Vector Machine, hay th∆∞·ªùng ƒë∆∞·ª£c g·ªçi l√† SVM, c√≥ l·∫Ω l√† m·ªôt trong nh·ªØng thu·∫≠t to√°n h·ªçc m√°y c√≥ gi√°m s√°t ph·ªï bi·∫øn nh·∫•t.","tags":[],"title":"B√†i 7: Quantum Support Vector Machine","type":"post"},{"authors":[],"categories":[],"content":"N·ªôi dung Gi·ªõi thi·ªáu Quantum PCA Gi·ªõi thi·ªáu Thu·∫≠t to√°n Principal Component Analysis (hay PCA) l√† m·ªôt trong nh·ªØng k·ªπ thu·∫≠t quan tr·ªçng trong Machine Learning ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu (Dimensionality Reduction). Th√¥ng th∆∞·ªùng data hay c√°c feature vectors ƒë∆∞·ª£c bi·ªÉu di·ªÖn tr√™n kh√¥ng gian ƒëa chi·ªÅu, th·∫≠m ch√≠ s·ªë chi·ªÅu c√≥ th·ªÉ l√™n t·ªõi v√†i ngh√¨n. Vi·ªác t√≠nh to√°n c≈©ng nh∆∞ l∆∞u tr·ªØ tr√™n b·ªô d·ªØ li·ªáu nh∆∞ v·∫≠y s·∫Ω g·∫∑p r·∫•t nhi·ªÅu kh√≥ khƒÉn. Do ƒë√≥, vi·ªác gi·∫£m chi·ªÅu d·ªØ li·ªáu l√† m·ªôt b∆∞·ªõc quan tr·ªçng trong nhi·ªÅu b√†i to√°n, v√† ƒëi·ªÉn h√¨nh ·ªü ƒë√¢y m√¨nh s·∫Ω ƒë·ªÅ c·∫≠p t·ªõi thu·∫≠t to√°n PCA.\nThu·∫≠t to√°n PCA ƒë∆∞·ª£c ph√°t tri·ªÉn d·ª±a theo m√¥ h√¨nh tuy·∫øn t√≠nh ƒë·ªÉ gi·ªØ l·∫°i $\\mathcal{K}$ ph·∫ßn t·ª≠ quan tr·ªçng nh·∫•t sao cho $\\mathcal{K} \u0026lt; \\mathcal{D}$ (s·ªë chi·ªÅu c·ªßa d·ªØ li·ªáu). Trong ƒë√≥, t·∫ßm quan tr·ªçng c·ªßa c√°c ph·∫ßn t·ª≠ ƒë∆∞·ª£c ƒë√°nh gi√° d·ª±a tr√™n gi√° tr·ªã ri√™ng (eigenvalues) c·ªßa ma tr·∫≠n ph∆∞∆°ng sai (covariance matrix) c·ªßa d·ªØ li·ªáu. Nh∆∞ng v·ªõi c√°c b·ªô d·ªØ li·ªáu l·ªõn, vi·ªác t√≠nh to√°n ma tr·∫≠n ph∆∞∆°ng sai hay ph√¢n r√£ tr·ªã ri√™ng c≈©ng l√† m·ªôt th√°ch th·ª©c. Th·ª±c t·∫ø, v·ªõi m·ªôt b·ªô d·ªØ li·ªáu g·ªìm $\\mathcal{N}$ v√©c-t∆° v√† m·ªói v√©c-t∆° ƒë∆∞·ª£c bi·ªÉn di·ªÖn tr√™n kh√¥ng gian $\\mathcal{D}$ chi·ªÅu th√¨ ta c·∫ßn $O(\\mathcal{D}^2\\mathcal{N})$ ƒë·ªÉ t√≠nh ma tr·∫≠n ph∆∞∆°ng sai v√† $O(\\mathcal{D}^3)$ cho vi·ªác ph√¢n r√£ tr·ªã ri√™ng. Nh∆∞ v·∫≠y, ƒë·ªô ph·ª©c t·∫°p c·ªßa thu·∫≠t to√°n PCA s·∫Ω l√† $O(\\mathcal{D}^2\\mathcal{N}+\\mathcal{D}^3)$.\nTrong b√†i vi·∫øt n√†y, m√¨nh s·∫Ω gi·ªõi thi·ªáu m·ªôt ph∆∞∆°ng ph√°p l∆∞·ª£ng t·ª≠ ƒë·ªÉ gi·∫£i quy·∫øt b√†i to√°n tr√™n v·ªõi ƒë·ªô ph·ª©c t·∫°p $O(\\mathcal{D}poly(\\log{\\mathcal{D}}))$ nh·ªù v√†o kh·∫£ nƒÉng t√≠nh to√°n v∆∞·ª£t tr·ªôi c·ªßa m√°y t√≠nh l∆∞·ª£ng t·ª≠: Quantum Principal Component Analysis (hay qPCA).\nQuantum PCA X√©t t·∫≠p d·ªØ li·ªáu $\\mathbb{X}$ g·ªìm $\\mathcal{N}$ v√©c-t∆°. Gi·ªëng v·ªõi thu·∫≠t to√°n PCA th√¥ng th∆∞·ªùng, qPCA c≈©ng c√≥ b∆∞·ªõc chu·∫©n h√≥a d·ªØ li·ªáu sao cho k·ª≥ v·ªçng c·ªßa d·ªØ li·ªáu l√† 0:\n$$ x_i \\rightarrow \\frac{x_i - \\frac{1}{\\mathcal{N}}\\sum_{i=1}^{\\mathcal{N}}x_i}{||x_i||_2} . $$ T·ª´ ƒë√¢y ta ho√†n to√†n c√≥ th·ªÉ bi·ªÉu di·ªÖn d·ªØ li·ªáu trong m√°y t√≠nh l∆∞·ª£ng t·ª≠ s·ª≠ d·ª•ng ph∆∞∆°ng ph√°p Amplitude Embedding nh∆∞ m√¨nh ƒë√£ ƒë·ªÅ c·∫≠p ·ªü B√†i 2:\n$$ \\ket{x_i} = \\sum_{k=0}^{\\mathcal{D}-1}x_{ik}\\ket{k}. $$ Gi·∫£ s·ª≠ ta g·ªçi $\\rho_i = \\ket{x_i}\\bra{x_i}$ l√† outer product c·ªßa v√©c-t∆° $\\ket{x_i}$. $$ \\rho_i = \\sum_{k=0}^{\\mathcal{D}-1}\\sum_{m=0}^{\\mathcal{D}-1}x_{ik}x_{im}\\ket{k}\\bra{m}, $$ Do ƒë√≥ ta x√©t tr√™n to√†n b·ªô t·∫≠p d·ªØ li·ªáu $\\mathcal{N}$ v√©c-t∆°, ta c√≥: $$ \\rho = \\frac{1}{\\mathcal{N}}\\sum_{i=1}^{\\mathcal{N}}\\rho_i = \\frac{1}{\\mathcal{N}}\\sum_{i=1}^{\\mathcal{N}}\\sum_{k=0}^{\\mathcal{D}-1}\\sum_{m=0}^{\\mathcal{D}-1}x_{ik}x_{im}\\ket{k}\\bra{m}, $$ Nh∆∞ m·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y th√¨ amplitude c·ªßa $\\rho$ ch√≠nh l√† c√°c ph·∫ßn t·ª≠ t∆∞∆°ng ·ª©ng c·ªßa ma tr·∫≠n hi·ªáp ph∆∞∆°ng sai t·∫°i v·ªã tr√≠ $(k,m)$.\nNh∆∞ v·∫≠y vi·ªác c√≤n l·∫°i c·ªßa ch√∫ng ta c·∫ßn ph·∫£i l√†m l√† t√¨m c√°c tr·ªã ri√™ng v√† v√©c-t∆° ri√™ng t∆∞∆°ng ·ª©ng c·ªßa $\\rho$. T·ª´ ƒë√≥, b√†i to√°n ƒë∆∞a ta t·ªõi v·ªõi thu·∫≠t to√°n Quantum Phase Estimation (hay QPE), nh∆∞ m√¨nh ƒë·ªÉ c·∫≠p ·ªü B√†i 4. Trong thu·∫≠t to√°n QPE, cho m·ªôt ma tr·∫≠n ƒë∆°n nh·∫•t $U$ v√† m·ªôt trong nh·ªØng v√©c-t∆° ri√™ng c·ªßa n√≥ th√¨ ta lu√¥n t√≠nh ƒë∆∞·ª£c gi√° tr·ªã ri√™ng t∆∞∆°ng ·ª©ng c·ªßa v√©c-t∆° ƒë√≥. Tuy nhi√™n, $\\rho$ kh√¥ng l√† ma tr·∫≠n ƒë∆°n nh·∫•t (unitary matrix), n√™n m√¨nh c·∫ßn d√πng m·ªôt k·ªπ thu·∫≠t kh√°c ƒë·ªÉ bi·∫øn ƒë·ªïi: $\\rho \\longrightarrow U$. V√¨ $\\rho$ l√† m·ªôt ma tr·∫≠n Hermitian (ch·ª©ng m√≠nh ·ªü Box 1), gi·ªëng nh∆∞ng c√°ch ·ªü B√†i 5, m√¨nh c√≥ th·ªÉ ch·ªçn ma tr·∫≠n ƒë∆°n nh·∫•t $U = e^{i\\rho t}$.\nBox 1: Ma tr·∫≠n $H$ ƒë∆∞·ª£c g·ªçi l√† Hermitian n·∫øu $H = H^{\\dagger}$. Ta c√≥:\n$$ \\rho = \\frac{1}{\\mathcal{N}}\\sum_{i=1}^{\\mathcal{N}}\\sum_{k=0}^{\\mathcal{D}-1}\\sum_{m=0}^{\\mathcal{D}-1}x_{ik}x_{im}\\ket{k}\\bra{m}, $$\nD·ªÖ th·∫•y $\\rho$ ƒë·ªëi x·ª©ng, v√†:\n$$ \\rho^{\\dagger} = \\frac{1}{\\mathcal{N}}\\sum_{i=1}^{\\mathcal{N}}\\sum_{k=0}^{\\mathcal{D}-1}\\sum_{m=0}^{\\mathcal{D}-1}x_{ik}^{*}x_{im}^{*}\\ket{k}\\bra{m} $$\nTrong ƒë√≥, $x_{ik}^{*}$ l√† s·ªë li√™n h·ª£p c·ªßa $x_{ik}$. Tuy nhi√™n, ·ªü ƒë√¢y ta x√©t v√©c-t∆° $\\overrightarrow{x} \\in R$ n√™n $x_{ik}^{*} = x_{ik}$. Do ƒë√≥, $\\rho = \\rho^{\\dagger}$\nX√©t ma tr·∫≠n Hermitian $\\rho = \\sum_{j} \\lambda_j \\ket{\\phi_j}\\bra{\\phi_j}$ (theo ph√¢n t√≠ch ph·ªï - spectral decomposition), trong ƒë√≥ $\\ket{\\phi_j}$ v√† $\\lambda_j$ l√† v√©c-t∆° ri√™ng v√† gi√° tr·ªã ri√™ng t∆∞∆°ng ·ª©ng c·ªßa $\\rho$, ta c√≥: $$ U = e^{i\\rho t} = \\sum_{j} e^{i\\lambda_j t} \\ket{\\phi_j}\\bra{\\phi_j} = \\sum_{j} e^{2\\pi i\\frac{\\lambda_j t}{2\\pi}} \\ket{\\phi_j}\\bra{\\phi_j} $$ M√¨nh ƒë·∫∑t $\\tilde{\\lambda_j} = \\frac{\\lambda_j t}{2\\pi}$, th√¨ b√†i to√°n s·∫Ω ƒë∆∞·ª£c ƒë∆∞a v·ªÅ khu√¥n m·∫´u c·ªßa thu·∫≠t to√°n QPE. Tuy nhi√™n thu·∫≠t to√°n QPE y√™u c·∫ßu ta bi·∫øt tr∆∞·ªõc v√©c-t∆° ri√™ng $\\ket{\\phi_j}$ n√™n thay v√†o ƒë√≥ ta c√≥ th·ªÉ d√πng m·ªôt ƒëi·ªÉm d·ªØ li·ªáu b·∫•t k·ª≥ $x_i=\\sum_{j} \\alpha_{ij} \\ket{\\phi_j}$, trong ƒë√≥ $\\alpha_{ij} = \\left\\langle \\phi_j \\vert x_i \\right\\rangle$. Do ƒë√≥ ta c√≥ th·ªÉ bi·ªÉu di·ªÖn thu·∫≠t to√°n QPE nh∆∞ sau: $$ \\text{QPE:} \\ket{0}^{\\otimes n} \\ket{x_i} \\rightarrow \\sum_{j}^{\\mathcal{D}} \\alpha_{ij} \\ket{\\tilde{\\lambda_j}} \\ket{\\phi_j} $$ Kh√¥ng m·∫•t t√≠nh t√¥ng qu√°t, ta thay $\\ket{x_i}$ b·∫±ng $\\ket{x_i}\\bra{x_i}$, ta ƒë∆∞·ª£c: $$ \\text{QPE:} \\ket{0}^{\\otimes n} \\otimes \\rho_i \\rightarrow \\sum_{j}^{\\mathcal{D}} |\\alpha_{ij}|^2 \\ket{\\tilde{\\lambda_j}}\\bra{\\tilde{\\lambda_j}} \\otimes \\ket{\\phi_j}\\bra{\\phi_j} $$ hay, $$ \\text{QPE:} \\ket{0}^{\\otimes n} ‚Ä¶","date":1671432686,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1671432686,"objectID":"57c6d9e773abe9de2b4d829ef55276bb","permalink":"https://example.com/post/qpca/","publishdate":"2022-12-19T13:51:26+07:00","relpermalink":"/post/qpca/","section":"post","summary":"N·ªôi dung Gi·ªõi thi·ªáu Quantum PCA Gi·ªõi thi·ªáu Thu·∫≠t to√°n Principal Component Analysis (hay PCA) l√† m·ªôt trong nh·ªØng k·ªπ thu·∫≠t quan tr·ªçng trong Machine Learning ƒë·ªÉ gi·∫£m chi·ªÅu d·ªØ li·ªáu (Dimensionality Reduction).","tags":[],"title":"B√†i 6: Quantum Principal Component Analysis","type":"post"},{"authors":[],"categories":[],"content":"News üì∞ D-Wave Says Revenue Up, Added More Commercial Customers in Q3\nIonQ and Hyundai Motors Expand Quantum Computing Partnership, Continuing Pursuit of Automotive Innovation\nQNLP at The Beeb ‚Äî Quantinuum, BBC Join Consortium to Explore Real-World Uses for Quantum Natural Language Processing\nUnilever‚Äôs Head of R\u0026amp;D Alberto Prado Talks Quantum at IoT World \u0026amp; The AI Summit Austin\nHackathon and Internship üéì 2023 Quantum Computing Summer School Fellowship provided by Los Alamos National Laboratory\n2023 Zapata Quantum Computing Internship\n2023 IBM Quantum Summer Internship\nChicago Quantum Exchange Intership\nPublications üìÉ Reservoir Computing via Quantum Recurrent Neural Networks Abstract: Recent developments in quantum computing and machine learning have propelled the interdisciplinary study of quantum machine learning. Sequential modeling is an important task with high scientific and commercial value. Existing VQC or QNN-based methods require significant computational resources to perform the gradient-based optimization of a larger number of quantum circuit parameters. The major drawback is that such quantum gradient calculation requires a large amount of circuit evaluation, posing challenges in current near-term quantum hardware and simulation software. In this work, we approach sequential modeling by applying a reservoir computing (RC) framework to quantum recurrent neural networks (QRNN-RC) that are based on classical RNN, LSTM and GRU. The main idea to this RC approach is that the QRNN with randomly initialized weights is treated as a dynamical system and only the final classical linear layer is trained. Our numerical simulations show that the QRNN-RC can reach results comparable to fully trained QRNN models for several function approximation and time series prediction tasks. Since the QRNN training complexity is significantly reduced, the proposed model trains notably faster. In this work we also compare to corresponding classical RNN-based RC implementations and show that the quantum version learns faster by requiring fewer training epochs in most cases. Our results demonstrate a new possibility to utilize quantum neural network for sequential modeling with greater quantum hardware efficiency, an important design consideration for noisy intermediate-scale quantum (NISQ) computers.\nBenchmarking Adversarially Robust Quantum Machine Learning at Scale Abstract: Machine learning (ML) methods such as artificial neural networks are rapidly becoming ubiquitous in modern science, technology and industry. Despite their accuracy and sophistication, neural networks can be easily fooled by carefully designed malicious inputs known as adversarial attacks. While such vulnerabilities remain a serious challenge for classical neural networks, the extent of their existence is not fully understood in the quantum ML setting. In this work, we benchmark the robustness of quantum ML networks, such as quantum variational classifiers (QVC), at scale by performing rigorous training for both simple and complex image datasets and through a variety of high-end adversarial attacks. Our results show that QVCs offer a notably enhanced robustness against classical adversarial attacks by learning features which are not detected by the classical neural networks, indicating a possible quantum advantage for ML tasks. Contrarily, and remarkably, the converse is not true, with attacks on quantum networks also capable of deceiving classical neural networks. By combining quantum and classical network outcomes, we propose a novel adversarial attack detection technology. Traditionally quantum advantage in ML systems has been sought through increased accuracy or algorithmic speed-up, but our work has revealed the potential for a new kind of quantum advantage through superior robustness of ML models, whose practical realisation will address serious security concerns and reliability issues of ML algorithms employed in a myriad of applications including autonomous vehicles, cybersecurity, and surveillance robotic systems.\nQuantum Feature Maps for Graph Machine Learning on a Neutral Atom Quantum Processor Abstract: Using a quantum processor to embed and process classical data enables the generation of correlations between variables that are inefficient to represent through classical computation. A fundamental question is whether these correlations could be harnessed to enhance learning performances on real datasets. Here, we report the use of a neutral atom quantum processor comprising up to 32 qubits to implement machine learning tasks on graph-structured data. To that end, we introduce a quantum feature map to encode the information about graphs in the parameters of a tunable Hamiltonian acting on an array of qubits. Using this tool, we first show that interactions in the quantum system can be used to distinguish non-isomorphic graphs that are locally equivalent. We then realize a toxicity screening experiment, consisting of a binary classification ‚Ä¶","date":1670676790,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670676790,"objectID":"9297d9e12cf9ee548cfc0a557c349bb8","permalink":"https://example.com/newsletter/november-2022/","publishdate":"2022-12-10T19:53:10+07:00","relpermalink":"/newsletter/november-2022/","section":"newsletter","summary":"News üì∞ D-Wave Says Revenue Up, Added More Commercial Customers in Q3\nIonQ and Hyundai Motors Expand Quantum Computing Partnership, Continuing Pursuit of Automotive Innovation\nQNLP at The Beeb ‚Äî Quantinuum, BBC Join Consortium to Explore Real-World Uses for Quantum Natural Language Processing","tags":[],"title":"November 2022","type":"newsletter"},{"authors":[],"categories":[],"content":" Cu·ªëi c√πng trong b·ªô 3 b√†i vi·∫øt v·ªÅ c√°c subroutines c·ªßa thu·∫≠t to√°n l∆∞·ª£ng t·ª≠, m√¨nh mu·ªën ƒë·ªÅ c·∫≠p t·ªõi thu·∫≠t to√°n Harrow-Hassidim-Lloyd (hay HHL) ƒë∆∞·ª£c ƒë·∫∑t theo t√™n c·ªßa ba t√°c gi·∫£ ƒë√£ gi·ªõi thi·ªáu thu·∫≠t to√°n 1. Thu·∫≠t to√°n HHL ƒë∆∞·ª£c ph√°t tri·ªÉn nh·∫±m gi·∫£i quy·∫øt ph∆∞∆°ng tr√¨nh ho·∫∑c h·ªá ph∆∞∆°ng tr√¨nh tuy·∫øn t√≠nh b·∫±ng m√°y t√≠nh l∆∞·ª£ng t·ª≠, hay n√≥i c√°ch kh√°c thu·∫≠t to√°n s·∫Ω gi·∫£i ph∆∞∆°ng tr√¨nh:\n$$ Ax = b $$ Theo ph∆∞∆°ng ph√°p truy·ªÅn th·ªëng, ph∆∞∆°ng tr√¨nh tr√™n c√≥ th·ªÉ ƒë∆∞·ª£c gi·∫£i b·∫±ng c√°ch ngh·ªãch ƒë·∫£o ma tr·∫≠n $A$ v√† c√≥ k·∫øt qu·∫£ l√† $x=A^{-1}b$. Tuy nhi√™n, vi·ªác t√≠nh to√°n ma tr·∫≠n ngh·ªãch ƒë·∫£o t·ªën r·∫•t nhi·ªÅu t√†i nguy√™n khi ma tr·∫≠n $A$ l·ªõn. V·ªõi m·ªôt c√°ch ph·ªë bi·∫øn nh·∫•t nh∆∞ l√† Gaussian emilination, ta c·∫ßn t·ªõi ƒë·ªô ph·ª©c t·∫°p $O(N^3)$ ƒë·ªÉ ngh·ªãch ƒë·∫£o m·ªôt ma tr·∫≠n $N\\times N$; hay ti√™n ti·∫øn h∆°n v·ªõi thu·∫≠t to√°n conjugate gradient c√≥ ƒë·ªô ph·ª©c t·∫°p $O(Nsk \\log{1/\\epsilon})$, ·ªü ƒë√≥ $s$ l√† t·ª∑ l·ªá ph·∫•n t·ª≠ c√≥ gi√° tr·ªã $0$ trong ma tr·∫≠n $A$ (sparsity proportion), $k$ l√† t·ª∑ s·ªë gi·ªØa gi√° tr·ªã ri√™ng l·ªõn nh·∫•t v√† gi√° tr·ªã ri√™ng nh·ªè nh·∫•t, v√† cu·ªëi c√πng l√† $\\epsilon$ k√Ω hi·ªáu cho ƒë·ªô sai s·ªë c·ªßa thu·∫≠t to√°n. Nh∆∞ng v·ªõi HHL, thu·∫≠t to√°n c√≥ th·ªÉ x·ª≠ l√Ω v·∫´n ƒë·ªÅ ma tr·∫≠n ngh·ªãch ƒë·∫£o v·ªõi $O(\\log{(N)}s^2k^2/\\epsilon)$.\n·ªû ƒë√¢y, m√¨nh s·∫Ω c√πng m·ªçi ng∆∞·ªùi ph√¢n t√≠ch r√µ h∆°n v·ªÅ thu·∫≠t to√°n n√†y.\nN·ªôi dung Thu·∫≠t to√°n HHL Source Code Thu·∫≠t to√°n HHL Thu·∫≠t to√°n HHL ƒë·∫∑c bi·ªát quan tr·ªçng cho c√°c b√†i to√°n c·ªßa QML, v√¨ nh∆∞ m·ªçi ng∆∞·ªùi ƒë√£ bi·∫øt h·∫ßu h·∫øt c√°c thu·∫≠t to√°n machine learning ƒë·ªÅu ƒëi h·ªçc tham s·ªë $\\theta$ theo ph∆∞∆°ng tr√¨nh $y = \\theta^{T}x$. Do ƒë√≥, ta ho√†n to√†n c√≥ th·ªÉ √°p d·ª•ng HHL trong vi·ªác t√¨m $\\theta$ trong c√°c b√†i to√°n c·ªßa machine learning.\nTh·ª±c ch·∫•t v·∫•n ƒë·ªÅ l·ªõn nh·∫•t c·ªßa HHL l√† l√†m sao ƒë∆∞a ma tr·∫≠n $A$ c·ªßa ph∆∞∆°ng tr√¨nh $Ax = b$ v√†o m√°y t√≠nh l∆∞·ª£ng t·ª≠. Nh∆∞ m√¨nh ƒë√£ ƒë·ªÅ c·∫≠p r·∫•t nhi·ªÅu t·ª´ c√°c b√†i vi·∫øt tr∆∞·ªõc, c√°c ph√©p bi·∫øn ƒë·ªïi trong m√°y t√≠nh l∆∞·ª£ng t·ª≠ ph·∫£i t·ª´ ma tr·∫≠n ƒë∆°n nh·∫•t (unitary matrix). Do ƒë√≥, ta kh√¥ng th·ªÉ tr·ª±c ti·∫øp d√πng $A$ ƒë·ªÉ bi·∫øn ƒë·ªïi, nh∆∞ng may thay m·ªçi ma tr·∫≠n ƒë∆°n nh·∫•t ƒë·ªÅu c√≥ th·ªÉ bi·ªÉu di·ªÖn theo c√¥ng th·ª©c t·ªïng qu√°t: $U = e^{iHt}$, trong ƒë√≥ $H$ l√† ma tr·∫≠n Hermitian, v√† $t$ bi·ªÉu th·ªã th·ªùi gian (m·ªçi ng∆∞·ªùi c√≥ th·ªÉ xem ch·ª©ng minh ·ªü Box 1).\nBox 1: Ma tr·∫≠n $H$ ƒë∆∞·ª£c g·ªçi l√† Hermitian n·∫øu $H = H^{\\dagger}$ v√† $U$ l√† m·ªôt ma tr·∫≠n ƒë∆°n nh·∫•t khi $UU^{\\dagger} = U^{\\dagger}U = I$.\nDo ƒë√≥, ta c√≥: $UU^{\\dagger} = e^{iHt}e^{-iH^{\\dagger}t} = e^{iHt-iHt} = I$. Nh∆∞ v·∫≠y n·∫øu $H$ l√† m·ªôt ma tr·∫≠n Hermitian th√¨ $U = e^{iHt}$ l√† m·ªôt ma tr·∫≠n ƒë∆°n nh·∫•t.\nM·ªçi ng∆∞·ªùi c√≥ th·ªÉ ƒë·ªçc chi ti·∫øt h∆°n l√Ω do h√¨nh th√†nh c·ªßa c√¥ng th·ª©c tr√™n ·ªü Section 2.2.2 Nielson \u0026amp; Chuang.\nNh∆∞ v·∫≠y, ta ho√†n to√†n c√≥ th·ªÉ s·ª≠ d·ª•ng m·ªôt k·ªπ thu·∫≠t nh·ªè ƒë·ªÉ bi·∫øn $A$ th√†nh m·ªôt ma tr·∫≠n Hermitian, $\\tilde{A}$, ƒë·ªÉ c√≥ m·ªôt ma tr·∫≠n ƒë∆°n nh·∫•t $U = e^{i\\tilde{A}t}$. ·ªû ƒë√¢y, m√¨nh c√≥ th·ªÉ bi·ªÉu di·ªÖn $\\tilde{A}$ d∆∞·ªõi d·∫°ng:\n$$ \\tilde{A} = \\left( \\begin{array}{cc} 0 \u0026amp; A^{\\dagger} \\\\ A \u0026amp; 0 \\end{array} \\right) $$ D·ªÖ d√†ng c√≥ th·ªÉ th·∫•y $\\tilde{A} =\\tilde{A}^{\\dagger}$ n√™n $\\tilde{A}$ l√† m·ªôt ma tr·∫≠n Hermitian. Nh∆∞ v·∫≠y thay v√¨ tr·ª±c ti·∫øp gi·∫£i $x = A^{-1}b$, m√¨nh s·∫Ω ƒëi gi·∫£i quy·∫øt b√†i to√°n $\\ket{x} = \\tilde{A}^{-1}\\ket{b}$\nM·∫∑t kh√°c, n·∫øu ta ph√¢n r√£ tr·ªã ri√™ng c·ªßa $\\tilde{A}$ ta ƒë∆∞·ª£c $V\\Lambda V^{-1}$ trong ƒë√≥ $V$ g·ªìm c√°c v√©c-t∆° c·ªôt l√† v√©c-t∆° ri√™ng c·ªßa $\\tilde{A}$ v√† $\\Lambda$ c√≥ c√°c ph·∫ßn t·ª≠ ƒë∆∞·ªùng ch√©o l√† c√°c gi√° tr·ªã ri√™ng c·ªßa $\\tilde{A}$. M√† $\\tilde{A} = \\tilde{A}^{\\dagger}$ n√™n d·ªÖ d√†ng c√≥ th·ªÉ th·∫•y $VV^{\\dagger} = I$ n√™n $V^{-1} = V^{\\dagger}$. Do ƒë√≥, ta c√≥ th·ªÉ vi·∫øt c√¥ng th·ª©c ph√¢n r√£ tr·ªã ri√™ng c·ªßa $\\tilde{A}$ nh∆∞ sau: $$ \\tilde{A} = \\sum_{i} \\lambda_i \\ket{v_i}\\bra{v_i} $$ $$ \\Rightarrow \\tilde{A}^{-1} = \\sum_{i} \\frac{1}{\\lambda_i} \\ket{v_i}\\bra{v_i} \\quad (1) $$ ·ªû ƒë√≥, c√°c v√©c-t∆° ri√™ng $\\ket{v_i}$ t·∫°o th√†nh h·ªá c∆° s·ªü tr·ª±c giao (orthonormal basis), hay $\\left\\langle v_i| v_i \\right\\rangle = 1$ v√† $\\left\\langle v_i| v_j \\right\\rangle = 0 $. N√™n ta c≈©ng c√≥ th·ªÉ bi·ªÉu di·ªÖn ƒë∆∞·ª£c $\\ket{b}$ theo $\\ket{v_i}$: $$ \\ket{b} = \\sum_{i} \\beta_i \\ket{v_i} \\quad (2) $$ Thay (1) v√† (2) v√†o $\\ket{x} = \\tilde{A}^{-1}\\ket{b}$, ta ƒë∆∞·ª£c:\n$$ \\ket{x} = \\sum_{i} \\frac{1}{\\lambda_i} \\ket{v_i}\\bra{v_i} \\sum_{i} \\beta_i \\ket{v_i} $$ $$ = \\sum_{i} \\frac{\\beta_i}{\\lambda_i} \\ket{v_i} \\quad (3) $$ T·ª´ k·∫øt qu·∫£ tr√™n, c√≥ th·ªÉ th·∫•y nhi·ªám v·ª• c·ªßa thu·∫≠t to√°n HHL s·∫Ω bi·∫øn ƒë·ªïi ƒë·∫øn tr·∫°ng th√°i $\\sum_{i} \\frac{\\beta_i}{\\lambda_i} \\ket{v_i}$. Sau ƒë√¢y, m√¨nh s·∫Ω ph√¢n t√≠ch t·ª´ng b∆∞·ªõc m·ªôt trong nh·ªØng b∆∞·ªõc tri·ªÉn khai c·ªßa HHL.\nH√¨nh 1: C·∫•u tr√∫c m·∫°ch c·ªßa thu·∫≠t to√°n HHL B∆∞·ªõc 1: Kh·ªüi t·∫°o Nh∆∞ minh h·ªça ·ªü H√¨nh 1, thu·∫≠t to√°n HHL c√≥ ƒë·∫ßu v√†o g·ªëm c√≥ 3 thanh ghi:\nThanh ghi Ancilla g·ªìm 1 qubit ƒë∆∞·ª£c kh·ªüi t·∫°o b·∫±ng $\\ket{0}_{ANC}$. Qubit n√†y s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng ƒë·ªÉ h·ªó tr·ª£ cho ph√©p quay (rotation) t·ª´ $\\lambda_i\\ket{v_i}$ th√†nh $\\frac{1}{\\lambda_i}\\ket{v_i}$.\nThanh ghi th·ª© hai s·∫Ω ch·ªãu tr√°ch nhi·ªám l∆∞u gi·ªØ th√¥ng tin v·ªÅ gi√° tr·ªã ri√™ng t·ª´ k·∫øt qu·∫£ c·ªßa thu·∫≠t to√°n Phase estimation - QPE. V√† t∆∞∆°ng t·ª± v·ªõi B√†i 4, s·ªë qubits ƒë∆∞·ª£c d√πng ·ªü thanh ghi n√†y s·∫Ω ph·ª• thu·ªôc v√†o sai s·ªë c·ªßa thu·∫≠t to√°n QPE. C√°c qubits ·ªü ƒë√¢y c≈©ng ƒë∆∞·ª£c kh·ªüi t·∫°o v·ªõi gi√° tr·ªã $\\ket{0}_{W}$.\nThanh ghi cu·ªëi c√πng m√£ h√≥a gi√° tr·ªã c·ªßa $\\ket{b}$. M·ªçi bi·∫øn ƒë·ªïi sau n√†y m√¨nh s·∫Ω gi·∫£ s·ª≠ $\\ket{b}$ ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a ƒë·ªÉ ƒë∆°n gi·∫£n trong qu√° tr√¨nh tr√¨nh b√†y c√°c c√¥ng th·ª©c. Trong ‚Ä¶","date":1670555740,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670555740,"objectID":"48744332ee7a2ee512d09a68251ae4f9","permalink":"https://example.com/post/important-subroutine-3/","publishdate":"2022-12-09T10:15:40+07:00","relpermalink":"/post/important-subroutine-3/","section":"post","summary":"Cu·ªëi c√πng trong b·ªô 3 b√†i vi·∫øt v·ªÅ c√°c subroutines c·ªßa thu·∫≠t to√°n l∆∞·ª£ng t·ª≠, m√¨nh mu·ªën ƒë·ªÅ c·∫≠p t·ªõi thu·∫≠t to√°n Harrow-Hassidim-Lloyd (hay HHL) ƒë∆∞·ª£c ƒë·∫∑t theo t√™n c·ªßa ba t√°c gi·∫£ ƒë√£ gi·ªõi thi·ªáu thu·∫≠t to√°n 1.","tags":[],"title":"B√†i 5: Important Subroutine 3 - HHL Algorithm","type":"post"},{"authors":[],"categories":[],"content":" N·∫±m th·ª© hai trong b·ªô ba b√†i vi·∫øt v·ªÅ c√°c *subroutines* quan tr·ªçng trong khoa h·ªçc l∆∞·ª£ng t·ª≠ n√≥i chung v√† QML n√≥i ri√™ng, m√¨nh mu·ªën gi·ªõi thi·ªáu m·ªçi ng∆∞·ªùi v·ªÅ thu·∫≠t to√°n ***Quantum Phase Estimation*** (QPE). Ch√≠nh s·ª± xu·∫•t hi·ªán c·ªßa thu·∫≠t to√°n n√†y ƒë√£ t·∫°o ti·ªÅn ƒë·ªÅ cho nh∆∞ng ·ª©ng d·ª•ng c·ªßa m√°y t√≠nh l∆∞·ª£ng t·ª≠ cho c√°c b√†i to√°n ph·ª©c t·∫°p d∆∞·ªùng nh∆∞ kh√¥ng th·ªÉ x·ª≠ l√Ω tr√™n m√°y t√≠nh truy·ªÅn th·ªëng nh∆∞: *period finding* hay *factoring numbers*. Do ƒë√≥, m√¨nh s·∫Ω c√πng m·ªçi ng∆∞·ªùi ph√¢n t√≠ch r√µ h∆°n v·ªÅ thu·∫≠t to√°n n√†y. N·ªôi dung Quantum Phase Estimation ƒê·ªô ph·ª©c t·∫°p c·ªßa QPE Source Code Quantum Phase Estimation Cho m·ªôt ma tr·∫≠n ƒë∆°n nh·∫•t (unitary matrix), $\\mathcal{U}$, ta c·∫ßn ph·∫£i t√¨m gi√° tr·ªã ri√™ng t∆∞∆°ng ·ª©ng v·ªõi v√©c-t∆° ri√™ng $\\ket{\\psi}$ c·ªßa ma tr·∫≠n $\\mathcal{U}$. V√¨ $\\mathcal{U}$ l√† m·ªôt ma tr·∫≠n ƒë∆°n nh·∫•t n√™n kh√° d·ªÖ d√†ng cho m√¨nh ch·ª©ng minh ƒë∆∞·ª£c c√°c gi√° tr·ªã ri√™ng $\\lambda$ c·ªßa $\\mathcal{U}$ c√≥ ƒë·ªô l·ªõn l√† $1$ (m·ªçi ng∆∞·ªùi c√≥ th·ªÉ xem ch·ª©ng minh ·ªü Box 1). Do ƒë√≥, $\\lambda$ ho√†n to√†n c√≥ th·ªÉ vi·∫øt d∆∞·ªõi d·∫°ng $e^{2\\pi i \\phi}$, $0\\leq \\phi \u0026lt; 1$, v√¨ $\\lambda^{*} \\lambda = e^{-2\\pi i \\phi + 2\\pi i \\phi} = e^0 = 1$. Nh∆∞ v·∫≠y, thu·∫≠t to√°n Quantum Phase Estimation (hay QPE) th·ª±c ch·∫•t s·∫Ω ƒëi t√≠nh $\\phi$ cho gi√° tr·ªã ri√™ng $\\lambda$.\nBox 1: Ta c√≥ $ \\mathcal{U} \\ket{\\psi}=\\lambda \\ket{\\psi}$ v√† $\\mathcal{U}^{\\dagger}\\mathcal{U} = I$ (v√¨ $\\mathcal{U}$ l√† ma tr·∫≠n ƒë∆°n nh·∫•t), n√™n: $\\left\\langle \\psi| \\psi \\right\\rangle = \\left\\langle \\psi| \\mathcal{U}^{\\dagger} \\mathcal{U} |\\psi \\right\\rangle = \\bra{\\psi} \\lambda^{*} \\lambda \\ket{\\psi} = |\\lambda|^2\\left\\langle \\psi| \\psi \\right\\rangle $. Do ƒë√≥, $|\\lambda|^2 = 1$ H√¨nh 1: C·∫•u tr√∫c m·∫°ch c·ªßa thu·∫≠t to√°n QPE C·∫•u tr√∫c m·∫°ch c·ªßa thu·∫≠t to√°n QPE ƒë∆∞·ª£c minh h·ªça ·ªü H√¨nh 1. Thu·∫≠t to√°n s·∫Ω c√≥ 2 th√†nh ph·∫ßn ch√≠nh: v·ªõi thanh ghi ƒë·∫ßu ti√™n, g·ªìm $n$ qubits ƒë∆∞·ª£c kh·ªüi t·∫°o ban ƒë·∫ßu b·∫±ng $\\ket{0}$, ch·ªãu tr√°ch nhi·ªám gi·ªØ nh∆∞ng th√¥ng tin v·ªÅ $\\phi$ sau khi thu·∫≠t to√°n k·∫øt th√∫c; c√≤n thanh ghi th·ª© 2 m√£ h√≥a gi√° tr·ªã c·ªßa v√©c-t∆° ri√™ng $\\ket{\\psi}$. V√¨ gi√° tr·ªã c·ªßa $\\phi$ b·∫•t ƒë·ªãnh trong kho·∫£ng $[0,1)$ n√™n c√≥ th·ªÉ th·∫•y r·∫±ng gi√° tr·ªã c·ªßa $n$ d√πng ƒë·ªÉ gi·∫£i m√£ $\\phi$ s·∫Ω linh ho·∫°t t√πy thu·ªôc ƒë·ªô ch√≠nh x√°c m√† ch√∫ng ta mong mu·ªën (m·ªçi ng∆∞·ªùi c√≥ th·ªÉ xem gi·∫£i th√≠ch ·ªü Box 2).\nBox 2: Ta c√≥ $\\phi \\in [0,1)$, t∆∞∆°ng t·ª± v·ªõi thu·∫≠t to√°n QFT ·ªü B√†i 3, m√¨nh c√≥ th·ªÉ bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng nh·ªã ph√¢n nh∆∞ sau:\n$$ \\phi = 0.\\phi_1\\phi_2‚Ä¶\\phi_k\\phi_{k+1}‚Ä¶ $$\nV√¨ $\\phi$ ch∆∞a x√°c ƒë·ªãnh n√™n gi·∫£ s·ª≠ ta d√πng $n$ qubit ƒë·ªÉ gi·∫£i m√£ $\\phi$ th√¨ lu√¥n t·ªìn t·∫°i gi√° tr·ªã $\\epsilon \\geq 0$ sao cho\n$$ \\phi = 0.\\phi_1\\phi_2‚Ä¶\\phi_n + \\epsilon $$\nTrong ƒë√≥ $\\epsilon = 0$ khi v√† ch·ªâ khi $\\phi$ c√≥ th·ªÉ bi·ªÉu di·ªÖn ch√≠nh x√°c v·ªõi $n$ qubits. Do ƒë√≥ ƒë·ªô ch√≠nh x√°c c·ªßa thu·∫≠t to√°n QPE s·∫Ω ph√π thu·ªôc v√†o m·ªçi ng∆∞·ªùi tinh ch·ªânh gi√° tr·ªã $n$.\nNh∆∞ m·ªçi ng∆∞·ªùi c√≥ th·ªÉ th·∫•y ·ªü H√¨nh 1, thu·∫≠t to√°n QPE g·ªìm c√≥ 3 b∆∞·ªõc:\nB∆∞·ªõc 1: S·ª≠ d·ª•ng c·ªïng Hadamard l√™n c√°c qubits ·ªü thanh ghi ƒë·∫ßu ti√™n. V√¨ c√°c qubits ƒë∆∞·ª£c kh·ªüi t·∫°o b·∫±ng v√©c-t∆° $\\ket{0}$ n√™n: $$ \\ket{q^1} = H^{\\otimes n} \\ket{0}^{\\otimes n} = \\frac{1}{2^{n/2}} (\\ket{0}+\\ket{1})(\\ket{0}+\\ket{1})...(\\ket{0}+\\ket{1}) $$ $$ = \\frac{1}{2^{n/2}} \\sum_{k=0}^{2^n-1}\\ket{k} $$ B∆∞·ªõc 2: L·∫ßn l∆∞·ª£t bi·∫øn ƒë·ªïi qubits ·ªü thanh ghi th·ª© hai v·ªõi ma tr·∫≠n $\\mathcal{U}^{2^{n-m}}$; ·ªü ƒë√≥, ph√©p bi·∫øn ƒë·ªïi n√†y s·∫Ω ph·ª• thu·ªôc v√†o qubit th·ª© $m$ c·ªßa thanh ghi ƒë·∫ßu ti√™n, $\\ket{q^1_m}$. N·∫øu $\\ket{q^1_m} = \\ket{0}$, ph√©p bi·∫øn ƒë·ªïi s·∫Ω gi·ªØ nguy√™n gi√° tr·ªã c·ªßa $\\ket{\\psi}$ ·ªü thanh ghi th·ª© hai, v√† n√≥ ch·ªâ th·ª±c hi·ªán bi·∫øn ƒë·ªïi v·ªõi ma tr·∫≠n $\\mathcal{U}^{n-m}$ khi $\\ket{q^1_m} = \\ket{1}$. Do ƒë√≥, m√¨nh c√≥ th·ªÉ k√Ω hi·ªáu ph√©p ƒë·ªïi n√†y l√† controlled- $\\mathcal{U}^{2^{n-m}}$ (hay $C-U^{2^{n-m}}$ nh∆∞ H√¨nh 1). Nh∆∞ v·∫≠y, c·ªïng $C-U^{2^{n-m}}$ s·∫Ω bi·∫øn ƒë·ªïi b√†i to√°n c·ªßa ch√∫ng ta nh∆∞ sau: $$ (C-U^{2^{n-m}}) \\ket{\\psi} \\frac{1}{\\sqrt{2}} (\\ket{0}+\\ket{1}) = \\frac{1}{\\sqrt{2}} (\\ket{\\psi}\\ket{0}+\\mathcal{U}^{2^{n-m}}\\ket{\\psi}\\ket{1}) $$ $$ = \\frac{1}{\\sqrt{2}} (\\ket{\\psi}\\ket{0}+\\lambda^{2^{n-m}}\\ket{\\psi}\\ket{1}) = \\frac{1}{\\sqrt{2}} (\\ket{\\psi}\\ket{0}+e^{2\\pi i \\phi 2^{n-m}}\\ket{\\psi}\\ket{1}) $$ $$ = \\ket{\\psi} \\otimes \\frac{1}{\\sqrt{2}} (\\ket{0}+e^{2\\pi i \\phi 2^{n-m}}\\ket{1}) $$ C√≥ th·ªÉ th·∫•y, ph√©p bi·∫øn ƒë·ªïi controlled- $\\mathcal{U}^{2^{n-m}}$ th·ª±c ch·∫•t kh√¥ng thay ƒë·ªïi tr·∫°ng th√°i c·ªßa thanh ghi th·ª© hai, $\\ket{\\psi}$, m√† n√≥ s·∫Ω thay ƒë·ªïi tr·∫°ng th√°i c·ªßa qubit $\\ket{q^1_m}$. T·ª´ ƒë√¢y b√†i to√°n ƒë∆∞a ta v·ªÅ v·∫•n ƒë·ªÅ gi·ªëng v·ªõi thu·∫≠t to√°n QFT ·ªü B√†i 3.\nGi·∫£ s·ª≠, $\\phi$ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng nh·ªã ph√¢n: $$ \\phi = 0.\\phi_1\\phi_2...\\phi_n=\\phi_1 2^{-1}+\\phi_2 2^{-2}+...+\\phi_n 2^{-n} $$ V·ªõi m = 1, ta c√≥: $$ \\phi 2^{n-1} =\\phi_1 2^{n-2}+\\phi_2 2^{n-3}+...+\\phi_n 2^{-1} $$ $$ \\Rightarrow e^{2\\pi i \\phi 2^{n-1}} =e^{2\\pi i \\phi_1 2^{n-2}}e^{2\\pi i \\phi_2 2^{n-3}}...e^{2\\pi i\\phi_n 2^{-1}} $$ M·∫∑t kh√°c, m√¨nh c√≥ th·ªÉ th·∫•y $e^{2\\pi i} = 1$ n√™n $e^{2\\pi ik} = 1 \\forall k \\in \\mathbb{Z}$. T·ª´ ƒë√≥ c√≥ th·ªÉ d·ªÖ d√†ng suy ra t·∫•t c·∫£ c√°c ph·∫ßn t·ª≠ trong c√¥ng th·ª©c tr√™n ƒë·ªÅu b·∫±ng 1 tr·ª´ $e^{2\\pi i \\phi_n 2^{-1}}$, n√™n $$ e^{2\\pi i \\phi 2^{n-1}} = 1.1.1...e^{2\\pi i \\phi_n 2^{-1}} = e^{2\\pi i 0.\\phi_n} $$ T∆∞∆°ng t·ª± ta c≈©ng c√≥ th·ªÉ suy ra: $$ e^{2\\pi i \\phi 2^{n-2}} = e^{2\\pi i 0.\\phi_{n-1}\\phi_n} $$ $$ ... $$ $$ e^{2\\pi i \\phi 2^{0}} = ‚Ä¶","date":1670468805,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1670468805,"objectID":"8f9aa04c83051c3138aba4dca40b3435","permalink":"https://example.com/post/important-subroutine-2/","publishdate":"2022-12-08T10:06:45+07:00","relpermalink":"/post/important-subroutine-2/","section":"post","summary":"N·∫±m th·ª© hai trong b·ªô ba b√†i vi·∫øt v·ªÅ c√°c *subroutines* quan tr·ªçng trong khoa h·ªçc l∆∞·ª£ng t·ª≠ n√≥i chung v√† QML n√≥i ri√™ng, m√¨nh mu·ªën gi·ªõi thi·ªáu m·ªçi ng∆∞·ªùi v·ªÅ thu·∫≠t to√°n ***Quantum Phase Estimation*** (QPE).","tags":[],"title":"B√†i 4: Important Subroutine 2 - Quantum Phase Estimation","type":"post"},{"authors":[],"categories":[],"content":" ·ªû ph·∫ßn n√†y m√¨nh s·∫Ω c√≥ 3 b√†i vi·∫øt n√≥i v·ªÅ 3 subroutines r·∫•t quan tr·ªçng t·∫°o n√™n kh·∫£ nƒÉng t√≠nh to√°n v∆∞·ª£t tr·ªôi c·ªßa thu·∫≠t to√°n l∆∞·ª£ng t·ª≠: Quantum Fourier Transform, Quantum Phase Estimation, and HHL algorithm. Nh·ªØng thu·∫≠t to√°n tr√™n c√≥ m·∫∑t nhi·ªÅu trong c√°c b√†i to√°n c·ªßa QML. Do ƒë√≥ m√¨nh tin r·∫±ng vi·ªác n·∫Øm r√µ nh·ªØng thu·∫≠t to√°n n√†y ƒë·∫ßu ti√™n s·∫Ω gi√∫p c√°c b·∫°n trong vi·ªác t√¨m hi·ªÅu c√°c thu·∫≠t to√°n c·ªßa QML.\nƒê·ªÉ kh·ªüi ƒë·∫ßu trong danh s√°ch tr√™n m√¨nh s·∫Ω tr√¨nh b√†y thu·∫≠t to√°n Quantum Fourier Transform (QFT).\nN·ªôi dung Discrete Fourier Transform Quantum Fourier Transform ƒê·ªô ph·ª©c t·∫°p c·ªßa QFT Source Code Discrete Fourier Transform Fourier Transform ƒë√£ v√† ƒëang xu·∫•t hi·ªán trong r·∫•t nhi·ªÅu ·ª©ng d·ª•ng ·ªü m√°y t√≠nh truy·ªÅn th·ªëng, c√≥ th·ªÉ k·ªÉ ƒë·∫øn nh∆∞: signal processing hay data compression. Fourier Transform s·∫Ω bi·∫øn ƒë·ªïi m·ªôt h√†m s·ªë ho·∫∑c m·ªôt v√©c-t∆° theo mi·ªÅn th·ªùi gian ho·∫∑c kh√¥ng gian sang mi·ªÅn t·∫ßn s·ªë. V√≠ d·ª• Discrete Fourier Transform (DFT) c·ªßa m·ªôt bi·∫øn r·ªùi r·∫°c $\\mathcal{X_N} = \\{x_0, x_1, ‚Ä¶, x_{N-1}\\} \\in \\mathbb{C}^N$ c√≥ d·∫°ng:\n$$ y_k = \\frac{1}{\\sqrt{N}} \\sum_{j=0}^{N-1} x_j e^{\\frac{2\\pi ijk}{N}}, $$ T·ª´ ƒë√≥ ph√©p bi·∫øn ƒë·ªïi s·∫Ω cho ra m·ªôt chu·ªói m·ªõi $\\mathcal{Y_N} = \\{y_0, y_1, ‚Ä¶, y_{N-1}\\} \\in \\mathbb{C}^N$ c√πng chi·ªÅu v·ªõi chu·ªói ƒë·∫ßu v√†o.\nM·∫∑t kh√°c, m√¨nh mu·ªën ƒë·ªÅ c·∫≠p t·ªõi h√†m Kronecker delta, $\\delta_{Nj}$, ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a l√† m·ªôt v√©c-t∆° $N$ chi·ªÅu, c√≥ gi√° tr·ªã 1 ·ªü v·ªã tr√≠ th·ª© $j$ v√† 0 ·ªü c√°c v·ªã tr√≠ c√≤n l·∫°i. Do ƒë√≥, m√¨nh ho√†n to√†n c√≥ th·ªÉ coi h√†m Kronecker delta l√† c√°c v√©c-t∆° c∆° s·ªü tr·ª±c chu·∫©n cho bi·∫øn $\\mathcal{X_N}$:\n$$ \\mathcal{X_N} = \\sum_{j=0}^{N} x_j \\delta_{Nj} $$ Kh√¥ng m·∫•t t√≠nh t·ªïng qu√°t, m√¨nh v·∫´n ho√†n to√†n c√≥ th·ªÉ l·∫•y h√†m Kronecker delta nh∆∞ c√°c v√©c-t∆° c∆° s·ªü tr·ª±c chu·∫©n $\\ket{j}$ s·∫Ω ƒë∆∞·ª£c s·ª≠ d·ª•ng cho thu·∫≠t to√°n l∆∞·ª£ng t·ª≠ c·ªßa ch√∫ng ta. N√≥i c√°ch kh√°c, bi·∫øn $\\mathcal{X_N}$ s·∫Ω ƒë∆∞·ª£c m√£ h√≥a trong thu·∫≠t to√°n l∆∞·ª£ng t·ª≠ c√≥ d·∫°ng 1: $$ \\ket{\\mathcal{X_N}} = \\sum_{j=0}^{N} x_j \\ket{j} $$ Kh√°c ph√©p bi·∫øn ƒë·ªïi Fourier tr√™n m√°y t√≠nh truy·ªÅn th·ªëng, thu·∫≠t to√°n Quantum Fourier Transform s·∫Ω bi·∫øn ƒë·ªïi c√°c v√©c-t∆° c∆° s·ªü tr·ª±c chu·∫©n $\\ket{j}$:\n$$ \\ket{j} \\longrightarrow \\frac{1}{\\sqrt{N}} \\sum_{k=0}^{N-1} e^{\\frac{2\\pi ijk}{N}} \\ket{k} $$ V√† n√≥ t∆∞∆°ng ƒë∆∞∆°ng v·ªõi vi·ªác:\n$$ \\sum_{j=0}^{N-1}x_j \\ket{j} \\longrightarrow \\sum_{k=0}^{N-1} y_k \\ket{k} $$ $$ \\ket{\\mathcal{X_N}} \\longrightarrow \\ket{\\mathcal{Y_N}} $$ V·∫≠y chi ti·∫øt ƒë·∫±ng sau ph√©p bi·∫øn ƒë·ªïi n√†y nh∆∞ th·∫ø n√†o, m√¨nh s·∫Ω ƒëi t·ªõi ph·∫ßn ti·∫øp theo.\nQuantum Fourier Transform V·ªõi bi·∫øn $\\mathcal{X_N}$ g·ªìm $N$ ph·∫ßn t·ª≠, m√¨nh c·∫ßn $n = \\log_2{N}$ qubits ƒë·ªÉ bi·ªÉu di·ªÖn to√†n b·ªô c√°c v√©c-t∆° $\\ket{j}$. ·ªû ƒë√¢y, $j$ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng nh·ªã ph√¢n $j_1j_2‚Ä¶j_n$ sao cho:\n$$ j = j_1 2^{n-1} + j_2 2^{n-2} + ...+ j_n 2^0 $$ V√† ƒë·ªÉ thu·∫≠n ti·ªán h∆°n trong vi·ªác ph√°t tri·ªÉn c√¥ng th·ª©c d∆∞·ªõi ƒë√¢y, m√¨nh s·∫Ω k√Ω hi·ªáu $0.j_lj_{l+1}‚Ä¶j_m = j_l/2 + j_{l+1}/4+‚Ä¶+j_m/2^{m-l+1}$.\nH√¨nh 1: C·∫•u tr√∫c m·∫°ch c·ªßa thu·∫≠t to√°n QFT H√£y b·∫Øt ƒë·∫ßu v·ªõi qubit ƒë·∫ßu ti√™n $\\ket{j_1}$, c·ªïng Hadamard (H gate) bi·∫øn ƒë·ªïi tr·∫°ng th√°i c·ªßa $\\ket{j_1}$ th√†nh $\\frac{1}{\\sqrt{2}}(\\ket{0}+(-1)^{j_1}\\ket{1})$. M√¨nh thay $(-1) = e^{\\pi i}$, ta ƒë∆∞·ª£c: $$ H\\ket{j_1} = \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i (\\frac{j_1}{2})}\\ket{1}) = \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_1} \\ket{1}) $$ Qubit n√†y ti·∫øp ƒë·∫øn ƒë∆∞·ª£c bi·∫øn ƒë·ªïi theo ${\\mathbf{R}}_m$ b·ªã r√†ng bu·ªôc b·ªüi gi√° tr·ªã c·ªßa $j_m$: $$ {\\mathbf{R}}_m = \\left( \\begin{array}{cc} 1 \u0026amp; 0 \\\\ 0 \u0026amp; e^{\\frac{2\\pi i j_m}{2^m}} \\end{array} \\right) $$ Nh∆∞ v·∫≠y n·∫øu th·ª±c hi·ªán ph√©p bi·∫øn ƒë·ªïi ${\\mathbf{R}}_2$ tr√™n qubit th·ª© nh·∫•t ta ƒë∆∞·ª£c: $$ \\left( \\begin{array}{cc} 1 \u0026amp; 0 \\\\ 0 \u0026amp; e^{\\frac{2\\pi i j_2}{2^2}} \\end{array} \\right) \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_1} \\ket{1}) = \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i (0.j_1+j_2/2^2)} \\ket{1}) $$ $$ = \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_1j_2} \\ket{1}) $$ T∆∞∆°ng t·ª± nh∆∞ v·∫≠y ta ti·∫øp t·ª•c √°p d·ª•ng ph√©p bi·∫øn ƒë·ªïi ${\\mathbf{R}}_3$, ${\\mathbf{R}}_4$,‚Ä¶ ${\\mathbf{R}}_n$ m√¨nh c√≥ tr·∫°ng th√°i c·ªßa qubit th·ª© nh·∫•t ƒë∆∞·ª£c bi·ªÉu di·ªÖn nh∆∞ sau: $$ \\ket{j_1} \\longrightarrow \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_1j_2...j_n} \\ket{1}) $$ L·∫ßn l∆∞·ª£t nh∆∞ v·∫≠y m√¨nh d√πng c√°c ph√©p bi·∫øn ƒë·ªïi t∆∞∆°ng t·ª± cho c√°c qubit ti·∫øp theo, thu ƒë∆∞·ª£c: $$ \\ket{j_2} \\longrightarrow \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_2...j_n} \\ket{1}) $$ $$ \\ket{j_3} \\longrightarrow \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_3...j_n} \\ket{1}) $$ $$ ... $$ $$ \\ket{j_n} \\longrightarrow \\frac{1}{\\sqrt{2}}(\\ket{0}+e^{2\\pi i 0.j_n} \\ket{1}) $$ Nh∆∞ v·∫≠y n·∫øu m√¨nh t·ªïng h·ª£p c√°c k·∫øt qu·∫£ t·ª´ $n$ qubits, b√†i to√°n c·ªßa m√¨nh s·∫Ω ƒë∆∞·ª£c bi·ªÉu di·ªÖn nh∆∞ sau: $$ \\ket{j_1j_2...j_n} \\! \\longrightarrow \\! $$ $$ \\frac{1}{2^\\frac{n}{2}}(\\ket{0}+e^{2\\pi i 0.j_1j_2...j_n} \\ket{1})(\\ket{0}+e^{2\\pi i 0.j_2...j_n} \\ket{1})...(\\ket{0}+e^{2\\pi i 0.j_n} \\ket{1}) $$ V√† b∆∞·ªõc cu·ªëi c√πng m√¨nh s·∫Ω swap 2 tr·∫°ng th√°i c·ªßa qubit th·ª© $m$ v·ªõi qubit th·ª© $n-m$, hay n√≥i c√°ch kh√°c m√¨nh ƒë·∫£o ng∆∞·ª£c th·ª© t·ª± tr·∫°ng th√°i c·ªßa c√°c qubit: $$ \\frac{1}{2^\\frac{n}{2}}(\\ket{0}\\!+\\!e^{2\\pi i 0.j_n} \\ket{1})(\\ket{0}\\!+\\!e^{2\\pi i 0.j_{n-1}j_n} \\ket{1})...(\\ket{0}\\!+\\!e^{2\\pi i 0.j_1j_2...j_n} \\ket{1}) \\quad (*) $$ ƒê·∫øn ƒë√¢y, m·ªçi ng∆∞·ªùi c√≥ th·ªÉ v·∫´n c√≤n th·∫Øc m·∫Øc v·ªÅ k·∫øt qu·∫£ tr√™n n√™n m√¨nh s·∫Ω ƒëi s√¢u h∆°n ƒë·ªÉ gi·∫£i ‚Ä¶","date":1669532765,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1669532765,"objectID":"58dc9016fa2d0a0763779e0481b5afbe","permalink":"https://example.com/post/important-subroutine-1/","publishdate":"2022-11-27T14:06:05+07:00","relpermalink":"/post/important-subroutine-1/","section":"post","summary":"·ªû ph·∫ßn n√†y m√¨nh s·∫Ω c√≥ 3 b√†i vi·∫øt n√≥i v·ªÅ 3 subroutines r·∫•t quan tr·ªçng t·∫°o n√™n kh·∫£ nƒÉng t√≠nh to√°n v∆∞·ª£t tr·ªôi c·ªßa thu·∫≠t to√°n l∆∞·ª£ng t·ª≠: Quantum Fourier Transform, Quantum Phase Estimation, and HHL algorithm.","tags":[],"title":"B√†i 3: Important Subroutine 1 - Quantum Fourier Transform","type":"post"},{"authors":[],"categories":[],"content":"Ch√†o b·∫°n,\nC·∫£m ∆°n b·∫°n ƒë√£ gh√© qua blog c·ªßa m√¨nh.\nM√¨nh mong mu·ªën nh·ªØng b√†i vi·∫øt c·ªßa m√¨nh c√≥ th·ªÉ ƒë·∫øn g·∫ßn h∆°n t·ªõi m·ªçi ng∆∞·ªùi. Khi m√¨nh vi·∫øt, m√¨nh lu√¥n c·ªë g·∫Øng gi·∫£i th√≠ch m·ªôt c√°ch d·ªÖ hi·ªÉu nh·∫•t t·ª´ c√°c b∆∞·ªõc bi·∫øn ƒë·ªïi to√°n h·ªçc hay c√°c d√≤ng code c·ªßa thu·∫≠t to√°n ƒë√≥. M√¨nh mong mu·ªën m·ªçi ng∆∞·ªùi kh√¥ng ch·ªâ hi·ªÉu l√Ω thuy·∫øt m√† c√≤n c√≥ th·ªÉ √°p d·ª•ng nh·ªØng ki·∫øn th·ª©c ƒë√≥ v√†o trong nh·ªØng lƒ©nh v·ª±c c·ªßa m·ªçi ng∆∞·ªùi.\nV√† ƒë√∫ng v·ªõi t√≠nh ch·∫•t ‚ÄúPrepare for the Future‚Äù, m√¨nh mong m·ªçi ng∆∞·ªùi c√≥ th·ªÉ ·ªßng h·ªô v√† gi√∫p nh·ªØng b√†i vi·∫øt c·ªßa m√¨nh d·∫ßn d·∫ßn ho√†n thi·ªán h∆°n, t·ª´ ƒë√≥ c√≥ th·ªÉ ti·∫øp c·∫≠n v·ªõi nhi·ªÅu b·∫°n ƒë·ªçc h∆°n n·ªØa.\nN·ªôi dung tr√™n blog n√†y l√† ho√†n to√†n mi·ªÖn ph√≠. M√¨nh s·∫Ω kh√¥ng s·ª≠ d·ª•ng d·ªãch v·ª• qu·∫£ng c√°o n√†o v√¨ kh√¥ng mu·ªën g√¢y m·∫•t t·∫≠p trung c√°c b·∫°n trong qu√° tr√¨nh ƒë·ªçc. Tuy nhi√™n, n·∫øu b·∫°n th·∫•y n·ªôi dung h·ªØu √≠ch v√† mu·ªën ƒë√≥ng g√≥p cho blog, b·∫°n c√≥ th·ªÉ ·ªßng h·ªô blog qua m·ªôt trong hai c√°ch:\n1.T√†i kho·∫£n BIDV\nS·ªë t√†i kho·∫£n: 16010000503754\nCh·ªß t√†i kho·∫£n: NGUYEN QUANG TUYEN\n2.T√†i kho·∫£n Techcombank\nS·ªë t√†i kho·∫£n: 19035848694011\nCh·ªß t√†i kho·∫£n: NGUYEN QUANG TUYEN\nM·ªçi s·ª± ·ªßng h·ªô c·ªßa c√°c b·∫°n ƒë·ªÅu r·∫•t tr√¢n tr·ªçng ƒë·ªëi v·ªõi m√¨nh. M√¨nh xin ch√¢n th√†nh c·∫£m ∆°n.\nM√¨nh xin ch√¢n th√†nh c·∫£m ∆°n,\nNguy·ªÖn Quang Tuy·∫øn\n","date":1667030093,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667030093,"objectID":"c9693cfba7a0c38f1e754567e1de0648","permalink":"https://example.com/donate_me/","publishdate":"2022-10-29T14:54:53+07:00","relpermalink":"/donate_me/","section":"","summary":"Ch√†o b·∫°n,\nC·∫£m ∆°n b·∫°n ƒë√£ gh√© qua blog c·ªßa m√¨nh.\nM√¨nh mong mu·ªën nh·ªØng b√†i vi·∫øt c·ªßa m√¨nh c√≥ th·ªÉ ƒë·∫øn g·∫ßn h∆°n t·ªõi m·ªçi ng∆∞·ªùi. Khi m√¨nh vi·∫øt, m√¨nh lu√¥n c·ªë g·∫Øng gi·∫£i th√≠ch m·ªôt c√°ch d·ªÖ hi·ªÉu nh·∫•t t·ª´ c√°c b∆∞·ªõc bi·∫øn ƒë·ªïi to√°n h·ªçc hay c√°c d√≤ng code c·ªßa thu·∫≠t to√°n ƒë√≥.","tags":[],"title":"Donate me","type":"page"},{"authors":[],"categories":[],"content":"News üì∞ Determinable and interpretable network representation for link prediction\nAzure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students\nPenn State researchers to explore using quantum computers to design new drugs\nMultiverse Computing and Mila Join Forces to Advance Artificial Intelligence with Quantum Computing\nQuantum AI Elon Musk Review / Scam App Or Legit?\nVideos üìΩÔ∏è Quantum Machine Learning Explained\nQiskit Falll Fest CIC-IPN Mexico 2022- Quantum Machine Learning\nQuantum Machine Learning Neuroimaging for Alzheimer‚Äôs Disease\nMLBBQ: Quantum Machine Learning by Pavel Popov\nQuantum Machine Learning: Opportunities and Challenges\nHands-on quantum machine learning | Rodrigo Morales | ADC2022\n‚ÄúReinforcement Learning for Quantum Technologies,‚Äù presented by Florian Marquardt\nPublications üìÉ Representation Theory for Geometric Quantum Machine Learning Abstract: Recent advances in classical machine learning have shown that creating models with inductive biases encoding the symmetries of a problem can greatly improve performance. Importation of these ideas, combined with an existing rich body of work at the nexus of quantum theory and symmetry, has given rise to the field of Geometric Quantum Machine Learning (GQML). Following the success of its classical counterpart, it is reasonable to expect that GQML will play a crucial role in developing problem-specific and quantum-aware models capable of achieving a computational advantage. Despite the simplicity of the main idea of GQML ‚Äì create architectures respecting the symmetries of the data ‚Äì its practical implementation requires a significant amount of knowledge of group representation theory. We present an introduction to representation theory tools from the optics of quantum learning, driven by key examples involving discrete and continuous groups. These examples are sewn together by an exposition outlining the formal capture of GQML symmetries via ‚Äúlabel invariance under the action of a group representation‚Äù, a brief (but rigorous) tour through finite and compact Lie group representation theory, a reexamination of ubiquitous tools like Haar integration and twirling, and an overview of some successful strategies for detecting symmetries.\nTheory for Equivariant Quantum Neural Networks Abstract: Most currently used quantum neural network architectures have little-to-no inductive biases, leading to trainability and generalization issues. Inspired by a similar problem, recent breakthroughs in classical machine learning address this crux by creating models encoding the symmetries of the learning task. This is materialized through the usage of equivariant neural networks whose action commutes with that of the symmetry. In this work, we import these ideas to the quantum realm by presenting a general theoretical framework to understand, classify, design and implement equivariant quantum neural networks. As a special implementation, we show how standard quantum convolutional neural networks (QCNN) can be generalized to group-equivariant QCNNs where both the convolutional and pooling layers are equivariant under the relevant symmetry group. Our framework can be readily applied to virtually all areas of quantum machine learning, and provides hope to alleviate central challenges such as barren plateaus, poor local minima, and sample complexity.\nTheoretical Guarantees for Permutation-Equivariant Quantum Neural Networks Abstract: Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry Sn), and show how to build Sn-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and can generalize well from small amounts of data. To verify our results, we perform numerical simulations for a graph state classification task. Our work provides the first theoretical guarantees for equivariant QNNs, thus indicating the extreme power and potential of GQML.\nProtocols for classically training quantum generative models on probability distributions Abstract: Quantum Generative Modelling (QGM) relies on preparing quantum states and generating samples from these states as hidden - or known - probability distributions. As distributions from some classes of quantum states (circuits) are inherently hard to sample classically, QGM represents an excellent ‚Ä¶","date":1667030093,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667030093,"objectID":"3d5ac705d961acc8202e0fd046e4603d","permalink":"https://example.com/newsletter/october-2022/","publishdate":"2022-10-29T14:54:53+07:00","relpermalink":"/newsletter/october-2022/","section":"newsletter","summary":"News üì∞ Determinable and interpretable network representation for link prediction\nAzure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students\nPenn State researchers to explore using quantum computers to design new drugs","tags":[],"title":"October 2022","type":"newsletter"},{"authors":[],"categories":[],"content":" Trong b√†i n√†y, m√¨nh ƒëi qua m·ªôt ph∆∞∆°ng ph√°p x·ª≠ l√Ω b√†i to√°n nearest neighbour b·∫±ng thu·∫≠t to√°n quantum. B√†i vi·∫øt d∆∞·ªõi ƒë√¢y s·∫Ω d·ª±a v√†o b√†i b√°o g·ªëc: Implementing a distance-based classifier with a quantum interference circuit, n·∫øu ai mu·ªën t√¨m hi·ªÉu s√¢u h∆°n v·ªÅ √Ω t∆∞·ªüng n√†y th√¨ c√≥ th·ªÉ gh√© qua.\nN·ªôi dung Squared-Distance Classifier Quantum Squared-Distance Classifier K·∫øt lu·∫≠n Source Code Squared-Distance Classifier ·ªû ƒë√¢y, m√¨nh x√©t v√≠ d·ª• b√†i to√°n ph√¢n lo·∫°i t·∫≠p data Titanic. Gi·∫£ s·ª≠ t·∫≠p data ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng:\n$$ \\mathcal{D} = \\Big\\{ ({\\bf{x}}^1, y^1), \\ldots ({\\bf{x}}^M , y^M) \\Big\\}, $$ trong ƒë√≥ c√°c v√©c-t∆° ƒë·∫ßu v√†o 2 chi·ªÅu: ${ {\\bf{x}}^m = ({x_0}^m, {x_1}^m)^T}, m = 1,2,‚Ä¶,M$ t∆∞·ª£ng tr∆∞ng cho m·ªôt h√†nh kh√°ch tr√™n chuy·∫øn t√†u Titanic ƒë√£ b·ªã nh·∫•n ch√¨m v√†o nƒÉm 1912. Trong ƒë√≥ $x_0$ l√† gi√° v√© trong kho·∫£ng t·ª´ 0 ƒë·∫øn 10,000 ƒë√¥ la, v√† $x_1$ l√† s·ªë hi·ªáu cabin trong kho·∫£ng t·ª´ 1 ƒë·∫øn 2,500. ·ª®ng v·ªõi m·ªói m·ªôt v√©c-t∆° ƒë·∫ßu v√†o l√† nh√£n $y^m = {0,1}$ t∆∞∆°ng ·ª©ng ƒë·ªÉ ch·ªâ ra h√†nh kh√°ch ƒë√≥ ƒë√£ s·ªëng s√≥t hay kh√¥ng.\nSouce N·∫øu t·ª´ng t√¨m hi·ªÉu qua v·ªÅ Machine Learning, ch·∫Øc h·∫≥n c√°c b·∫°n ƒë√£ nghe ho·∫∑c ƒë·ªçc qua v·ªÅ thu·∫≠t to√°n nearest neighbour: v·ªõi m·ªói v√©c-t∆° ƒë·∫ßu v√†o m·ªõi, th√¨ nh√£n c·ªßa n√≥ s·∫Ω ƒë∆∞·ª£c quy·∫øt ƒë·ªãnh b·ªüi ƒëi·ªÉm d·ªØ li·ªáu g·∫ßn nh·∫•t v·ªõi n√≥. C√≥ nhi·ªÅu c√°ch ƒë·ªÉ x√°c ƒë·ªãnh nh·ªØng ƒëi·ªÉm d·ªØ li·ªáu g·∫ßn nh·∫•t ƒë√≥ nh∆∞ng ph·ªï bi·∫øn l√† Euclidean distance. Do v·∫≠y, ta c√≥ c√°ch t√≠nh h·ªá s·ªë cho vi·ªác g√°n nh√£n v√©c-t∆° $\\tilde{x}$ m·ªõi theo nh√£n c·ªßa $x^m$: $$ \\gamma_m = 1-\\frac{1}{c}|\\tilde{{\\bf x}}-{\\bf x}^m|^2, $$ trong ƒë√≥ $c$ l√† h·∫±ng s·ªë. H·ªá s·ªë c√†ng cao ch·ª©ng t·ªè $\\tilde{{\\bf x}}$ c√†ng g·∫ßn $x^m$. G·ªçi $\\tilde{y}$ l√† nh√£n ƒë∆∞·ª£c g√°n cho $\\tilde{{\\bf x}}$, ta c√≥ x√°c xu·∫•t $p_{\\tilde{{\\bf x}}}(\\tilde{y}=1)$ l√† t·ªïng trung b√¨nh h·ªá s·ªë c·ªßa $M_1$ ƒëi·ªÉm d·ªØ li·ªáu m√† c√≥ nh√£n l√† $1$: $$ p_{\\tilde{{\\bf x}}}(\\tilde{y}=1) = \\frac{1}{\\chi}\\frac{1}{M_1} \\sum_{m|y^m=1}(1-\\frac{1}{c}|\\tilde{{\\bf x}}-{\\bf x}^m|^2) $$ T∆∞∆°ng t·ª± nh∆∞ v·∫≠y, $p_{\\tilde{{\\bf x}}}(\\tilde{y}=0)$ l√† t·ªïng trung b√¨nh h·ªá s·ªë c·ªßa c√°c ƒëi·ªÉm d·ªØ li·ªáu m√† c√≥ nh√£n l√† $0$. Trong ƒë√≥ $\\frac{1}{\\chi}$ l√† normalizing factor sao cho $p_{\\tilde{{\\bf x}}}(\\tilde{y}=0)+p_{\\tilde{{\\bf x}}}(\\tilde{y}=1)=1$.\nSouce √Åp d·ª•ng ph∆∞∆°ng ph√°p ta th·∫•y Passenger 3 s·∫Ω g·∫ßn v·ªõi Passenger 1 h∆°n so v·ªõi Passenger 2 (Fig 1.2), v√† m√¥ h√¨nh s·∫Ω ƒë∆∞a ra d·ª± ƒëo√°n l√† $1$ t∆∞∆°ng ·ª©ng v·ªõi survival.\nQuantum Squared-Distance Classifier Gi·ªù h√£y x·ª≠ l√Ω b√†i to√†n n√†y b·∫±ng ph∆∞∆°ng ph√°p ‚Äòquantum‚Äô.\nB∆∞·ªõc 1: Data preprocessing and Encoding ƒê·∫ßu ti√™n ch√∫ng ta s·∫Ω ƒëi t·ªõi m·ªôt c√¢u h·ªèi kinh ƒëi·ªÉn ‚ÄúL√†m sao c√≥ th·ªÉ bi·ªÉu di·ªÖn d·ªØ li·ªáu tr√™n m√°y t√≠nh l∆∞·ª£ng t·ª≠?‚Äù. N·∫øu nh∆∞ tr√™n m√°y t√≠nh truy·ªÅn th·ªëng c√°c th√¥ng tin nh∆∞ ·∫£nh s·∫Ω th∆∞·ªùng ƒë∆∞·ª£c bi·ªÉn di·ªÖn tr√™n kh√¥ng gian RBG c√≥ gi√° tr·ªã t·ª´ 0 ƒë·∫øn 255, hay ch√∫ng ta c√≥ Word Embedding ƒë·ªÉ bi·ªÉu th√¥ng tin d·∫°ng vƒÉn b·∫£n th√†nh c√°c v√©c-t∆°, th√¨ v·∫•n ƒë·ªÅ c·ªßa c√°c thu·∫≠t to√°n l∆∞·ª£ng t·ª≠ c≈©ng nh∆∞ v·∫≠y. Th·ª±c ch·∫•t, ch·ªß ƒë·ªÅ v·ªÅ vi·ªác m√£ h√≥a th√¥ng tin tr√™n kh√¥ng gian l∆∞·ª£ng t·ª≠ (Quantum Embedding) v·∫´n ƒëang ƒë∆∞·ª£c c·ªông ƒë·ªìng nghi√™n c·ª©u quan t√¢m ƒë·∫∑c bi·ªát trong lƒ©nh v·ª±c Quantum Machine Learning. N·∫øu c√°c b·∫°n quan t√¢m ƒë·∫øn ch·ªß ƒë·ªÅ n√†y c√≥ th·ªÉ xem qua paper n√†y ƒë·ªÉ bi·∫øt r√µ h∆°n v·ªÅ c√°c c√°ch m√£ th√¥ng tin trong QML v√† s·ª± quan tr·ªçng c·ªßa n√≥. M√¨nh s·∫Ω l√†m m·ªôt b√†i vi·∫øt chi ti·∫øt h∆°n v·ªÅ ch·ªß ƒë·ªÅ n√†y trong t∆∞∆°ng lai.\nQuay l·∫°i v·ªõi b√†i to√°n c·ªßa ch√∫ng ta, m√¨nh s·∫Ω √°p d·ª•ng m·ªôt ph∆∞∆°ng ph√°p g·ªçi l√† Amplitude Embedding - m·ªôt ph∆∞∆°ng ph√°p r·∫•t ph·ªë bi·∫øn trong QML: Cho $X \\in \\mathbb{R}^N$ l√† m·ªôt v√©c-t∆° ƒë∆°n nh·∫•t ($||X|| = 1$), ta c√≥ th·ªÉ m√£ h√≥a $X$ b·∫±ng $n$ qubits d∆∞·ªõi d·∫°ng: $$ \\ket{\\psi_X} = \\sum_{i=0}^{N-1}x_i \\ket{i}, $$ trong ƒë√≥ $n = \\log{N}$. C√≥ th·ªÉ th·∫•y ph∆∞∆°ng ph√°p n√†y ch·ªâ t·ªën $O(\\log{N})$ qubits ƒë·ªÉ bi·ªÉu di·ªÖn m·ªôt v√©c-t∆° $N$ chi·ªÅu. H√£y l·∫•y v√≠ d·ª• trong b√†i to√°n x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n, gi·∫£ s·ª≠ b·∫°n c√≥ m·ªôt text corpus v·ªõi 10000 t·ª´ th√¨ n·∫øu nh∆∞ c√°ch th√¥ng th∆∞·ªùng ta s·ª≠ d·ª•ng One-hot encoding ta s·∫Ω c·∫ßn t·ªõi 10000 bits ƒë·ªÉ m√£ h√≥a, nh∆∞ng ƒëi·ªÅu n√†y ho√†n to√°n c√≥ th·ªÉ gi·∫£i quy·∫øt v·ªõi 14 ($\\lceil \\log{10000} \\rceil$) qubits v·ªõi amplitude embedding.\nT·ª´ ƒë√¢y, v·ªõi m·ªói ƒë·∫ßu v√†o $\\ket{\\psi_{\\bf\\tilde{x}}}$ m·ªõi, b√†i to√°n s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o d∆∞·ªõi d·∫°ng:\nTrong ƒë√≥ $\\ket{m}$ v√† $\\ket{y^m}$ m√£ h√≥a cho s·ªë th·ª© t·ª± v√† nh√£n t∆∞∆°ng ·ª©ng c·ªßa v√©c-t∆° ƒë·∫ßu v√†o th·ª© $m^{th}$. Tuy nhi√™n ƒëi·ªÅu ch√∫ √Ω ·ªü ƒë√¢y n·∫±m ·ªü ancilla qubit ƒë∆∞·ª£c k·∫øt n·ªëi v·ªõi $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ v√† $\\ket{\\psi_{\\bf{x}^m}}$. Ch√∫ √Ω r·∫±ng khi ta th·ª±c hi·ªán ph√©p do tr√™n m·ªôt ho·∫∑c m·ªôt h·ªá qubits th√¨ qubit(s) s·∫Ω b·ªã collapsed hay terminated. V·∫≠y khi ta chuy·ªÉn ph√©p ƒëo c·ªßa $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ hay $\\ket{\\psi_{\\bf{x}^m}}$ sang ph√©p ƒëo c·ªßa m·ªôt ancilla qubit ƒë∆∞·ª£c k·∫øt n·ªëi v·ªõi ch√∫ng th√¨ ta v·∫´n thu ƒë∆∞·ª£c k·∫øt qu·∫£ c·∫ßn thi·∫øt c·ªßa $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ v√† $\\ket{\\psi_{\\bf{x}^m}}$ m√† kh√¥ng c·∫ßn ch·∫•m d·ª©t (terminate) c·∫£ h·ªá th·ªëng. K·ªπ thu·∫≠t n√†y r·∫•t hay s·ª≠ d·ª•ng ·ªü trong c√°c thu·∫≠t to√°n l∆∞·ª£ng t·ª≠ v√† vi·ªác k·∫øt n·ªëi gi·ªØa ancilla qubit v·ªõi h·ªá th·ªëng l√† ch√∫ng ta ƒëang t·∫°o ra entanglement - m·ªôt t√≠nh ch·∫•t quan tr·ªçng kh√°c trong lƒ©nh v·ª±c t√≠nh to√°n l∆∞·ª£ng t·ª≠.\nNh∆∞ v·∫≠y v·ªõi v√≠ d·ª• Titanic tr√™n ta c√≥: $$ \\ket{\\mathcal{D}} = \\frac{1}{\\sqrt{4}} \\Big\\{\\ket{0}[\\ket{0}(0.866\\ket{0}+0.5\\ket{1})+\\ket{1}(0.921\\ket{0}+0.39\\ket{1})]\\ket{1} $$ $$ + ‚Ä¶","date":1665505552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665505552,"objectID":"41280be87b10845fb59ca0bce349f39e","permalink":"https://example.com/post/example-1/","publishdate":"2022-10-11T23:25:52+07:00","relpermalink":"/post/example-1/","section":"post","summary":"Trong b√†i n√†y, m√¨nh ƒëi qua m·ªôt ph∆∞∆°ng ph√°p x·ª≠ l√Ω b√†i to√°n nearest neighbour b·∫±ng thu·∫≠t to√°n quantum. B√†i vi·∫øt d∆∞·ªõi ƒë√¢y s·∫Ω d·ª±a v√†o b√†i b√°o g·ªëc: Implementing a distance-based classifier with a quantum interference circuit, n·∫øu ai mu·ªën t√¨m hi·ªÉu s√¢u h∆°n v·ªÅ √Ω t∆∞·ªüng n√†y th√¨ c√≥ th·ªÉ gh√© qua.","tags":[],"title":"B√†i 2: Quantum Squared-Distance Classifier","type":"post"},{"authors":[],"categories":[],"content":" N·ªôi dung Gi·ªõi thi·ªáu b√†i to√°n Classical Interference Quantum Interference K·∫øt lu·∫≠n Tr∆∞·ªõc khi ƒëi v√†o c√°c thu·∫≠t to√°n quantum trong h·ªçc m√°y, m√¨nh mu·ªën so s√°nh b√†i to√°n suy lu·∫≠n x√°c su·∫•t tr√™n m√°y t√≠nh truy·ªÅn th·ªëng c≈©ng nh∆∞ tr√™n m√°y t√≠nh l∆∞·ª£ng t·ª≠. B√†i vi·∫øt n√†y s·∫Ω gi√∫p c√°c b·∫°n vi·∫øt qua nh·ªØng th√†nh ph·∫ßn c∆° b·∫£n c·ªßa v·∫≠t l√Ω l∆∞·ª£ng t·ª≠: qubits, unitary transformation, v√† measurement v√† c√°ch ho·∫°t ƒë·ªông c·ªßa ch√∫ng th√¥ng qua m·ªôt v√≠ d·ª• c·ª• th·ªÉ.\nGi·ªõi thi·ªáu b√†i to√°n Cho hai ƒë·ªìng xu ƒë·ªìng ch·∫•t: $c_1$ v√† $c_2$ v·ªõi x√°c su·∫•t ·ªü m·∫∑t s·∫•p (tail) hay m·∫∑t ng·ª≠a (head) l√† nh∆∞ nhau. Kh√¥ng gian m·∫´u c·ªßa vi·ªác tung 2 ƒë·ªìng xu tr√™n s·∫Ω bao g·ªìm: (head, head), (head, tail), (tail, head), v√† (tail, tail). M√¨nh s·∫Ω x√©t b√†i to√°n nh∆∞ sau: B∆∞·ªõc 1, ta l·∫≠t 2 ƒë·ªìng xu th√†nh m·∫∑t ng·ª≠a (head s·∫Ω l√† gi√° tr·ªã ban ƒë·∫ßu c·ªßa 2 ƒë·ªëng xu). B∆∞·ªõc 2, ta tung ƒë·ªìng xu th·ª© nh·∫•t $c_1$ v√† ki·ªÉm tra k·∫øt qu·∫£. V√† b∆∞·ªõc 3, ta c≈©ng l·∫°i tung ƒë·ªìng xu $c_1$ l·∫ßn th·ª© hai v√† ki·ªÉm tra k·∫øt qu·∫£ (gi·∫£ s·ª≠ k·∫øt qu·∫£ thu ƒë∆∞·ª£c l√† sau s·ªë l·∫ßn th·ª≠ ƒë·ªß l·ªõn).\nClassical Interference (Suy lu·∫≠n x√°c su·∫•t truy·ªÅn th·ªëng) C√≥ th·ªÉ th·∫•y v·ªõi m√°y t√≠nh truy·ªÅn th·ªëng (classical computer), 2 ƒë·ªìng xu c√≥ th·ªÉ ƒë∆∞·ª£c coi l√† 2 bits ng·∫´u nhi√™n (random bits). ·ªû b∆∞·ªõc 1, b√†i to√°n s·∫Ω ƒë∆∞a v·ªÅ k·∫øt qu·∫£ (head, head). Tuy nhi√™n, sau b∆∞·ªõc hai th√¨ (head, head) v√† (tail, head) s·∫Ω c√≥ x√°c su·∫•t b·∫±ng nhau v√† b·∫±ng 0.5. Ph√¢n ph·ªëi n√†y s·∫Ω kh√¥ng thay ƒë·ªïi sau b∆∞·ªõc 3.\nQuantum Interference (Suy lu·∫≠n x√°c su·∫•t tr√™n m√°y t√≠nh quantum) Tuy nhi√™n, ·ªü ƒë√¢y s·∫Ω c√≥ m·ªôt ch√∫t kh√°c bi·ªát n·∫øu ta bi·ªÉu di·ªÖn b√†i to√°n tr√™n m√°y t√≠nh quantum. Hai ƒë·ªìng xu s·∫Ω ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng hai qubits (quantum bits). M·ªói qubit c√≥ d·∫°ng $\\alpha \\ket{0} + \\beta \\ket{1}$, trong ƒë√≥ $\\ket{0}$ v√† $\\ket{1}$ l√† hai tr·∫°ng th√°i c∆° s·ªü (basis state) ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng v√©c-t∆° t∆∞∆°ng ·ª©ng: $[1,0]^T$ v√† $[0,1]^T$. C√≥ th·ªÉ th·∫•y r·∫±ng $\\ket{0}$ v√† $\\ket{1}$ t∆∞∆°ng ·ª©ng v·ªõi hai gi√° tr·ªã nh·ªã ph√¢n 0, 1 ·ªü m√°y t√≠nh truy·ªÅn th·ªëng; tuy nhi√™n, thay v√¨ ƒë∆∞·ª£c m√£ h√≥a r·ªùi r·∫°c th√†nh chu·ªói bit 1 ho·∫∑c 0, m·ªói qubit c√≥ th·ªÉ t·∫°o th√†nh m·ªôt t·ªï h·ª£p tuy·∫øn t√≠nh c·ªßa c√°c tr·∫°ng th√°i c∆° s·ªü theo x√°c su·∫•t: $$ p(0) = |\\alpha|^2, p(1) = |\\beta|^2; \\alpha, \\beta \\in \\mathbb{C} $$ Ch√∫ √Ω r·∫±ng $|\\alpha|^2 + |\\beta|^2 = 1$ ƒë·ªÉ th·ªèa m√£n x√°c su·∫•t tr√™n. Nh∆∞ v·∫≠y n·∫øu ta coi hai m·∫∑t c·ªßa ƒë·ªìng xu l√† hai tr·∫°ng th√°i c∆° s·ªü: $\\ket{head} = \\ket{0}$ v√† $\\ket{tail} = \\ket{1}$, th√¨ vi·ªác tung ƒë·ªìng xu s·∫Ω t∆∞∆°ng ƒë∆∞∆°ng vi·ªác ch√∫ng ta th·ª±c hi·ªán bi·∫øn ƒë·ªïi Hadamard. Vi·ªác bi·∫øn ƒë·ªïi ·ªü ƒë√¢y tr√™n m√°y t√≠nh quantum ch√≠nh l√† ph√©p nh√¢n ma tr·∫≠n Hadamard v·ªõi tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa qubit. Trong ƒë√≥ bi·∫øn ƒë·ªïi Hadamard c√≥ th·ªÉ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d∆∞·ªõi d·∫°ng ma tr·∫≠n:\n$$ H = \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{cc} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{array} \\right) $$ T·ª´ ƒë√≥, ta c√≥ th·ªÉ tri·ªÉn khai b√†i to√°n tr√™n nh∆∞ sau: 2 qubits s·∫Ω ƒë∆∞·ª£c kh·ªüi t·∫°o th√†nh $\\ket{head}\\ket{head}$ sau b∆∞·ªõc 1. ·ªû b∆∞·ªõc 2, ta nh√¢n ma tr·∫≠n Hadamard v·ªõi tr·∫°ng th√°i c·ªßa qubit th·ª© nh·∫•t, ta c√≥: $$ H\\ket{head}\\ket{head}= H\\ket{0}\\ket{head} = \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{cc} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right) \\ket{head} $$ $$ =\\!\\frac{1}{\\sqrt{2}}\\left( \\begin{array}{c} 1 \\\\ 1 \\end{array} \\right)\\! \\ket{head}\\! =\\! \\frac{1}{\\sqrt{2}} (\\ket{0}\\!+\\!\\ket{1})\\!\\ket{head}\\! =\\! \\frac{1}{\\sqrt{2}} (\\ket{head}\\!+\\!\\ket{tail})\\!\\ket{head} $$ $$ = \\frac{1}{\\sqrt{2}}\\ket{head}\\ket{head} + \\frac{1}{\\sqrt{2}}\\ket{tail}\\ket{head} $$ Nh∆∞ v·∫≠y, ta th·∫•y gi·ªëng nh∆∞ tr∆∞·ªùng h·ª£p tr√™n, sau b∆∞·ªõc 2 x√°c su·∫•t ƒë·∫°t ƒë∆∞·ª£c $\\ket{head}\\ket{head}$ v√† $\\ket{tail}\\ket{head}$ l√† b·∫±ng nhau l√† c≈©ng b·∫±ng $(\\frac{1}{\\sqrt{2}})^2 = 0.5$. Tuy nhi√™n s·ª± kh√°c bi·ªát n·∫±m ·ªü b∆∞·ªõc 3, n·∫øu ta ti·∫øp t·ª•c tung ƒë·ªìng xu th·ª© nh·∫•t (hay th·ª±c hi·ªán bi·∫øn ƒë·ªïi Hadamard), v·ªõi m·ªôt ch√∫t t√≠nh to√°n ta nh·∫≠n ƒë∆∞·ª£c k·∫øt qu·∫£:\n$$ \\frac{1}{\\sqrt{2}}H\\ket{head}\\ket{head} + \\frac{1}{\\sqrt{2}}H\\ket{tail}\\ket{head} = \\ket{head}\\ket{head} $$ Sau b∆∞·ªõc 3 ta s·∫Ω lu√¥n nh·∫≠n ƒë∆∞·ª£c $\\ket{head}\\ket{head}$, k·∫øt qu·∫£ n√†y kh√°c ho√†n to√°n khi th·ª±c hi·ªán b√†i to√°n tr√™n m√°y t√≠nh truy·ªÅn th·ªëng. C√≥ th·ªÉ n√≥i ƒë√¢y l√† m·ªôt s·ª± kh√°c nhau th√∫ v·ªã gi·ªØa m√°y t√≠nh l∆∞·ª£ng t·ª≠ v√† m√°y t√≠nh truy·ªÅn th·ªëng. Kh√°c v·ªõi m√°y t√≠nh truy·ªÅn th·ªëng c√≥ xu h∆∞·ªõng t·ªëi ƒëa h√≥a s·ª± kh√¥ng ch·∫Øc ch·∫Øn (maximize uncertainty) v√¨ lu√¥n cho ra k·∫øt qu·∫£ 50-50 gi·ªØa 2 tr·∫°ng th√°i (head, head) v√† (tail, head), th√¨ m√°y t√≠nh l∆∞·ª£ng t·ª≠ cho ra k·∫øt qu·∫£ c√≥ ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn th·∫•p h∆°n. Ch√≠nh v√¨ l√Ω do n√†y, ƒë√£ c√≥ nghi√™n c·ª©u √°p d·ª•ng quantum inference nh∆∞ m·ªôt h√†m d·ª± ƒëo√°n (prediction function) trong b√†i to√°n h·ªçc m√°y c√≥ gi√°m s√°t (supervised learning) [1]\nS·ª± kh√°c nhau tr√™n c≈©ng d·∫´n ta t·ªõi m·ªôt v·∫•n ƒë·ªÅ quan tr·ªçng kh√°c: measurement (ph√©p ƒëo). Ph√©p ƒëo ch√≠nh l√† c·∫ßu n·ªëi gi·ªØa quantum v√† classical, gi√∫p ch√∫ng ta ƒë√°nh gi√° v√† ph√¢n t√≠ch tr·∫°ng th√°i hi·ªán t·∫°i c·ªßa m·ªôt ho·∫∑c m·ªôt h·ªá qubit, v√† n√≥ th∆∞·ªùng mang t√≠nh th·ªëng k√™ x√°c su·∫•t h∆°n l√† m·ªôt ƒë√°nh gi√° ƒë∆°n l·∫ª. N√≥i c√°ch kh√°c, ph√©p ƒëo cho ph√©p ch√∫ng ta ph√° b·ªè t√≠nh ch·ªëng ch√¢t c·ªßa qubits (ph√° b·ªè ƒëi t·ªï h·ª£p tuy·∫øn t√≠nh), t·ª´ ƒë√≥ l√†m cho tr·∫°ng th√°i c·ªßa qubit ‚Äòcollapse‚Äô v·ªÅ m·ªôt trong c√°c tr·∫°ng th√°i c∆° s·ªü. V√≠ d·ª•, th·ª±c hi·ªán ph√©p ƒëo m·ªôt qubit b·∫•t k·ª≥ $\\ket{\\phi} = \\alpha \\ket{0} + \\beta \\ket{1}$ tr√™n c∆° s·ªü chu·∫©n (canonical basis) th√¨ ta s·∫Ω ƒë·∫°t ƒë∆∞·ª£c gi√° tr·ªã 0 ‚Ä¶","date":1665301286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665301286,"objectID":"e81b5d40081a3db7ff1a08d56238a88b","permalink":"https://example.com/post/fair-coins/","publishdate":"2022-10-09T14:41:26+07:00","relpermalink":"/post/fair-coins/","section":"post","summary":"N·ªôi dung Gi·ªõi thi·ªáu b√†i to√°n Classical Interference Quantum Interference K·∫øt lu·∫≠n Tr∆∞·ªõc khi ƒëi v√†o c√°c thu·∫≠t to√°n quantum trong h·ªçc m√°y, m√¨nh mu·ªën so s√°nh b√†i to√°n suy lu·∫≠n x√°c su·∫•t tr√™n m√°y t√≠nh truy·ªÅn th·ªëng c≈©ng nh∆∞ tr√™n m√°y t√≠nh l∆∞·ª£ng t·ª≠.","tags":[],"title":"B√†i 1: Quantum vs Classical Interference: First Example","type":"post"},{"authors":[],"categories":[],"content":" N·ªôi dung Quantum Machine Learning l√† g√¨? M·ªôt v√†i h∆∞·ªõng ti·∫øp c·∫≠n c·ªßa Quantum Machine Learning K·∫øt lu·∫≠n Kh√°c v·ªõi c√°c ch·ªß ƒë·ªÅ c·ªßa Machine Learning hay Deep Learning khi m√† c√°c ·ª©ng d·ª•ng c·ªßa ch√∫ng ƒëang d·∫ßn tr·ªü n√™n ph·ªï bi·∫øn nh·ªØng nƒÉm g·∫ßn ƒë√¢y, ch·ªß ƒë·ªÅ v·ªÅ Quantum Machine Learning (hay QML) l√† m·ªôt lƒ©nh v·ª±c nghi√™n c·ª©u m·ªõi v√† ƒëang ƒë∆∞·ª£c ch√∫ √Ω ·ªü c√°c c√¥ng ty h√†ng ƒë·∫ßu th·∫ø gi·ªõi nh∆∞ IBM hay Google. Do ƒë√≥, ·ªü b√†i vi·∫øt n√†y ngo√†i vi·ªác cung c·∫•p cho b·∫°n ƒë·ªçc c√°i nh√¨n c·ª• th·ªÉ Quantum Machine Learning l√† g√¨, m√¨nh c≈©ng s·∫Ω gi·∫£i th√≠ch t·∫°i sao ch√∫ng ta l·∫°i c·∫ßn QML v√† m·ªôt v√†i h∆∞·ªõng ti·∫øp c·∫≠n c·ª• th·ªÉ.\nQuantum Machine Learning l√† g√¨? N·∫øu nh∆∞ ai ƒë√£ l√†m quen v·ªõi c√°c b√†i to√°n c·ªßa Machine Learning hay Deep Learning, c√°c m√¥ h√¨nh ƒëang d·∫ßn ƒë∆∞·ª£c x√¢y d·ª±ng l·ªõn h∆°n v√† ph·ª©c t·∫°p h∆°n ƒë·ªÉ gi·∫£i quy·∫øt c√°c b√†i to√°n kh√≥ (hard combinatorial optimization problems), n√≥ d·∫´n t·ªõi vi·ªác ti√™u t·ªën r·∫•t nhi·ªÅu t√†i nguy√™n t√≠nh to√°n (computational resources) trong vi·ªác hu·∫•n luy·ªán c≈©ng nh∆∞ l√† v·∫≠n h√†nh. M·ªôt v√≠ d·ª• ƒëi·ªÉn h√¨nh l√† m√¥ h√¨nh GPT-3 g·ªìm 175 t·ª∑ tham s·ªë s·∫Ω c·∫ßn t·ªën 355 nƒÉm v√† g·∫ßn 5 tri·ªáu ƒë√¥ n·∫øu train tr√™n m·ªôt NVIDIA Tesla V100 GPU. Do ƒë√≥, tr√™n th·ª±c t·∫ø h·ªç ƒë√£ train GPT-3 v·ªõi 1024 A100 GPUs v√† m·∫•t 34 ng√†y.\nTuy nhi√™n, v·∫•n ƒë·ªÅ ƒë√≥ c√≥ th·ªÉ s·∫Ω ƒë∆∞·ª£c gi·∫£i quy·∫øt v·ªõi s·ª± xu·∫•t hi·ªán c·ªßa m√°y t√≠nh l∆∞·ª£ng t·ª≠ (quantum computer). M√°y t√≠nh l∆∞·ª£ng t·ª≠ ƒë∆∞·ª£c ph√°t tri·ªÉn d·ª±a theo c√°c thuy·∫øt c·ªßa v·∫≠t l√Ω l∆∞·ª£ng t·ª≠ ƒë·ªÉ ƒë∆∞a ra m·ªôt kh·∫£ nƒÉng t√≠nh to√°n v∆∞·ª£t tr·ªôi so v·ªõi m√°y t√≠nh truy·ªÅn th·ªëng. H√£y l·∫•y m·ªôt b√†i to√°n t√¨m ki·∫øm l√† m·ªôt v√≠ d·ª•: gi·∫£ s·ª≠ b·∫°n ph·∫£i t√¨m 1 qu·∫£ b√≥ng trong 1 tri·ªáu ngƒÉn k√©o v√† c√¢u h·ªèi l√† b·∫°n s·∫Ω ph·∫£i m·ªü qua bao nhi√™u ngƒÉn k√©o tr∆∞·ªõc khi t√¨m ƒë∆∞·ª£c qu·∫£ b√≥ng ƒë√≥? ƒê√¥i khi b·∫°n s·∫Ω may m·∫Øn t√¨m ƒë∆∞·ª£c qu·∫£ b√≥ng trong ch·ªâ v√†i l·∫ßn th·ª≠ v√† ng∆∞·ª£c l·∫°i b·∫°n c≈©ng c√≥ th·ªÉ ph·∫£i m·ªü g·∫ßn nh∆∞ to√†n b·ªô 1 tri·ªáu ngƒÉn k√©o kia. Trung b√¨nh b·∫°n s·∫Ω c·∫ßn t·ªõi 500,000 l∆∞·ª£t ƒë·ªÉ t√¨m ra qu·∫£ b√≥ng. Tuy nhi√™n, v·ªõi m√°y t√≠nh l∆∞·ª£ng t·ª≠, b·∫°n c√≥ th·ªÉ th·ª±c hi·ªán b√†i to√°n ƒë√≥ trong v√≤ng 1000 l∆∞·ª£t b·∫±ng m·ªôt thu·∫≠t to√°n ƒë∆∞·ª£c g·ªçi l√† Grover‚Äôs algorithm.\nT·ª´ ƒë√≥ s·ª± ra ƒë·ªùi c·ªßa Quantum Machine Learning nh∆∞ m·ªôt s·ª± giao thoa c·ªßa c√°c thu·∫≠t to√°n tr√™n m√°y t√≠nh l∆∞·ª£ng t·ª≠ v·ªõi m√¥ h√¨nh Machine Learning ƒë·ªÉ c·∫£i thi·ªán c·∫£ v·ªÅ m·∫∑t t√≠nh to√°n c≈©ng nh∆∞ ƒë·ªô ch√≠nh x√°c (ƒë∆∞·ª£c g·ªçi l√† quantum advantage).\nM·ªôt v√†i h∆∞·ªõng ti·∫øp c·∫≠n c·ªßa Quantum Machine Learning Cho ƒë·∫øn n√†y ƒë√£ c√≥ kh√° nhi·ªÅu h∆∞·ªõng tri·ªÉn khai QML ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t, m·∫∑c d√π nhi·ªÅu trong s·ªë ch√∫ng v·∫´n ch·ªâ l√† l√Ω thuy·∫øt thu·∫ßn t√∫y v√† c·∫ßn m·ªôt m√°y t√≠nh l∆∞·ª£ng t·ª≠ ho√†n ch·ªânh ƒë·ªÉ th·ª±c nghi·ªám; tuy nhi√™n, c≈©ng ƒë√£ c√≥ c√°c thu·∫≠t to√°n ƒë√£ ƒë∆∞·ª£c tri·ªÉn khai tr√™n quy m√¥ nh·ªè v√† ch·ª©ng minh ƒë·∫°t ƒë∆∞·ª£c ‚Äòquantum advantage‚Äô. Sau ƒë√¢y m√¨nh s·∫Ω ƒë·ªÅ c·∫≠p t·ªõi hai h∆∞·ªõng ti·ªáp c·∫≠n ph·ªï bi·∫øn c·ªßa QML.\na) QRAM-based Quantum Machine Learning\nT∆∞∆°ng t·ª± RAM (Random Access Memory) ·ªü c√°c m√°y t√≠nh truy·ªÅn th·ªëng, c√°c nh√† nghi√™n c·ª©u ƒë√£ gi·ªõi thi·ªáu m·ªôt ‚Äòquantum-version‚Äô c·ªßa RAM ƒë∆∞·ª£c g·ªçi l√† QRAM ƒë·ªÉ x·ª≠ l√Ω v·∫•n ƒë·ªÅ ghi v√† ƒë·ªçc th√¥ng tin tr√™n m√°y t√≠nh l∆∞·ª£ng t·ª≠. C√≥ th·ªÉ n√≥i QRAM l√† m·ªôt ph·∫ßn r·∫•t quan tr·ªçng nhi·ªÅu thu·∫≠t to√°n c·ªßa QML. Th·∫≠m ch√≠ ch√∫ng ƒë·∫°t ƒë∆∞·ª£c ‚Äòquantum advantage‚Äô l√† nh·ªù QRAM.\nM·ªôt ·ª©ng d·ª•ng c·ª• th·ªÉ v√† c≈©ng nh∆∞ ƒë∆∞·ª£c d√πng nhi·ªÅu nh·∫•t c·ªßa QRAM l√† kh·∫£ nƒÉng c·∫£i thi·ªán t·ªëc ƒë·ªô t√≠nh to√°n c·ªßa t√≠ch v√¥ h∆∞·ªõng (dot product) hay Kernel Method - m·ªôt ph∆∞∆°ng ph√°p quen thu·ªôc c·ªßa Machine Learning m√† ƒëi·ªÉn h√¨nh l√† Support Vector Machine (SVM). V·ªõi s·ª± can thi·ªáp c·ªßa QRAM, ta c√≥ th·ªÉ t√≠nh t√≠ch v√¥ h∆∞·ªõng $x^Ty$ v·ªõi ƒë·ªô ph·ª©c t·∫°p l√† $O(logN)$ so v·ªõi $O(N)$ tr√™n m√°y t√≠nh truy·ªÅn th·ªëng, trong ƒë√≥ $x, y$ l√† c√°c vectors $N$ chi·ªÅu.\nT·ª´ ƒë√≥ c√°c thu·∫≠t to√°n ƒë∆∞·ª£c ra ƒë·ªùi nh∆∞ l√† Quantum K-Means d·ª±a v√†o QRAM ƒë·ªÉ c√≥ ƒë·ªô ph·ª©c t·∫°p $O(log(Nd))$ (so v·ªõi $O(Nd)$ c·ªßa thu·∫≠t to√°n K-Means), trong ƒë√≥ $N$ l√† s·ªë data v√† $d$ l√† s·ªë chi·ªÅu. Hay Quantum Support Vector Machine ƒë·∫°t ƒë∆∞·ª£c ƒë·ªô ph·ª©c t·∫°p $O(log(Nd))$ so v·ªõi $O(poly(N,d))$ c·ªßa thu·∫≠t to√°n SVM b√¨nh th∆∞·ªùng, v√† m·ªôt s·ªë kh√°c: Quantum PCA, Quantum K-Medians, etc.\n·ªû h∆∞·ªõng ti·∫øp c·∫≠n n√†y, c√°c thu·∫≠t to√°n s·∫Ω d·ª±a v√†o kh·∫£ nƒÉng t√≠nh to√°n v∆∞·ª£t tr·ªôi c·ªßa quantum computing ƒë·ªÉ c·∫£i thi·ªán ƒë·ªô ph·ª©c t·∫°p. Tuy nhi√™n ·ªü m√°y t√≠nh l∆∞·ª£ng t·ª≠ kh√¥ng ch·ªâ c√≥ v·∫≠y. Th√¥ng tin ·ªü ƒë√≥ ƒë∆∞·ª£c bi·ªÉu di·ªÖn d·ª±a theo nguy√™n l√Ω ch·ªìng ch·∫≠p (Superposition), thay v√¨ ƒë∆∞·ª£c m√£ h√≥a r·ªùi r·∫°c th√†nh c√°c bits 0 v√† 1, c√≥ nghƒ©a th√¥ng tin c√≥ th·ªÉ t·ªìn t·∫°i ƒë·ªìng th·ªùi ·ªü bit 0 v√† bit 1 theo m·ªôt ph√¢n ph·ªëi n√†o ƒë√≥. Do ƒë√≥, ‚Äôlearning space‚Äô ·ªü m√°y t√≠nh l∆∞·ª£ng t·ª≠ s·∫Ω ho√†n to√†n kh√°c v√† th·∫≠m ch√≠ ƒë∆∞·ª£c m·ªü r·ªông h∆°n so v·ªõi m√°y t√≠nh truy·ªÅn th·ªëng. Th·ª±c t·∫ø ƒë√£ c√≥ nhi·ªÅu nghi√™n c·ª©u v·ªõi m·ª•c ti√™u kh√°m ph√° kh√¥ng gian n√†y ƒë·ªÉ c·∫£i thi·ªán kh·∫£ nƒÉng h·ªçc t·∫≠p (learning capability) c·ªßa m√¥ h√¨nh Machine Learning v√† h∆∞·ªõng ti·∫øp c·∫≠n sau ƒë√¢y l√† v√≠ d·ª• ƒëi·ªÉn h√¨nh cho vi·ªác n√†y.\nb) Quantum Neural Network.\nƒê∆∞·ª£c th√∫c ƒë·∫©y t·ª´ s·ª± th√†nh c√¥ng c·ªßa m·∫°ng h·ªçc s√¢u (classical deep learning), m·∫°ng n∆°-ron l∆∞·ª£ng t·ª≠ (Quantum Neural Network, hay QNN) c≈©ng mang nh·ªØng n√©t t∆∞∆°ng ƒë·ªìng v·ªõi m·∫°ng n∆°-ron truy·ªÅn th·ªëng (NN). Ch√∫ng ƒë∆∞·ª£c thi·∫øt k·∫ø theo c·∫•u tr√∫c feed-forward, trong ƒë√≥ c√°c layers l√† c√°c ph√©p bi·∫øn ƒë·ªïi ƒë∆°n nh·∫•t (unitary transformation). H·∫ßu h·∫øt c·∫•u tr√∫c c·ªßa QNN d·ª±a theo Variational Quantum Circuits hay th∆∞·ªùng ƒë∆∞·ª£c g·ªçi Parameterised Quantum Circuits. ·ªû ƒë√≥ c√°c bi·∫øn ƒë·ªïi trong c·∫•u tr√∫c m·∫°ng QNN s·∫Ω ph·ª• thu·ªôc v√†o tham s·ªë $\\theta$ (learning parameters) v√† ‚Ä¶","date":1664552270,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664552270,"objectID":"db5dfd9db2c98204b3c7e2f0bf0ffcb8","permalink":"https://example.com/post/why-qml/","publishdate":"2022-09-30T22:37:50+07:00","relpermalink":"/post/why-qml/","section":"post","summary":"N·ªôi dung Quantum Machine Learning l√† g√¨? M·ªôt v√†i h∆∞·ªõng ti·∫øp c·∫≠n c·ªßa Quantum Machine Learning K·∫øt lu·∫≠n Kh√°c v·ªõi c√°c ch·ªß ƒë·ªÅ c·ªßa Machine Learning hay Deep Learning khi m√† c√°c ·ª©ng d·ª•ng c·ªßa ch√∫ng ƒëang d·∫ßn tr·ªü n√™n ph·ªï bi·∫øn nh·ªØng nƒÉm g·∫ßn ƒë√¢y, ch·ªß ƒë·ªÅ v·ªÅ Quantum Machine Learning (hay QML) l√† m·ªôt lƒ©nh v·ª±c nghi√™n c·ª©u m·ªõi v√† ƒëang ƒë∆∞·ª£c ch√∫ √Ω ·ªü c√°c c√¥ng ty h√†ng ƒë·∫ßu th·∫ø gi·ªõi nh∆∞ IBM hay Google.","tags":[],"title":"B√†i 0: Gi·ªõi thi·ªáu v·ªÅ Quantum Machine Learning","type":"post"}]