
    
    
    
    
    [{"authors":[],"categories":[],"content":"News ğŸ“° Determinable and interpretable network representation for link prediction\nAzure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students\nPenn State researchers to explore using quantum computers to design new drugs\nMultiverse Computing and Mila Join Forces to Advance Artificial Intelligence with Quantum Computing\nQuantum AI Elon Musk Review / Scam App Or Legit?\nVideos ğŸ“½ï¸ Quantum Machine Learning Explained\nQiskit Falll Fest CIC-IPN Mexico 2022- Quantum Machine Learning\nQuantum Machine Learning Neuroimaging for Alzheimerâ€™s Disease\nMLBBQ: Quantum Machine Learning by Pavel Popov\nQuantum Machine Learning: Opportunities and Challenges\nHands-on quantum machine learning | Rodrigo Morales | ADC2022\nâ€œReinforcement Learning for Quantum Technologies,â€ presented by Florian Marquardt\nPublications ğŸ“ƒ Representation Theory for Geometric Quantum Machine Learning Abstract: Recent advances in classical machine learning have shown that creating models with inductive biases encoding the symmetries of a problem can greatly improve performance. Importation of these ideas, combined with an existing rich body of work at the nexus of quantum theory and symmetry, has given rise to the field of Geometric Quantum Machine Learning (GQML). Following the success of its classical counterpart, it is reasonable to expect that GQML will play a crucial role in developing problem-specific and quantum-aware models capable of achieving a computational advantage. Despite the simplicity of the main idea of GQML â€“ create architectures respecting the symmetries of the data â€“ its practical implementation requires a significant amount of knowledge of group representation theory. We present an introduction to representation theory tools from the optics of quantum learning, driven by key examples involving discrete and continuous groups. These examples are sewn together by an exposition outlining the formal capture of GQML symmetries via â€œlabel invariance under the action of a group representationâ€, a brief (but rigorous) tour through finite and compact Lie group representation theory, a reexamination of ubiquitous tools like Haar integration and twirling, and an overview of some successful strategies for detecting symmetries.\nTheory for Equivariant Quantum Neural Networks Abstract: Most currently used quantum neural network architectures have little-to-no inductive biases, leading to trainability and generalization issues. Inspired by a similar problem, recent breakthroughs in classical machine learning address this crux by creating models encoding the symmetries of the learning task. This is materialized through the usage of equivariant neural networks whose action commutes with that of the symmetry. In this work, we import these ideas to the quantum realm by presenting a general theoretical framework to understand, classify, design and implement equivariant quantum neural networks. As a special implementation, we show how standard quantum convolutional neural networks (QCNN) can be generalized to group-equivariant QCNNs where both the convolutional and pooling layers are equivariant under the relevant symmetry group. Our framework can be readily applied to virtually all areas of quantum machine learning, and provides hope to alleviate central challenges such as barren plateaus, poor local minima, and sample complexity.\nTheoretical Guarantees for Permutation-Equivariant Quantum Neural Networks Abstract: Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry Sn), and show how to build Sn-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and can generalize well from small amounts of data. To verify our results, we perform numerical simulations for a graph state classification task. Our work provides the first theoretical guarantees for equivariant QNNs, thus indicating the extreme power and potential of GQML.\nProtocols for classically training quantum generative models on probability distributions Abstract: Quantum Generative Modelling (QGM) relies on preparing quantum states and generating samples from these states as hidden - or known - probability distributions. As distributions from some classes of quantum states (circuits) are inherently hard to sample classically, QGM represents an excellent â€¦","date":1667030093,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1667030093,"objectID":"4e6d6305089bb3f03343b938b120491d","permalink":"https://example.com/newsletter/octorber-2022/","publishdate":"2022-10-29T14:54:53+07:00","relpermalink":"/newsletter/octorber-2022/","section":"newsletter","summary":"News ğŸ“° Determinable and interpretable network representation for link prediction\nAzure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students\nPenn State researchers to explore using quantum computers to design new drugs","tags":[],"title":"Octorber 2022","type":"newsletter"},{"authors":[],"categories":[],"content":"Trong bÃ i nÃ y, mÃ¬nh Ä‘i qua má»™t phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ bÃ i toÃ¡n nearest neighbour báº±ng thuáº­t toÃ¡n quantum. BÃ i viáº¿t dÆ°á»›i Ä‘Ã¢y sáº½ dá»±a vÃ o bÃ i bÃ¡o gá»‘c: Implementing a distance-based classifier with a quantum interference circuit, náº¿u ai muá»‘n tÃ¬m hiá»ƒu sÃ¢u hÆ¡n vá» Ã½ tÆ°á»Ÿng nÃ y thÃ¬ cÃ³ thá»ƒ ghÃ© qua.\nNá»™i dung Squared-Distance Classifier Quantum Squared-Distance Classifier Káº¿t luáº­n Source Code Squared-Distance Classifier á» Ä‘Ã¢y, mÃ¬nh xÃ©t vÃ­ dá»¥ bÃ i toÃ¡n phÃ¢n loáº¡i táº­p data Titanic. Giáº£ sá»­ táº­p data Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng:\n$$ \\mathcal{D} = \\Big\\{ ({\\bf{x}}^1, y^1), \\ldots ({\\bf{x}}^M , y^M) \\Big\\}, $$ trong Ä‘Ã³ cÃ¡c vÃ©c-tÆ¡ Ä‘áº§u vÃ o 2 chiá»u: ${ {\\bf{x}}^m = ({x_0}^m, {x_1}^m)^T}, m = 1,2,â€¦,M$ tÆ°á»£ng trÆ°ng cho má»™t hÃ nh khÃ¡ch trÃªn chuyáº¿n tÃ u Titanic Ä‘Ã£ bá»‹ nháº¥n chÃ¬m vÃ o nÄƒm 1912. Trong Ä‘Ã³ $x_0$ lÃ  giÃ¡ vÃ© trong khoáº£ng tá»« 0 Ä‘áº¿n 10,000 Ä‘Ã´ la, vÃ  $x_1$ lÃ  sá»‘ hiá»‡u cabin trong khoáº£ng tá»« 1 Ä‘áº¿n 2,500. á»¨ng vá»›i má»—i má»™t vÃ©c-tÆ¡ Ä‘áº§u vÃ o lÃ  nhÃ£n $y^m = {0,1}$ tÆ°Æ¡ng á»©ng Ä‘á»ƒ chá»‰ ra hÃ nh khÃ¡ch Ä‘Ã³ Ä‘Ã£ sá»‘ng sÃ³t hay khÃ´ng.\nSouce Náº¿u tá»«ng tÃ¬m hiá»ƒu qua vá» Machine Learning, cháº¯c háº³n cÃ¡c báº¡n Ä‘Ã£ nghe hoáº·c Ä‘á»c qua vá» thuáº­t toÃ¡n nearest neighbour: vá»›i má»—i vÃ©c-tÆ¡ Ä‘áº§u vÃ o má»›i, thÃ¬ nhÃ£n cá»§a nÃ³ sáº½ Ä‘Æ°á»£c quyáº¿t Ä‘á»‹nh bá»Ÿi Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t vá»›i nÃ³. CÃ³ nhiá»u cÃ¡ch Ä‘á»ƒ xÃ¡c Ä‘á»‹nh nhá»¯ng Ä‘iá»ƒm dá»¯ liá»‡u gáº§n nháº¥t Ä‘Ã³ nhÆ°ng phá»• biáº¿n lÃ  Euclidean distance. Do váº­y, ta cÃ³ cÃ¡ch tÃ­nh há»‡ sá»‘ cho viá»‡c gÃ¡n nhÃ£n vÃ©c-tÆ¡ $\\tilde{x}$ má»›i theo nhÃ£n cá»§a $x^m$: $$ \\gamma_m = 1-\\frac{1}{c}|\\tilde{{\\bf x}}-{\\bf x}^m|^2, $$ trong Ä‘Ã³ $c$ lÃ  háº±ng sá»‘. Há»‡ sá»‘ cÃ ng cao chá»©ng tá» $\\tilde{{\\bf x}}$ cÃ ng gáº§n $x^m$. Gá»i $\\tilde{y}$ lÃ  nhÃ£n Ä‘Æ°á»£c gÃ¡n cho $\\tilde{{\\bf x}}$, ta cÃ³ xÃ¡c xuáº¥t $p_{\\tilde{{\\bf x}}}(\\tilde{y}=1)$ lÃ  tá»•ng trung bÃ¬nh há»‡ sá»‘ cá»§a $M_1$ Ä‘iá»ƒm dá»¯ liá»‡u mÃ  cÃ³ nhÃ£n lÃ  $1$: $$ p_{\\tilde{{\\bf x}}}(\\tilde{y}=1) = \\frac{1}{\\chi}\\frac{1}{M_1} \\sum_{m|y^m=1}(1-\\frac{1}{c}|\\tilde{{\\bf x}}-{\\bf x}^m|^2) $$ TÆ°Æ¡ng tá»± nhÆ° váº­y, $p_{\\tilde{{\\bf x}}}(\\tilde{y}=0)$ lÃ  tá»•ng trung bÃ¬nh há»‡ sá»‘ cá»§a cÃ¡c Ä‘iá»ƒm dá»¯ liá»‡u mÃ  cÃ³ nhÃ£n lÃ  $0$. Trong Ä‘Ã³ $\\frac{1}{\\chi}$ lÃ  normalizing factor sao cho $p_{\\tilde{{\\bf x}}}(\\tilde{y}=0)+p_{\\tilde{{\\bf x}}}(\\tilde{y}=1)=1$.\nSouce Ãp dá»¥ng phÆ°Æ¡ng phÃ¡p ta tháº¥y Passenger 3 sáº½ gáº§n vá»›i Passenger 1 hÆ¡n so vá»›i Passenger 2 (Fig 1.2), vÃ  mÃ´ hÃ¬nh sáº½ Ä‘Æ°a ra dá»± Ä‘oÃ¡n lÃ  $1$ tÆ°Æ¡ng á»©ng vá»›i survival.\nQuantum Squared-Distance Classifier Giá» hÃ£y xá»­ lÃ½ bÃ i toÃ n nÃ y báº±ng phÆ°Æ¡ng phÃ¡p â€˜quantumâ€™.\nBÆ°á»›c 1: Data preprocessing and Encoding Äáº§u tiÃªn chÃºng ta sáº½ Ä‘i tá»›i má»™t cÃ¢u há»i kinh Ä‘iá»ƒn â€œLÃ m sao cÃ³ thá»ƒ biá»ƒu diá»…n dá»¯ liá»‡u trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­?â€. Náº¿u nhÆ° trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng cÃ¡c thÃ´ng tin nhÆ° áº£nh sáº½ thÆ°á»ng Ä‘Æ°á»£c biá»ƒn diá»…n trÃªn khÃ´ng gian RBG cÃ³ giÃ¡ trá»‹ tá»« 0 Ä‘áº¿n 255, hay chÃºng ta cÃ³ Word Embedding Ä‘á»ƒ biá»ƒu thÃ´ng tin dáº¡ng vÄƒn báº£n thÃ nh cÃ¡c vÃ©c-tÆ¡, thÃ¬ váº¥n Ä‘á» cá»§a cÃ¡c thuáº­t toÃ¡n lÆ°á»£ng tá»­ cÅ©ng nhÆ° váº­y. Thá»±c cháº¥t, chá»§ Ä‘á» vá» viá»‡c mÃ£ hÃ³a thÃ´ng tin trÃªn khÃ´ng gian lÆ°á»£ng tá»­ (Quantum Embedding) váº«n Ä‘ang Ä‘Æ°á»£c cá»™ng Ä‘á»“ng nghiÃªn cá»©u quan tÃ¢m Ä‘áº·c biá»‡t trong lÄ©nh vá»±c Quantum Machine Learning. Náº¿u cÃ¡c báº¡n quan tÃ¢m Ä‘áº¿n chá»§ Ä‘á» nÃ y cÃ³ thá»ƒ xem qua paper nÃ y Ä‘á»ƒ biáº¿t rÃµ hÆ¡n vá» cÃ¡c cÃ¡ch mÃ£ thÃ´ng tin trong QML vÃ  sá»± quan trá»ng cá»§a nÃ³. MÃ¬nh sáº½ lÃ m má»™t bÃ i viáº¿t chi tiáº¿t hÆ¡n vá» chá»§ Ä‘á» nÃ y trong tÆ°Æ¡ng lai.\nQuay láº¡i vá»›i bÃ i toÃ¡n cá»§a chÃºng ta, mÃ¬nh sáº½ Ã¡p dá»¥ng má»™t phÆ°Æ¡ng phÃ¡p gá»i lÃ  Amplitude Embedding - má»™t phÆ°Æ¡ng phÃ¡p ráº¥t phá»‘ biáº¿n trong QML: Cho $X \\in \\mathbb{R}^N$ lÃ  má»™t vÃ©c-tÆ¡ Ä‘Æ¡n nháº¥t ($||X|| = 1$), ta cÃ³ thá»ƒ mÃ£ hÃ³a $X$ báº±ng $n$ qubits dÆ°á»›i dáº¡ng: $$ \\ket{\\psi_X} = \\sum_{i=0}^{N-1}x_i \\ket{i}, $$ trong Ä‘Ã³ $n = \\log{N}$. CÃ³ thá»ƒ tháº¥y phÆ°Æ¡ng phÃ¡p nÃ y chá»‰ tá»‘n $O(\\log{N})$ qubits Ä‘á»ƒ biá»ƒu diá»…n má»™t vÃ©c-tÆ¡ $N$ chiá»u. HÃ£y láº¥y vÃ­ dá»¥ trong bÃ i toÃ¡n xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, giáº£ sá»­ báº¡n cÃ³ má»™t text corpus vá»›i 10000 tá»« thÃ¬ náº¿u nhÆ° cÃ¡ch thÃ´ng thÆ°á»ng ta sá»­ dá»¥ng One-hot encoding ta sáº½ cáº§n tá»›i 10000 bits Ä‘á»ƒ mÃ£ hÃ³a, nhÆ°ng Ä‘iá»u nÃ y hoÃ n toÃ¡n cÃ³ thá»ƒ giáº£i quyáº¿t vá»›i 14 ($\\lceil \\log{10000} \\rceil$) qubits vá»›i amplitude embedding.\nTá»« Ä‘Ã¢y, vá»›i má»—i Ä‘áº§u vÃ o $\\ket{\\psi_{\\bf\\tilde{x}}}$ má»›i, bÃ i toÃ¡n sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o dÆ°á»›i dáº¡ng:\nTrong Ä‘Ã³ $\\ket{m}$ vÃ  $\\ket{y^m}$ mÃ£ hÃ³a cho sá»‘ thá»© tá»± vÃ  nhÃ£n tÆ°Æ¡ng á»©ng cá»§a vÃ©c-tÆ¡ Ä‘áº§u vÃ o thá»© $m^{th}$. Tuy nhiÃªn Ä‘iá»u chÃº Ã½ á»Ÿ Ä‘Ã¢y náº±m á»Ÿ ancilla qubit Ä‘Æ°á»£c káº¿t ná»‘i vá»›i $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ vÃ  $\\ket{\\psi_{\\bf{x}^m}}$. ChÃº Ã½ ráº±ng khi ta thá»±c hiá»‡n phÃ©p do trÃªn má»™t hoáº·c má»™t há»‡ qubits thÃ¬ qubit(s) sáº½ bá»‹ collapsed hay terminated. Váº­y khi ta chuyá»ƒn phÃ©p Ä‘o cá»§a $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ hay $\\ket{\\psi_{\\bf{x}^m}}$ sang phÃ©p Ä‘o cá»§a má»™t ancilla qubit Ä‘Æ°á»£c káº¿t ná»‘i vá»›i chÃºng thÃ¬ ta váº«n thu Ä‘Æ°á»£c káº¿t quáº£ cáº§n thiáº¿t cá»§a $\\ket{\\psi_{\\bf\\tilde{{x}}}}$ vÃ  $\\ket{\\psi_{\\bf{x}^m}}$ mÃ  khÃ´ng cáº§n cháº¥m dá»©t (terminate) cáº£ há»‡ thá»‘ng. Ká»¹ thuáº­t nÃ y ráº¥t hay sá»­ dá»¥ng á»Ÿ trong cÃ¡c thuáº­t toÃ¡n lÆ°á»£ng tá»­ vÃ  viá»‡c káº¿t ná»‘i giá»¯a ancilla qubit vá»›i há»‡ thá»‘ng lÃ  chÃºng ta Ä‘ang táº¡o ra entanglement - má»™t tÃ­nh cháº¥t quan trá»ng khÃ¡c trong lÄ©nh vá»±c tÃ­nh toÃ¡n lÆ°á»£ng tá»­.\nNhÆ° váº­y vá»›i vÃ­ dá»¥ Titanic trÃªn ta cÃ³: $$ \\ket{\\mathcal{D}} = \\frac{1}{\\sqrt{4}} \\Big\\{\\ket{0}[\\ket{0}(0.866\\ket{0}+0.5\\ket{1})+\\ket{1}(0.921\\ket{0}+0.39\\ket{1})]\\ket{1} $$ $$ + â€¦","date":1665505552,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665505552,"objectID":"41280be87b10845fb59ca0bce349f39e","permalink":"https://example.com/post/example-1/","publishdate":"2022-10-11T23:25:52+07:00","relpermalink":"/post/example-1/","section":"post","summary":"Trong bÃ i nÃ y, mÃ¬nh Ä‘i qua má»™t phÆ°Æ¡ng phÃ¡p xá»­ lÃ½ bÃ i toÃ¡n nearest neighbour báº±ng thuáº­t toÃ¡n quantum. BÃ i viáº¿t dÆ°á»›i Ä‘Ã¢y sáº½ dá»±a vÃ o bÃ i bÃ¡o gá»‘c: Implementing a distance-based classifier with a quantum interference circuit, náº¿u ai muá»‘n tÃ¬m hiá»ƒu sÃ¢u hÆ¡n vá» Ã½ tÆ°á»Ÿng nÃ y thÃ¬ cÃ³ thá»ƒ ghÃ© qua.","tags":[],"title":"BÃ i 2: Quantum Squared-Distance Classifier","type":"post"},{"authors":[],"categories":[],"content":"Ná»™i dung Giá»›i thiá»‡u bÃ i toÃ¡n Classical Interference Quantum Interference Káº¿t luáº­n TrÆ°á»›c khi Ä‘i vÃ o cÃ¡c thuáº­t toÃ¡n quantum trong há»c mÃ¡y, mÃ¬nh muá»‘n so sÃ¡nh bÃ i toÃ¡n suy luáº­n xÃ¡c suáº¥t trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng cÅ©ng nhÆ° trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­. BÃ i viáº¿t nÃ y sáº½ giÃºp cÃ¡c báº¡n viáº¿t qua nhá»¯ng thÃ nh pháº§n cÆ¡ báº£n cá»§a váº­t lÃ½ lÆ°á»£ng tá»­: qubits, unitary transformation, vÃ  measurement vÃ  cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a chÃºng thÃ´ng qua má»™t vÃ­ dá»¥ cá»¥ thá»ƒ.\nGiá»›i thiá»‡u bÃ i toÃ¡n Cho hai Ä‘á»“ng xu Ä‘á»“ng cháº¥t: $c_1$ vÃ  $c_2$ vá»›i xÃ¡c suáº¥t á»Ÿ máº·t sáº¥p (tail) hay máº·t ngá»­a (head) lÃ  nhÆ° nhau. KhÃ´ng gian máº«u cá»§a viá»‡c tung 2 Ä‘á»“ng xu trÃªn sáº½ bao gá»“m: (head, head), (head, tail), (tail, head), vÃ  (tail, tail). MÃ¬nh sáº½ xÃ©t bÃ i toÃ¡n nhÆ° sau: BÆ°á»›c 1, ta láº­t 2 Ä‘á»“ng xu thÃ nh máº·t ngá»­a (head sáº½ lÃ  giÃ¡ trá»‹ ban Ä‘áº§u cá»§a 2 Ä‘á»‘ng xu). BÆ°á»›c 2, ta tung Ä‘á»“ng xu thá»© nháº¥t $c_1$ vÃ  kiá»ƒm tra káº¿t quáº£. VÃ  bÆ°á»›c 3, ta cÅ©ng láº¡i tung Ä‘á»“ng xu $c_1$ láº§n thá»© hai vÃ  kiá»ƒm tra káº¿t quáº£ (giáº£ sá»­ káº¿t quáº£ thu Ä‘Æ°á»£c lÃ  sau sá»‘ láº§n thá»­ Ä‘á»§ lá»›n).\nClassical Interference (Suy luáº­n xÃ¡c suáº¥t truyá»n thá»‘ng) CÃ³ thá»ƒ tháº¥y vá»›i mÃ¡y tÃ­nh truyá»n thá»‘ng (classical computer), 2 Ä‘á»“ng xu cÃ³ thá»ƒ Ä‘Æ°á»£c coi lÃ  2 bits ngáº«u nhiÃªn (random bits). á» bÆ°á»›c 1, bÃ i toÃ¡n sáº½ Ä‘Æ°a vá» káº¿t quáº£ (head, head). Tuy nhiÃªn, sau bÆ°á»›c hai thÃ¬ (head, head) vÃ  (tail, head) sáº½ cÃ³ xÃ¡c suáº¥t báº±ng nhau vÃ  báº±ng 0.5. PhÃ¢n phá»‘i nÃ y sáº½ khÃ´ng thay Ä‘á»•i sau bÆ°á»›c 3.\nQuantum Interference (Suy luáº­n xÃ¡c suáº¥t trÃªn mÃ¡y tÃ­nh quantum) Tuy nhiÃªn, á»Ÿ Ä‘Ã¢y sáº½ cÃ³ má»™t chÃºt khÃ¡c biá»‡t náº¿u ta biá»ƒu diá»…n bÃ i toÃ¡n trÃªn mÃ¡y tÃ­nh quantum. Hai Ä‘á»“ng xu sáº½ Ä‘Æ°á»£c biá»ƒu diá»…n báº±ng hai qubits (quantum bits). Má»—i qubit cÃ³ dáº¡ng $\\alpha \\ket{0} + \\beta \\ket{1}$, trong Ä‘Ã³ $\\ket{0}$ vÃ  $\\ket{1}$ lÃ  hai tráº¡ng thÃ¡i cÆ¡ sá»Ÿ (basis state) Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng vÃ©c-tÆ¡ tÆ°Æ¡ng á»©ng: $[1,0]^T$ vÃ  $[0,1]^T$. CÃ³ thá»ƒ tháº¥y ráº±ng $\\ket{0}$ vÃ  $\\ket{1}$ tÆ°Æ¡ng á»©ng vá»›i hai giÃ¡ trá»‹ nhá»‹ phÃ¢n 0, 1 á»Ÿ mÃ¡y tÃ­nh truyá»n thá»‘ng; tuy nhiÃªn, thay vÃ¬ Ä‘Æ°á»£c mÃ£ hÃ³a rá»i ráº¡c thÃ nh chuá»—i bit 1 hoáº·c 0, má»—i qubit cÃ³ thá»ƒ táº¡o thÃ nh má»™t tá»• há»£p tuyáº¿n tÃ­nh cá»§a cÃ¡c tráº¡ng thÃ¡i cÆ¡ sá»Ÿ theo xÃ¡c suáº¥t: $$ p(0) = |\\alpha|^2, p(1) = |\\beta|^2; \\alpha, \\beta \\in \\mathbb{C} $$ ChÃº Ã½ ráº±ng $|\\alpha|^2 + |\\beta|^2 = 1$ Ä‘á»ƒ thá»a mÃ£n xÃ¡c suáº¥t trÃªn. NhÆ° váº­y náº¿u ta coi hai máº·t cá»§a Ä‘á»“ng xu lÃ  hai tráº¡ng thÃ¡i cÆ¡ sá»Ÿ: $\\ket{head} = \\ket{0}$ vÃ  $\\ket{tail} = \\ket{1}$, thÃ¬ viá»‡c tung Ä‘á»“ng xu sáº½ tÆ°Æ¡ng Ä‘Æ°Æ¡ng viá»‡c chÃºng ta thá»±c hiá»‡n biáº¿n Ä‘á»•i Hadamard. Viá»‡c biáº¿n Ä‘á»•i á»Ÿ Ä‘Ã¢y trÃªn mÃ¡y tÃ­nh quantum chÃ­nh lÃ  phÃ©p nhÃ¢n ma tráº­n Hadamard vá»›i tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a qubit. Trong Ä‘Ã³ biáº¿n Ä‘á»•i Hadamard cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng ma tráº­n:\n$$ H = \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{cc} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{array} \\right) $$ Tá»« Ä‘Ã³, ta cÃ³ thá»ƒ triá»ƒn khai bÃ i toÃ¡n trÃªn nhÆ° sau: 2 qubits sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o thÃ nh $\\ket{head}\\ket{head}$ sau bÆ°á»›c 1. á» bÆ°á»›c 2, ta nhÃ¢n ma tráº­n Hadamard vá»›i tráº¡ng thÃ¡i cá»§a qubit thá»© nháº¥t, ta cÃ³: $$ H\\ket{head}\\ket{head}= H\\ket{0}\\ket{head} = \\frac{1}{\\sqrt{2}}\\left( \\begin{array}{cc} 1 \u0026amp; 1 \\\\ 1 \u0026amp; -1 \\end{array} \\right) \\left( \\begin{array}{c} 1 \\\\ 0 \\end{array} \\right) \\ket{head} $$ $$ =\\!\\frac{1}{\\sqrt{2}}\\left( \\begin{array}{c} 1 \\\\ 1 \\end{array} \\right)\\! \\ket{head}\\! =\\! \\frac{1}{\\sqrt{2}} (\\ket{0}\\!+\\!\\ket{1})\\!\\ket{head}\\! =\\! \\frac{1}{\\sqrt{2}} (\\ket{head}\\!+\\!\\ket{tail})\\!\\ket{head} $$ $$ = \\frac{1}{\\sqrt{2}}\\ket{head}\\ket{head} + \\frac{1}{\\sqrt{2}}\\ket{tail}\\ket{head} $$ NhÆ° váº­y, ta tháº¥y giá»‘ng nhÆ° trÆ°á»ng há»£p trÃªn, sau bÆ°á»›c 2 xÃ¡c suáº¥t Ä‘áº¡t Ä‘Æ°á»£c $\\ket{head}\\ket{head}$ vÃ  $\\ket{tail}\\ket{head}$ lÃ  báº±ng nhau lÃ  cÅ©ng báº±ng $(\\frac{1}{\\sqrt{2}})^2 = 0.5$. Tuy nhiÃªn sá»± khÃ¡c biá»‡t náº±m á»Ÿ bÆ°á»›c 3, náº¿u ta tiáº¿p tá»¥c tung Ä‘á»“ng xu thá»© nháº¥t (hay thá»±c hiá»‡n biáº¿n Ä‘á»•i Hadamard), vá»›i má»™t chÃºt tÃ­nh toÃ¡n ta nháº­n Ä‘Æ°á»£c káº¿t quáº£:\n$$ \\frac{1}{\\sqrt{2}}H\\ket{head}\\ket{head} + \\frac{1}{\\sqrt{2}}H\\ket{tail}\\ket{head} = \\ket{head}\\ket{head} $$ Sau bÆ°á»›c 3 ta sáº½ luÃ´n nháº­n Ä‘Æ°á»£c $\\ket{head}\\ket{head}$, káº¿t quáº£ nÃ y khÃ¡c hoÃ n toÃ¡n khi thá»±c hiá»‡n bÃ i toÃ¡n trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng. CÃ³ thá»ƒ nÃ³i Ä‘Ã¢y lÃ  má»™t sá»± khÃ¡c nhau thÃº vá»‹ giá»¯a mÃ¡y tÃ­nh lÆ°á»£ng tá»­ vÃ  mÃ¡y tÃ­nh truyá»n thá»‘ng. KhÃ¡c vá»›i mÃ¡y tÃ­nh truyá»n thá»‘ng cÃ³ xu hÆ°á»›ng tá»‘i Ä‘a hÃ³a sá»± khÃ´ng cháº¯c cháº¯n (maximize uncertainty) vÃ¬ luÃ´n cho ra káº¿t quáº£ 50-50 giá»¯a 2 tráº¡ng thÃ¡i (head, head) vÃ  (tail, head), thÃ¬ mÃ¡y tÃ­nh lÆ°á»£ng tá»­ cho ra káº¿t quáº£ cÃ³ Ä‘á»™ khÃ´ng cháº¯c cháº¯n tháº¥p hÆ¡n. ChÃ­nh vÃ¬ lÃ½ do nÃ y, Ä‘Ã£ cÃ³ nghiÃªn cá»©u Ã¡p dá»¥ng quantum inference nhÆ° má»™t hÃ m dá»± Ä‘oÃ¡n (prediction function) trong bÃ i toÃ¡n há»c mÃ¡y cÃ³ giÃ¡m sÃ¡t (supervised learning) [1]\nSá»± khÃ¡c nhau trÃªn cÅ©ng dáº«n ta tá»›i má»™t váº¥n Ä‘á» quan trá»ng khÃ¡c: measurement (phÃ©p Ä‘o). PhÃ©p Ä‘o chÃ­nh lÃ  cáº§u ná»‘i giá»¯a quantum vÃ  classical, giÃºp chÃºng ta Ä‘Ã¡nh giÃ¡ vÃ  phÃ¢n tÃ­ch tráº¡ng thÃ¡i hiá»‡n táº¡i cá»§a má»™t hoáº·c má»™t há»‡ qubit, vÃ  nÃ³ thÆ°á»ng mang tÃ­nh thá»‘ng kÃª xÃ¡c suáº¥t hÆ¡n lÃ  má»™t Ä‘Ã¡nh giÃ¡ Ä‘Æ¡n láº». NÃ³i cÃ¡ch khÃ¡c, phÃ©p Ä‘o cho phÃ©p chÃºng ta phÃ¡ bá» tÃ­nh chá»‘ng chÃ¢t cá»§a qubits (phÃ¡ bá» Ä‘i tá»• há»£p tuyáº¿n tÃ­nh), tá»« Ä‘Ã³ lÃ m cho tráº¡ng thÃ¡i cá»§a qubit â€˜collapseâ€™ vá» má»™t trong cÃ¡c tráº¡ng thÃ¡i cÆ¡ sá»Ÿ. VÃ­ dá»¥, thá»±c hiá»‡n phÃ©p Ä‘o má»™t qubit báº¥t ká»³ $\\ket{\\phi} = \\alpha \\ket{0} + \\beta \\ket{1}$ trÃªn cÆ¡ sá»Ÿ chuáº©n (canonical basis) thÃ¬ ta sáº½ Ä‘áº¡t Ä‘Æ°á»£c giÃ¡ trá»‹ 0 â€¦","date":1665301286,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1665301286,"objectID":"e81b5d40081a3db7ff1a08d56238a88b","permalink":"https://example.com/post/fair-coins/","publishdate":"2022-10-09T14:41:26+07:00","relpermalink":"/post/fair-coins/","section":"post","summary":"Ná»™i dung Giá»›i thiá»‡u bÃ i toÃ¡n Classical Interference Quantum Interference Káº¿t luáº­n TrÆ°á»›c khi Ä‘i vÃ o cÃ¡c thuáº­t toÃ¡n quantum trong há»c mÃ¡y, mÃ¬nh muá»‘n so sÃ¡nh bÃ i toÃ¡n suy luáº­n xÃ¡c suáº¥t trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng cÅ©ng nhÆ° trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­.","tags":[],"title":"BÃ i 1: Quantum vs Classical Interference: First Example","type":"post"},{"authors":[],"categories":[],"content":"Ná»™i dung Quantum Machine Learning lÃ  gÃ¬? Má»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n cá»§a Quantum Machine Learning Káº¿t luáº­n KhÃ¡c vá»›i cÃ¡c chá»§ Ä‘á» cá»§a Machine Learning hay Deep Learning khi mÃ  cÃ¡c á»©ng dá»¥ng cá»§a chÃºng Ä‘ang dáº§n trá»Ÿ nÃªn phá»• biáº¿n nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, chá»§ Ä‘á» vá» Quantum Machine Learning (hay QML) lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u má»›i vÃ  Ä‘ang Ä‘Æ°á»£c chÃº Ã½ á»Ÿ cÃ¡c cÃ´ng ty hÃ ng Ä‘áº§u tháº¿ giá»›i nhÆ° IBM hay Google. Do Ä‘Ã³, á»Ÿ bÃ i viáº¿t nÃ y ngoÃ i viá»‡c cung cáº¥p cho báº¡n Ä‘á»c cÃ¡i nhÃ¬n cá»¥ thá»ƒ Quantum Machine Learning lÃ  gÃ¬, mÃ¬nh cÅ©ng sáº½ giáº£i thÃ­ch táº¡i sao chÃºng ta láº¡i cáº§n QML vÃ  má»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n cá»¥ thá»ƒ.\nQuantum Machine Learning lÃ  gÃ¬? Náº¿u nhÆ° ai Ä‘Ã£ lÃ m quen vá»›i cÃ¡c bÃ i toÃ¡n cá»§a Machine Learning hay Deep Learning, cÃ¡c mÃ´ hÃ¬nh Ä‘ang dáº§n Ä‘Æ°á»£c xÃ¢y dá»±ng lá»›n hÆ¡n vÃ  phá»©c táº¡p hÆ¡n Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c bÃ i toÃ¡n khÃ³ (hard combinatorial optimization problems), nÃ³ dáº«n tá»›i viá»‡c tiÃªu tá»‘n ráº¥t nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n (computational resources) trong viá»‡c huáº¥n luyá»‡n cÅ©ng nhÆ° lÃ  váº­n hÃ nh. Má»™t vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh lÃ  mÃ´ hÃ¬nh GPT-3 gá»“m 175 tá»· tham sá»‘ sáº½ cáº§n tá»‘n 355 nÄƒm vÃ  gáº§n 5 triá»‡u Ä‘Ã´ náº¿u train trÃªn má»™t NVIDIA Tesla V100 GPU. Do Ä‘Ã³, trÃªn thá»±c táº¿ há» Ä‘Ã£ train GPT-3 vá»›i 1024 A100 GPUs vÃ  máº¥t 34 ngÃ y.\nTuy nhiÃªn, váº¥n Ä‘á» Ä‘Ã³ cÃ³ thá»ƒ sáº½ Ä‘Æ°á»£c giáº£i quyáº¿t vá»›i sá»± xuáº¥t hiá»‡n cá»§a mÃ¡y tÃ­nh lÆ°á»£ng tá»­ (quantum computer). MÃ¡y tÃ­nh lÆ°á»£ng tá»­ Ä‘Æ°á»£c phÃ¡t triá»ƒn dá»±a theo cÃ¡c thuyáº¿t cá»§a váº­t lÃ½ lÆ°á»£ng tá»­ Ä‘á»ƒ Ä‘Æ°a ra má»™t kháº£ nÄƒng tÃ­nh toÃ¡n vÆ°á»£t trá»™i so vá»›i mÃ¡y tÃ­nh truyá»n thá»‘ng. HÃ£y láº¥y má»™t bÃ i toÃ¡n tÃ¬m kiáº¿m lÃ  má»™t vÃ­ dá»¥: giáº£ sá»­ báº¡n pháº£i tÃ¬m 1 quáº£ bÃ³ng trong 1 triá»‡u ngÄƒn kÃ©o vÃ  cÃ¢u há»i lÃ  báº¡n sáº½ pháº£i má»Ÿ qua bao nhiÃªu ngÄƒn kÃ©o trÆ°á»›c khi tÃ¬m Ä‘Æ°á»£c quáº£ bÃ³ng Ä‘Ã³? ÄÃ´i khi báº¡n sáº½ may máº¯n tÃ¬m Ä‘Æ°á»£c quáº£ bÃ³ng trong chá»‰ vÃ i láº§n thá»­ vÃ  ngÆ°á»£c láº¡i báº¡n cÅ©ng cÃ³ thá»ƒ pháº£i má»Ÿ gáº§n nhÆ° toÃ n bá»™ 1 triá»‡u ngÄƒn kÃ©o kia. Trung bÃ¬nh báº¡n sáº½ cáº§n tá»›i 500,000 lÆ°á»£t Ä‘á»ƒ tÃ¬m ra quáº£ bÃ³ng. Tuy nhiÃªn, vá»›i mÃ¡y tÃ­nh lÆ°á»£ng tá»­, báº¡n cÃ³ thá»ƒ thá»±c hiá»‡n bÃ i toÃ¡n Ä‘Ã³ trong vÃ²ng 1000 lÆ°á»£t báº±ng má»™t thuáº­t toÃ¡n Ä‘Æ°á»£c gá»i lÃ  Groverâ€™s algorithm.\nTá»« Ä‘Ã³ sá»± ra Ä‘á»i cá»§a Quantum Machine Learning nhÆ° má»™t sá»± giao thoa cá»§a cÃ¡c thuáº­t toÃ¡n trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­ vá»›i mÃ´ hÃ¬nh Machine Learning Ä‘á»ƒ cáº£i thiá»‡n cáº£ vá» máº·t tÃ­nh toÃ¡n cÅ©ng nhÆ° Ä‘á»™ chÃ­nh xÃ¡c (Ä‘Æ°á»£c gá»i lÃ  quantum advantage).\nMá»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n cá»§a Quantum Machine Learning Cho Ä‘áº¿n nÃ y Ä‘Ã£ cÃ³ khÃ¡ nhiá»u hÆ°á»›ng triá»ƒn khai QML Ä‘Æ°á»£c Ä‘á» xuáº¥t, máº·c dÃ¹ nhiá»u trong sá»‘ chÃºng váº«n chá»‰ lÃ  lÃ½ thuyáº¿t thuáº§n tÃºy vÃ  cáº§n má»™t mÃ¡y tÃ­nh lÆ°á»£ng tá»­ hoÃ n chá»‰nh Ä‘á»ƒ thá»±c nghiá»‡m; tuy nhiÃªn, cÅ©ng Ä‘Ã£ cÃ³ cÃ¡c thuáº­t toÃ¡n Ä‘Ã£ Ä‘Æ°á»£c triá»ƒn khai trÃªn quy mÃ´ nhá» vÃ  chá»©ng minh Ä‘áº¡t Ä‘Æ°á»£c â€˜quantum advantageâ€™. Sau Ä‘Ã¢y mÃ¬nh sáº½ Ä‘á» cáº­p tá»›i hai hÆ°á»›ng tiá»‡p cáº­n phá»• biáº¿n cá»§a QML.\na) QRAM-based Quantum Machine Learning\nTÆ°Æ¡ng tá»± RAM (Random Access Memory) á»Ÿ cÃ¡c mÃ¡y tÃ­nh truyá»n thá»‘ng, cÃ¡c nhÃ  nghiÃªn cá»©u Ä‘Ã£ giá»›i thiá»‡u má»™t â€˜quantum-versionâ€™ cá»§a RAM Ä‘Æ°á»£c gá»i lÃ  QRAM Ä‘á»ƒ xá»­ lÃ½ váº¥n Ä‘á» ghi vÃ  Ä‘á»c thÃ´ng tin trÃªn mÃ¡y tÃ­nh lÆ°á»£ng tá»­. CÃ³ thá»ƒ nÃ³i QRAM lÃ  má»™t pháº§n ráº¥t quan trá»ng nhiá»u thuáº­t toÃ¡n cá»§a QML. Tháº­m chÃ­ chÃºng Ä‘áº¡t Ä‘Æ°á»£c â€˜quantum advantageâ€™ lÃ  nhá» QRAM.\nMá»™t á»©ng dá»¥ng cá»¥ thá»ƒ vÃ  cÅ©ng nhÆ° Ä‘Æ°á»£c dÃ¹ng nhiá»u nháº¥t cá»§a QRAM lÃ  kháº£ nÄƒng cáº£i thiá»‡n tá»‘c Ä‘á»™ tÃ­nh toÃ¡n cá»§a tÃ­ch vÃ´ hÆ°á»›ng (dot product) hay Kernel Method - má»™t phÆ°Æ¡ng phÃ¡p quen thuá»™c cá»§a Machine Learning mÃ  Ä‘iá»ƒn hÃ¬nh lÃ  Support Vector Machine (SVM). Vá»›i sá»± can thiá»‡p cá»§a QRAM, ta cÃ³ thá»ƒ tÃ­nh tÃ­ch vÃ´ hÆ°á»›ng $x^Ty$ vá»›i Ä‘á»™ phá»©c táº¡p lÃ  $O(logN)$ so vá»›i $O(N)$ trÃªn mÃ¡y tÃ­nh truyá»n thá»‘ng, trong Ä‘Ã³ $x, y$ lÃ  cÃ¡c vectors $N$ chiá»u.\nTá»« Ä‘Ã³ cÃ¡c thuáº­t toÃ¡n Ä‘Æ°á»£c ra Ä‘á»i nhÆ° lÃ  Quantum K-Means dá»±a vÃ o QRAM Ä‘á»ƒ cÃ³ Ä‘á»™ phá»©c táº¡p $O(log(Nd))$ (so vá»›i $O(Nd)$ cá»§a thuáº­t toÃ¡n K-Means), trong Ä‘Ã³ $N$ lÃ  sá»‘ data vÃ  $d$ lÃ  sá»‘ chiá»u. Hay Quantum Support Vector Machine Ä‘áº¡t Ä‘Æ°á»£c Ä‘á»™ phá»©c táº¡p $O(log(Nd))$ so vá»›i $O(poly(N,d))$ cá»§a thuáº­t toÃ¡n SVM bÃ¬nh thÆ°á»ng, vÃ  má»™t sá»‘ khÃ¡c: Quantum PCA, Quantum K-Medians, etc.\ná» hÆ°á»›ng tiáº¿p cáº­n nÃ y, cÃ¡c thuáº­t toÃ¡n sáº½ dá»±a vÃ o kháº£ nÄƒng tÃ­nh toÃ¡n vÆ°á»£t trá»™i cá»§a quantum computing Ä‘á»ƒ cáº£i thiá»‡n Ä‘á»™ phá»©c táº¡p. Tuy nhiÃªn á»Ÿ mÃ¡y tÃ­nh lÆ°á»£ng tá»­ khÃ´ng chá»‰ cÃ³ váº­y. ThÃ´ng tin á»Ÿ Ä‘Ã³ Ä‘Æ°á»£c biá»ƒu diá»…n dá»±a theo nguyÃªn lÃ½ chá»“ng cháº­p (Superposition), thay vÃ¬ Ä‘Æ°á»£c mÃ£ hÃ³a rá»i ráº¡c thÃ nh cÃ¡c bits 0 vÃ  1, cÃ³ nghÄ©a thÃ´ng tin cÃ³ thá»ƒ tá»“n táº¡i Ä‘á»“ng thá»i á»Ÿ bit 0 vÃ  bit 1 theo má»™t phÃ¢n phá»‘i nÃ o Ä‘Ã³. Do Ä‘Ã³, â€™learning spaceâ€™ á»Ÿ mÃ¡y tÃ­nh lÆ°á»£ng tá»­ sáº½ hoÃ n toÃ n khÃ¡c vÃ  tháº­m chÃ­ Ä‘Æ°á»£c má»Ÿ rá»™ng hÆ¡n so vá»›i mÃ¡y tÃ­nh truyá»n thá»‘ng. Thá»±c táº¿ Ä‘Ã£ cÃ³ nhiá»u nghiÃªn cá»©u vá»›i má»¥c tiÃªu khÃ¡m phÃ¡ khÃ´ng gian nÃ y Ä‘á»ƒ cáº£i thiá»‡n kháº£ nÄƒng há»c táº­p (learning capability) cá»§a mÃ´ hÃ¬nh Machine Learning vÃ  hÆ°á»›ng tiáº¿p cáº­n sau Ä‘Ã¢y lÃ  vÃ­ dá»¥ Ä‘iá»ƒn hÃ¬nh cho viá»‡c nÃ y.\nb) Quantum Neural Network.\nÄÆ°á»£c thÃºc Ä‘áº©y tá»« sá»± thÃ nh cÃ´ng cá»§a máº¡ng há»c sÃ¢u (classical deep learning), máº¡ng nÆ¡-ron lÆ°á»£ng tá»­ (Quantum Neural Network, hay QNN) cÅ©ng mang nhá»¯ng nÃ©t tÆ°Æ¡ng Ä‘á»“ng vá»›i máº¡ng nÆ¡-ron truyá»n thá»‘ng (NN). ChÃºng Ä‘Æ°á»£c thiáº¿t káº¿ theo cáº¥u trÃºc feed-forward, trong Ä‘Ã³ cÃ¡c layers lÃ  cÃ¡c phÃ©p biáº¿n Ä‘á»•i Ä‘Æ¡n nháº¥t (unitary transformation). Háº§u háº¿t cáº¥u trÃºc cá»§a QNN dá»±a theo Variational Quantum Circuits hay thÆ°á»ng Ä‘Æ°á»£c gá»i Parameterised Quantum Circuits. á» Ä‘Ã³ cÃ¡c biáº¿n Ä‘á»•i trong cáº¥u trÃºc máº¡ng QNN sáº½ phá»¥ thuá»™c vÃ o tham sá»‘ $\\theta$ (learning parameters) vÃ  chÃºng â€¦","date":1664552270,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664552270,"objectID":"db5dfd9db2c98204b3c7e2f0bf0ffcb8","permalink":"https://example.com/post/why-qml/","publishdate":"2022-09-30T22:37:50+07:00","relpermalink":"/post/why-qml/","section":"post","summary":"Ná»™i dung Quantum Machine Learning lÃ  gÃ¬? Má»™t vÃ i hÆ°á»›ng tiáº¿p cáº­n cá»§a Quantum Machine Learning Káº¿t luáº­n KhÃ¡c vá»›i cÃ¡c chá»§ Ä‘á» cá»§a Machine Learning hay Deep Learning khi mÃ  cÃ¡c á»©ng dá»¥ng cá»§a chÃºng Ä‘ang dáº§n trá»Ÿ nÃªn phá»• biáº¿n nhá»¯ng nÄƒm gáº§n Ä‘Ã¢y, chá»§ Ä‘á» vá» Quantum Machine Learning (hay QML) lÃ  má»™t lÄ©nh vá»±c nghiÃªn cá»©u má»›i vÃ  Ä‘ang Ä‘Æ°á»£c chÃº Ã½ á»Ÿ cÃ¡c cÃ´ng ty hÃ ng Ä‘áº§u tháº¿ giá»›i nhÆ° IBM hay Google.","tags":[],"title":"BÃ i 0: Giá»›i thiá»‡u vá» Quantum Machine Learning","type":"post"},{"authors":null,"categories":null,"content":"ChÃ o má»i ngÆ°á»i, cáº£m Æ¡n má»i ngÆ°á»i ghÃ© qua trang blog cá»§a mÃ¬nh.\nMÃ¬nh lÃ  Nguyá»…n Quang Tuyáº¿n, mÃ¬nh tá»‘t nghiá»‡p Ä‘áº¡i há»c nghÃ nh Khoa há»c mÃ¡y tÃ­nh táº¡i trÆ°á»ng University of Aizu, Nháº­t Báº£n. Trong thá»i gian lÃ m Ä‘á»“ Ã¡n tá»‘t nghiá»‡p liÃªn quan Ä‘áº¿n Quantum Machine Learning (mÃ¬nh táº¡m dá»‹ch lÃ  Há»c MÃ¡y LÆ°á»£ng Tá»­), mÃ¬nh Ä‘Ã£ cÃ³ mong muá»‘n lÃ m ra má»™t blog cÃ¡ nhÃ¢n viáº¿t vá» lÄ©nh vá»±c nÃ y. Do Ä‘Ã³, dá»±a theo hai nguá»“n cáº£m há»©ng tá»« trang Machine Learning cÆ¡ báº£n cá»§a anh VÅ© Há»¯u Tiá»‡p vÃ  Deep Learning cÆ¡ báº£n cá»§a anh Tuáº¥n, mÃ¬nh táº¡o trang nÃ y vá»›i hai ká»³ vá»ng. Má»™t lÃ  giÃºp mÃ¬nh tá»•ng há»£p kiáº¿n thá»©c vá» Quantum Machine Learning cÅ©ng giÃºp mÃ¬nh náº¯m cháº¯c ná»n mÃ³ng trong giai Ä‘oáº¡n sÆ¡ khai cá»§a lÄ©nh vá»±c nÃ y. Hai lÃ  chia sáº» cÅ©ng nhÆ° mong muá»‘n táº¡o Ä‘Æ°á»£c má»™t cá»™ng Ä‘á»“ng cÃ¡c báº¡n Ä‘á»c Viá»‡t Nam tiáº¿p cáº­n tá»›i má»™t lÄ©nh vá»±c má»›i nhÆ°ng cÅ©ng Ä‘áº§y triá»ƒn vá»ng nÃ y.\nTrong quÃ¡ trÃ¬nh chuáº©n bá»‹ cÅ©ng nhÆ° lÃ  viáº¿t bÃ i, mÃ¬nh sáº½ cá»‘ gáº¯ng Ã­t Ä‘á»™ng cháº¡m tá»›i cÃ¡c hiá»‡n tÆ°á»£ng vÄ© mÃ´ á»Ÿ trong tháº¿ giá»›i lÆ°á»£ng tá»­ vÃ  mÃ¬nh sáº½ Ä‘i tá»« gÃ³c nhÃ¬n cá»§a má»™t computer scientist chá»© khÃ´ng pháº£i lÃ  má»™t physicist. NhÆ°ng mÃ¬nh hi vá»ng má»i ngÆ°á»i Ä‘Ã£ lÃ m quen qua cÃ¡c kiáº¿n thá»©c cÆ¡ báº£n cá»§a Äáº¡i Sá»‘ Tuyáº¿n TÃ­nh (vÃ¬ háº§u háº¿t cÃ¡c biáº¿n Ä‘á»•i trong mÃ¡y tÃ­nh lÆ°á»£ng tá»­ lÃ  tuyáº¿n tÃ­nh) vÃ  ká»¹ nÄƒng láº­p trÃ¬nh Python (náº¿u cÃ³ thá»ƒ mÃ¬nh sáº½ cá»‘ Ä‘Æ°a ra cÃ¡c code demo sau cÃ¡c bÃ i viáº¿t Ä‘á»ƒ giÃºp má»i ngÆ°á»i hiá»ƒu rÃµ Ä‘Æ°á»£c váº¥n Ä‘á» hÆ¡n).\nNgoÃ i cÃ¡c bÃ i viáº¿t á»Ÿ trÃªn Blogs, Newsletter lÃ  nÆ¡i mÃ¬nh cáº­p nháº­t nhá»¯ng thÃ´ng tin má»›i nháº¥t hÃ ng thÃ¡ng liÃªn quan tá»›i Quantum Machine Learning (cÃ¡c bÃ i bÃ¡o má»›i, há»™i nghá»‹, workshop, etc.)\nMÃ¬nh vui lÃ²ng tiáº¿p nháº­n má»i Ã½ kiáº¿n tháº£o luáº­n cá»§a má»i ngÆ°á»i qua email quantummachinelearning.vietnam@gmail.com\nMÃ¬nh xin chÃ¢n thÃ nh cáº£m Æ¡n.\n","date":1664496000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1664496000,"objectID":"ed769a13ba9f25479e7927567f68c77f","permalink":"https://example.com/authors/intro/","publishdate":"2022-09-30T00:00:00Z","relpermalink":"/authors/intro/","section":"authors","summary":"Here we describe how to add a page to your site.","tags":null,"title":"About","type":"authors"}]