<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Newsletter | QML Vietnam</title>
    <link>https://example.com/newsletter/</link>
      <atom:link href="https://example.com/newsletter/index.xml" rel="self" type="application/rss+xml" />
    <description>Newsletter</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 06 Jan 2023 20:09:00 +0700</lastBuildDate>
    <image>
      <url>https://example.com/media/icon_hu03ccfb93536ffaa14d5c51dca71785c3_35563_512x512_fill_lanczos_center_3.png</url>
      <title>Newsletter</title>
      <link>https://example.com/newsletter/</link>
    </image>
    
    <item>
      <title>December 2022</title>
      <link>https://example.com/newsletter/december-2022/</link>
      <pubDate>Fri, 06 Jan 2023 20:09:00 +0700</pubDate>
      <guid>https://example.com/newsletter/december-2022/</guid>
      <description>&lt;h1 id=&#34;news-&#34;&gt;News üì∞&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://thequantuminsider.com/2022/12/30/hybrid-quantum-classical-algorithm-shows-promise-for-unraveling-the-protein-folding-problem/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hybrid Quantum-Classical Algorithm Shows Promise for Unraveling the Protein Folding Problem&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://thequantuminsider.com/2022/12/01/qubit-pharmaceuticals-works-with-nvidia-to-create-hybrid-computing-platform-to-accelerate-drug-discovery-with-nvidia/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qubit Pharmaceuticals Works With NVIDIA to Create Hybrid Computing Platform to Accelerate Drug Discovery with NVIDIA&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://thequantuminsider.com/2022/12/07/ionq-and-hyundai-motors-expand-quantum-computing-partnership-continuing-pursuit-of-automotive-innovation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IonQ and Hyundai Motors Expand Quantum Computing Partnership, Continuing Pursuit of Automotive Innovation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;videos-&#34;&gt;Videos üìΩÔ∏è&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href=&#34;https://www.youtube.com/playlist?list=PLnK6MrIqGXsJfcBdppW3CKJ858zR8P4eP&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Introduction to Quantum Computing: From Layperson to Programmer in 30 Steps&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;publications-&#34;&gt;Publications üìÉ&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.01851&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum Machine Learning: from physics to software engineering&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Quantum machine learning (QML) is a new, rapidly growing, and fascinating area of research where quantum information science and quantum technologies meet novel machine learning and artificial intelligent facilities. A comprehensive analysis of the main directions of current QML methods and approaches is performed in this review. The aim of our work is twofold. First, we show how classical machine learning approach can help improve the facilities of quantum computers and simulators available today. It is most important due to the modern noisy intermediate-scale quantum (NISQ) era of quantum technologies. In particular, the classical machine learning approach allows optimizing quantum hardware for achieving desired quantum states by implementing quantum devices. Second, we discuss how quantum algorithms and quantum computers may be useful for solving keystone classical machine learning tasks. Currently, quantum-inspired algorithms, which use a quantum approach to classical information processing, represent a powerful tool in software engineering for improving classical computation capacities. In this work, we discuss various quantum neural network capabilities that can be implemented in quantum-classical training algorithms for variational circuits. It is expected that quantum computers will be involved in routine machine learning procedures. In this sense, we are showing how it is essential to elucidate the speedup problem for random walks on arbitrary graphs, which are used in both classical and quantum algorithms. Quantum technologies enhanced by machine learning in fundamental and applied quantum physics, as well as quantum tomography and photonic quantum computing, are also covered.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2301.01597&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Demystify Problem-Dependent Power of Quantum Neural Networks on Multi-Class Classification&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Quantum neural networks (QNNs) have become an important tool for understanding the physical world, but their advantages and limitations are not fully understood. Some QNNs with specific encoding methods can be efficiently simulated by classical surrogates, while others with quantum memory may perform better than classical classifiers. Here we systematically investigate the problem-dependent power of quantum neural classifiers (QCs) on multi-class classification tasks. Through the analysis of expected risk, a measure that weighs the training loss and the generalization error of a classifier jointly, we identify two key findings: first, the training loss dominates the power rather than the generalization ability; second, QCs undergo a U-shaped risk curve, in contrast to the double-descent risk curve of deep neural classifiers. We also reveal the intrinsic connection between optimal QCs and the Helstrom bound and the equiangular tight frame. Using these findings, we propose a method that uses loss dynamics to probe whether a QC may be more effective than a classical classifier on a particular learning task. Numerical results demonstrate the effectiveness of our approach to explain the superiority of QCs over multilayer Perceptron on parity datasets and their limitations over convolutional neural networks on image datasets. Our work sheds light on the problem-dependent power of QNNs and offers a practical tool for evaluating their potential merit.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.14810&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;On Machine Learning Knowledge Representation In The Form Of Partially Unitary Operator. Knowledge Generalizing Operator&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;A new form of ML knowledge representation with high generalization power is developed and implemented numerically. Initial IN attributes and OUT class label are transformed into the corresponding Hilbert spaces by considering localized wavefunctions. A partially unitary operator optimally converting a state from IN Hilbert space into OUT Hilbert space is then built from an optimization problem of transferring maximal possible probability from IN to OUT, this leads to the formulation of a new algebraic problem. Constructed Knowledge Generalizing Operator U can be considered as a IN to OUT quantum channel; it is a partially unitary rectangular matrix of the dimension dim(OUT)√ódim(IN) transforming operators as AOUT=UAINU‚Ä†. Whereas only operator U projections squared are observable ‚ü®OUT|U|IN‚ü©2 (probabilities), the fundamental equation is formulated for the operator U itself. This is the reason of high generalizing power of the approach; the situation is the same as for the Schr√∂dinger equation: we can only measure œà2, but the equation is written for œà itself.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2212.14807&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Improving Convergence for Quantum Variational Classifiers using Weight Re-Mapping&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Constrained combinatorial optimization problems abound in industry, from portfolio optimization to logistics. One of the major roadblocks in solving these problems is the presence of non-trivial hard constraints which limit the valid search space. In some heuristic solvers, these are typically addressed by introducing certain Lagrange multipliers in the cost function, by relaxing them in some way, or worse yet, by generating many samples and only keeping valid ones, which leads to very expensive and inefficient searches. In this work, we encode arbitrary integer-valued equality constraints of the form Ax=b, directly into U(1) symmetric tensor networks (TNs) and leverage their applicability as quantum-inspired generative models to assist in the search of solutions to combinatorial optimization problems. This allows us to exploit the generalization capabilities of TN generative models while constraining them so that they only output valid samples. Our constrained TN generative model efficiently captures the constraints by reducing number of parameters and computational costs. We find that at tasks with constraints given by arbitrary equalities, symmetric Matrix Product States outperform their standard unconstrained counterparts at finding novel and better solutions to combinatorial optimization problems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pubs.rsc.org/en/content/articlepdf/2022/cs/d2cs00203e?fbclid=IwAR2eE5dU82cOhsJbreGrpkdqXokbg0UpBSEFVt9q7fCloipeljnxb91UMQQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum machine learning for chemistry and physics&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;In recent years, quantum machine learning has seen a substantial increase in the use of variational quantum circuits (VQCs). VQCs are inspired by artificial neural networks, which achieve extraordinary performance in a wide range of AI tasks as massively parameterized function approximators. VQCs have already demonstrated promising results, for example, in generalization and the requirement for fewer parameters to train, by utilizing the more robust algorithmic toolbox available in quantum computing. A VQCs&amp;rsquo; trainable parameters or weights are usually used as angles in rotational gates and current gradient-based training methods do not account for that. We introduce weight re-mapping for VQCs, to unambiguously map the weights to an interval of length 2œÄ, drawing inspiration from traditional ML, where data rescaling, or normalization techniques have demonstrated tremendous benefits in many circumstances. We employ a set of five functions and evaluate them on the Iris and Wine datasets using variational classifiers as an example. Our experiments show that weight re-mapping can improve convergence in all tested settings. Additionally, we were able to demonstrate that weight re-mapping increased test accuracy for the Wine dataset by 10% over using unmodified weights.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>November 2022</title>
      <link>https://example.com/newsletter/november-2022/</link>
      <pubDate>Sat, 10 Dec 2022 19:53:10 +0700</pubDate>
      <guid>https://example.com/newsletter/november-2022/</guid>
      <description>&lt;h1 id=&#34;news-&#34;&gt;News üì∞&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://thequantuminsider.com/2022/11/14/d-wave-says-revenue-up-added-more-commercial-customers-in-q3/?fbclid=IwAR2pnBlITY1DxKAgtEpiceJFW5LJXH6uUrWY30w7UqGgc_iiH-Q5EOx9SVY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;D-Wave Says Revenue Up, Added More Commercial Customers in Q3&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://thequantuminsider.com/2022/12/07/ionq-and-hyundai-motors-expand-quantum-computing-partnership-continuing-pursuit-of-automotive-innovation/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;IonQ and Hyundai Motors Expand Quantum Computing Partnership, Continuing Pursuit of Automotive Innovation&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://thequantuminsider.com/2022/11/28/qnlp-at-the-beeb-quantinuum-bbc-join-consortium-to-explore-real-world-uses-for-quantum-natural-language-processing/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;QNLP at The Beeb ‚Äî Quantinuum, BBC Join Consortium to Explore Real-World Uses for Quantum Natural Language Processing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://thequantuminsider.com/2022/11/21/unilevers-head-of-rd-alberto-prado-talks-quantum-at-iot-world-the-ai-summit-austin/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Unilever‚Äôs Head of R&amp;amp;D Alberto Prado Talks Quantum at IoT World &amp;amp; The AI Summit Austin&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;hackathon-and-internship-&#34;&gt;Hackathon and Internship üéì&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://app.smarterselect.com/programs/85950-Los-Alamos-National-Laboratory?fbclid=IwAR1CBVL0qG50vll6OPTyWwfzPdVjJtnqVL2_IU5hOMz7nLLyChXxoTRoksQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023 Quantum Computing Summer School Fellowship provided by Los Alamos National Laboratory&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.zapatacomputing.com/interns/?fbclid=IwAR0X8Gs_6e4sE_AfX8HJ6ry507b5FkzK3Zxq8wMvZsaz3ZQXgjzrA4flyqo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023 Zapata Quantum Computing Internship&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://research.ibm.com/blog/2023-quantum-internships?fbclid=IwAR1z2GdQhqcXCVtfZI71ZfMby3W1o8-hXU7-Fz5K1n8BxQlxM-e6ge4nJcs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;2023 IBM Quantum Summer Internship&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://chicagoquantum.org/education-and-training/internships?fbclid=IwAR3NjGc_ppUCMQUXeEEKcEXmco2fFtC4imqEs6uJvMKlCiESYVPLHVoAsWU&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Chicago Quantum Exchange Intership&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;publications-&#34;&gt;Publications üìÉ&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.02612?fbclid=IwAR0_N0EU9wBoo4g8JTg-IqF_3JGQy81NXN7ndtTak-cK2ENs_m0NCupsU5Q&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Reservoir Computing via Quantum Recurrent Neural Networks&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Recent developments in quantum computing and machine learning have propelled the interdisciplinary study of quantum machine learning. Sequential modeling is an important task with high scientific and commercial value. Existing VQC or QNN-based methods require significant computational resources to perform the gradient-based optimization of a larger number of quantum circuit parameters. The major drawback is that such quantum gradient calculation requires a large amount of circuit evaluation, posing challenges in current near-term quantum hardware and simulation software. In this work, we approach sequential modeling by applying a reservoir computing (RC) framework to quantum recurrent neural networks (QRNN-RC) that are based on classical RNN, LSTM and GRU. The main idea to this RC approach is that the QRNN with randomly initialized weights is treated as a dynamical system and only the final classical linear layer is trained. Our numerical simulations show that the QRNN-RC can reach results comparable to fully trained QRNN models for several function approximation and time series prediction tasks. Since the QRNN training complexity is significantly reduced, the proposed model trains notably faster. In this work we also compare to corresponding classical RNN-based RC implementations and show that the quantum version learns faster by requiring fewer training epochs in most cases. Our results demonstrate a new possibility to utilize quantum neural network for sequential modeling with greater quantum hardware efficiency, an important design consideration for noisy intermediate-scale quantum (NISQ) computers.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.12681?fbclid=IwAR0a9WtjG-YpnlU-iEaOUD5Z1JWM2vcgbbtu7qrAljz0Ob2J8fRksny0XF0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Benchmarking Adversarially Robust Quantum Machine Learning at Scale&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Machine learning (ML) methods such as artificial neural networks are rapidly becoming ubiquitous in modern science, technology and industry. Despite their accuracy and sophistication, neural networks can be easily fooled by carefully designed malicious inputs known as adversarial attacks. While such vulnerabilities remain a serious challenge for classical neural networks, the extent of their existence is not fully understood in the quantum ML setting. In this work, we benchmark the robustness of quantum ML networks, such as quantum variational classifiers (QVC), at scale by performing rigorous training for both simple and complex image datasets and through a variety of high-end adversarial attacks. Our results show that QVCs offer a notably enhanced robustness against classical adversarial attacks by learning features which are not detected by the classical neural networks, indicating a possible quantum advantage for ML tasks. Contrarily, and remarkably, the converse is not true, with attacks on quantum networks also capable of deceiving classical neural networks. By combining quantum and classical network outcomes, we propose a novel adversarial attack detection technology. Traditionally quantum advantage in ML systems has been sought through increased accuracy or algorithmic speed-up, but our work has revealed the potential for a new kind of quantum advantage through superior robustness of ML models, whose practical realisation will address serious security concerns and reliability issues of ML algorithms employed in a myriad of applications including autonomous vehicles, cybersecurity, and surveillance robotic systems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.16337?fbclid=IwAR39YeVFgOxIbHM2w4zSZDSq49JPQkpb7LepS7r8hsvqyrNyuehI5ilSSD4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum Feature Maps for Graph Machine Learning on a Neutral Atom Quantum Processor&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Using a quantum processor to embed and process classical data enables the generation of correlations between variables that are inefficient to represent through classical computation. A fundamental question is whether these correlations could be harnessed to enhance learning performances on real datasets. Here, we report the use of a neutral atom quantum processor comprising up to 32 qubits to implement machine learning tasks on graph-structured data. To that end, we introduce a quantum feature map to encode the information about graphs in the parameters of a tunable Hamiltonian acting on an array of qubits. Using this tool, we first show that interactions in the quantum system can be used to distinguish non-isomorphic graphs that are locally equivalent. We then realize a toxicity screening experiment, consisting of a binary classification protocol on a biochemistry dataset comprising 286 molecules of sizes ranging from 2 to 32 nodes, and obtain results which are comparable to those using the best classical kernels. Using techniques to compare the geometry of the feature spaces associated with kernel methods, we then show evidence that the quantum feature map perceives data in an original way, which is hard to replicate using classical kernels.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2211.09121?fbclid=IwAR11axzJZbhtsl311J4b_ek3vx4XflJxBrXClDDbLCl6fsjCWdwSxXuvoqo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Symmetric Tensor Networks for Generative Modeling and Constrained Combinatorial Optimization&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Constrained combinatorial optimization problems abound in industry, from portfolio optimization to logistics. One of the major roadblocks in solving these problems is the presence of non-trivial hard constraints which limit the valid search space. In some heuristic solvers, these are typically addressed by introducing certain Lagrange multipliers in the cost function, by relaxing them in some way, or worse yet, by generating many samples and only keeping valid ones, which leads to very expensive and inefficient searches. In this work, we encode arbitrary integer-valued equality constraints of the form Ax=b, directly into U(1) symmetric tensor networks (TNs) and leverage their applicability as quantum-inspired generative models to assist in the search of solutions to combinatorial optimization problems. This allows us to exploit the generalization capabilities of TN generative models while constraining them so that they only output valid samples. Our constrained TN generative model efficiently captures the constraints by reducing number of parameters and computational costs. We find that at tasks with constraints given by arbitrary equalities, symmetric Matrix Product States outperform their standard unconstrained counterparts at finding novel and better solutions to combinatorial optimization problems.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://pubs.rsc.org/en/content/articlepdf/2022/cs/d2cs00203e?fbclid=IwAR2eE5dU82cOhsJbreGrpkdqXokbg0UpBSEFVt9q7fCloipeljnxb91UMQQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum machine learning for chemistry and physics&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Machine learning (ML) has emerged as a formidable force for identifying hidden but pertinent patterns within a given data set with the objective of subsequent generation of automated predictive behavior. In recent years, it is safe to conclude that ML and its close cousin, deep learning (DL), have ushered in unprecedented developments in all areas of physical sciences, especially chemistry. Not only classical variants of ML, even those trainable on near-term quantum hardwares have been developed with promising outcomes. Such algorithms have revolutionized materials design and performance of photovoltaics, electronic structure calculations of ground and excited states of correlated matter, computation of force-fields and potential energy surfaces informing chemical reaction dynamics, reactivity inspired rational strategies of drug designing and even classification of phases of matter with accurate identification of emergent criticality. In this review we shall explicate a subset of such topics and delineate the contributions made by both classical and quantum computing enhanced machine learning algorithms over the past few years. We shall not only present a brief overview of the well-known techniques but also highlight their learning strategies using statistical physical insight. The objective of the review is not only to foster exposition of the aforesaid techniques but also to empower and promote cross-pollination among future research in all areas of chemistry which can benefit from ML and in turn can potentially accelerate the growth of such algorithms.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>October 2022</title>
      <link>https://example.com/newsletter/october-2022/</link>
      <pubDate>Sat, 29 Oct 2022 14:54:53 +0700</pubDate>
      <guid>https://example.com/newsletter/october-2022/</guid>
      <description>&lt;h1 id=&#34;news-&#34;&gt;News üì∞&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.nature.com/articles/s41598-022-21607-4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Determinable and interpretable network representation for link prediction&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://cloudblogs.microsoft.com/quantum/2022/10/20/azure-quantum-credits-program-propels-quantum-innovation-and-exploration-for-researchers-educators-and-students/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Azure Quantum Credits Program propels quantum innovation and exploration for researchers, educators, and students&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.eurekalert.org/news-releases/969327&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Penn State researchers to explore using quantum computers to design new drugs&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.einnews.com/pr_news/597582546/multiverse-computing-and-mila-join-forces-to-advance-artificial-intelligence-with-quantum-computing&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Multiverse Computing and Mila Join Forces to Advance Artificial Intelligence with Quantum Computing&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.tribuneindia.com/news/brand-connect/quantum-ai-elon-musk-review-scam-app-or-legit-444921&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum AI Elon Musk Review / Scam App Or Legit?&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;videos-&#34;&gt;Videos üìΩÔ∏è&lt;/h1&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=NqHKr9CGWJ0&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum Machine Learning Explained&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=gyU5tIppmIk&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Qiskit Falll Fest CIC-IPN Mexico 2022- Quantum Machine Learning&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=zqxCnDcGSgQ&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum Machine Learning Neuroimaging for Alzheimer‚Äôs Disease&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=6Mo0EQ7bOpg&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;MLBBQ: Quantum Machine Learning by Pavel Popov&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=x6oLsJtxWzs&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Quantum Machine Learning: Opportunities and Challenges&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=2rJFsxcM0N4&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Hands-on quantum machine learning | Rodrigo Morales | ADC2022&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;&lt;a href=&#34;https://www.youtube.com/watch?v=CDCiQdtOhLY&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;‚ÄúReinforcement Learning for Quantum Technologies,‚Äù presented by Florian Marquardt&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;h1 id=&#34;publications-&#34;&gt;Publications üìÉ&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.07980&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Representation Theory for Geometric Quantum Machine Learning&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-0&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Recent advances in classical machine learning have shown that creating models with inductive biases encoding the symmetries of a problem can greatly improve performance. Importation of these ideas, combined with an existing rich body of work at the nexus of quantum theory and symmetry, has given rise to the field of Geometric Quantum Machine Learning (GQML). Following the success of its classical counterpart, it is reasonable to expect that GQML will play a crucial role in developing problem-specific and quantum-aware models capable of achieving a computational advantage. Despite the simplicity of the main idea of GQML &amp;ndash; create architectures respecting the symmetries of the data &amp;ndash; its practical implementation requires a significant amount of knowledge of group representation theory. We present an introduction to representation theory tools from the optics of quantum learning, driven by key examples involving discrete and continuous groups. These examples are sewn together by an exposition outlining the formal capture of GQML symmetries via &amp;ldquo;label invariance under the action of a group representation&amp;rdquo;, a brief (but rigorous) tour through finite and compact Lie group representation theory, a reexamination of ubiquitous tools like Haar integration and twirling, and an overview of some successful strategies for detecting symmetries.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.08566&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Theory for Equivariant Quantum Neural Networks&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-1&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Most currently used quantum neural network architectures have little-to-no inductive biases, leading to trainability and generalization issues. Inspired by a similar problem, recent breakthroughs in classical machine learning address this crux by creating models encoding the symmetries of the learning task. This is materialized through the usage of equivariant neural networks whose action commutes with that of the symmetry. In this work, we import these ideas to the quantum realm by presenting a general theoretical framework to understand, classify, design and implement equivariant quantum neural networks. As a special implementation, we show how standard quantum convolutional neural networks (QCNN) can be generalized to group-equivariant QCNNs where both the convolutional and pooling layers are equivariant under the relevant symmetry group. Our framework can be readily applied to virtually all areas of quantum machine learning, and provides hope to alleviate central challenges such as barren plateaus, poor local minima, and sample complexity.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.09974&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Theoretical Guarantees for Permutation-Equivariant Quantum Neural Networks&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-2&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Despite the great promise of quantum machine learning models, there are several challenges one must overcome before unlocking their full potential. For instance, models based on quantum neural networks (QNNs) can suffer from excessive local minima and barren plateaus in their training landscapes. Recently, the nascent field of geometric quantum machine learning (GQML) has emerged as a potential solution to some of those issues. The key insight of GQML is that one should design architectures, such as equivariant QNNs, encoding the symmetries of the problem at hand. Here, we focus on problems with permutation symmetry (i.e., the group of symmetry Sn), and show how to build Sn-equivariant QNNs. We provide an analytical study of their performance, proving that they do not suffer from barren plateaus, quickly reach overparametrization, and can generalize well from small amounts of data. To verify our results, we perform numerical simulations for a graph state classification task. Our work provides the first theoretical guarantees for equivariant QNNs, thus indicating the extreme power and potential of GQML.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/pdf/2210.13442.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Protocols for classically training quantum generative models on probability distributions&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-3&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Quantum Generative Modelling (QGM) relies on preparing quantum states and generating samples from these states as hidden - or known - probability distributions. As distributions from some classes of quantum states (circuits) are inherently hard to sample classically, QGM represents an excellent testbed for quantum supremacy experiments. Furthermore, generative tasks are increasingly relevant for industrial machine learning applications, and thus QGM is a strong candidate for demonstrating a practical quantum advantage. However, this requires that quantum circuits are trained to represent industrially relevant distributions, and the corresponding training stage has an extensive training cost for current quantum hardware in practice. In this work, we propose protocols for classical training of QGMs based on circuits of the specific type that admit an efficient gradient computation, while remaining hard to sample. In particular, we consider Instantaneous Quantum Polynomial (IQP) circuits and their extensions. Showing their classical simulability in terms of the time complexity, sparsity and anti-concentration properties, we develop a classically tractable way of simulating their output probability distributions, allowing classical training to a target probability distribution. The corresponding quantum sampling from IQPs can be performed efficiently, unlike when using classical sampling. We numerically demonstrate the end-to-end training of IQP circuits using probability distributions for up to 30 qubits on a regular desktop computer. When applied to industrially relevant distributions this combination of classical training with quantum sampling represents an avenue for reaching advantage in the NISQ era.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2210.11850&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Universal algorithms for quantum data learning&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-4&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Operating quantum sensors and quantum computers would make data in the form of quantum states available for purely quantum processing, opening new avenues for studying physical processes and certifying quantum technologies. In this Perspective, we review a line of works dealing with measurements that reveal structural properties of quantum datasets given in the form of product states. These algorithms are universal, meaning that their performances do not depend on the reference frame in which the dataset is provided. Requiring the universality property implies a characterization of optimal measurements via group representation theory.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.05523&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Generalization despite overfitting in quantum machine learning models&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-5&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;The widespread success of deep neural networks has revealed a surprise in classical machine learning: very complex models often generalize well while simultaneously overfitting training data. This phenomenon of benign overfitting has been studied for a variety of classical models with the goal of better understanding the mechanisms behind deep learning. Characterizing the phenomenon in the context of quantum machine learning might similarly improve our understanding of the relationship between overfitting, overparameterization, and generalization. In this work, we provide a characterization of benign overfitting in quantum models. To do this, we derive the behavior of a classical interpolating Fourier features models for regression on noisy signals, and show how a class of quantum models exhibits analogous features, thereby linking the structure of quantum circuits (such as data-encoding and state preparation operations) to overparameterization and overfitting in quantum models. We intuitively explain these features according to the ability of the quantum model to interpolate noisy data with locally &amp;ldquo;spiky&amp;rdquo; behavior and provide a concrete demonstration example of benign overfitting.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://arxiv.org/abs/2209.14353&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Interpretable Quantum Advantage in Neural Sequence Learning&lt;/a&gt;
&lt;details class=&#34;spoiler &#34;  id=&#34;spoiler-6&#34;&gt;
  &lt;summary&gt;Abstract:&lt;/summary&gt;
  &lt;p&gt;&lt;blockquote&gt;
&lt;p&gt;Quantum neural networks have been widely studied in recent years, given their potential practical utility and recent results regarding their ability to efficiently express certain classical data. However, analytic results to date rely on assumptions and arguments from complexity theory. Due to this, there is little intuition as to the source of the expressive power of quantum neural networks or for which classes of classical data any advantage can be reasonably expected to hold. Here, we study the relative expressive power between a broad class of neural network sequence models and a class of recurrent models based on Gaussian operations with non-Gaussian measurements. We explicitly show that quantum contextuality is the source of an unconditional memory separation in the expressivity of the two model classes. Additionally, as we are able to pinpoint quantum contextuality as the source of this separation, we use this intuition to study the relative performance of our introduced model on a standard translation data set exhibiting linguistic contextuality. In doing so, we demonstrate that our introduced quantum models are able to outperform state of the art classical models even in practice.&lt;/p&gt;
&lt;/blockquote&gt;
&lt;/p&gt;
&lt;/details&gt;&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
